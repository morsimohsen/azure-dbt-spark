[0m20:45:21.987473 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C6EB60F1A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C6EB60DC10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C6E8FF78C0>]}


============================== 20:45:21.994567 | 98831fb3-c2b7-47ef-b491-2e7dbc6bc67d ==============================
[0m20:45:21.994567 [info ] [MainThread]: Running with dbt=1.9.4
[0m20:45:21.995227 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\morsi\\.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': 'H:\\Acadmy\\My\\Projects\\Azure_Spark_DBT\\azure_dbt_spark\\logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt debug', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m20:45:22.007527 [info ] [MainThread]: dbt version: 1.9.4
[0m20:45:22.008533 [info ] [MainThread]: python version: 3.12.6
[0m20:45:22.008533 [info ] [MainThread]: python path: H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Scripts\python.exe
[0m20:45:22.009536 [info ] [MainThread]: os info: Windows-11-10.0.26100-SP0
[0m20:45:22.835378 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:45:22.836885 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:45:22.836885 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:45:23.446781 [info ] [MainThread]: Using profiles dir at C:\Users\morsi\.dbt
[0m20:45:23.452300 [info ] [MainThread]: Using profiles.yml file at C:\Users\morsi\.dbt\profiles.yml
[0m20:45:23.453312 [info ] [MainThread]: Using dbt_project.yml file at H:\Acadmy\My\Projects\Azure_Spark_DBT\azure_dbt_spark\dbt_project.yml
[0m20:45:23.454490 [info ] [MainThread]: adapter type: databricks
[0m20:45:23.454490 [info ] [MainThread]: adapter version: 1.10.1
[0m20:45:23.547677 [info ] [MainThread]: Configuration:
[0m20:45:23.548754 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m20:45:23.549679 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m20:45:23.549679 [info ] [MainThread]: Required dependencies:
[0m20:45:23.550713 [debug] [MainThread]: Executing "git --help"
[0m20:45:23.588117 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m20:45:23.589134 [debug] [MainThread]: STDERR: "b''"
[0m20:45:23.589134 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m20:45:23.590174 [info ] [MainThread]: Connection:
[0m20:45:23.591126 [info ] [MainThread]:   host: adb-2755154010893639.19.azuredatabricks.net
[0m20:45:23.591126 [info ] [MainThread]:   http_path: sql/protocolv1/o/2755154010893639/0521-160211-rrnl6jhk
[0m20:45:23.592140 [info ] [MainThread]:   catalog: hive_metastore
[0m20:45:23.592140 [info ] [MainThread]:   schema: saleslt
[0m20:45:23.593131 [info ] [MainThread]: Registered adapter: databricks=1.10.1
[0m20:45:23.899604 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=debug, idle-time=0s, language=None, compute-name=) - Creating connection
[0m20:45:23.899604 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m20:45:23.900504 [debug] [MainThread]: Using databricks connection "debug"
[0m20:45:23.900504 [debug] [MainThread]: On debug: select 1 as id
[0m20:45:23.900504 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:47:22.724385 [error] [MainThread]: Encountered an error:

[0m20:47:22.741038 [error] [MainThread]: Traceback (most recent call last):
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\cli\requires.py", line 153, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\cli\requires.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\cli\main.py", line 414, in debug
    results = task.run()
              ^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\task\debug.py", line 144, in run
    connection_status = self.test_connection()
                        ^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\task\debug.py", line 465, in test_connection
    connection_result = self.attempt_connection(self.profile)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\task\debug.py", line 443, in attempt_connection
    adapter.debug_query()
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\spark\impl.py", line 519, in debug_query
    self.execute("select 1 as id")
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\databricks\impl.py", line 306, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt_common\record.py", line 507, in record_replay_wrapper
    return func_to_record(*call_args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\base\impl.py", line 438, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\databricks\connections.py", line 337, in execute
    _, cursor = self.add_query(sql, auto_begin)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\databricks\connections.py", line 310, in add_query
    handle: DatabricksHandle = connection.handle
                               ^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\contracts\connection.py", line 96, in handle
    self._handle.resolve(self)
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\contracts\connection.py", line 120, in resolve
    return self.opener(connection)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\databricks\connections.py", line 471, in open
    return cls.retry_connection(
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\base\connections.py", line 237, in retry_connection
    connection.handle = connect()
                        ^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\databricks\connections.py", line 450, in connect
    conn = DatabricksHandle.from_connection_args(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\databricks\handle.py", line 211, in from_connection_args
    conn = dbsql.connect(**conn_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\__init__.py", line 90, in connect
    return Connection(server_hostname, http_path, access_token, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\client.py", line 272, in __init__
    self._open_session_resp = self.thrift_backend.open_session(
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\thrift_backend.py", line 552, in open_session
    response = self.make_request(self._client.OpenSession, open_session_req)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\thrift_backend.py", line 469, in make_request
    response_or_error_info = attempt_request(attempt)
                             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\thrift_backend.py", line 379, in attempt_request
    response = method(request)
               ^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\thrift_api\TCLIService\TCLIService.py", line 204, in OpenSession
    self.send_OpenSession(req)
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\thrift_api\TCLIService\TCLIService.py", line 213, in send_OpenSession
    self._oprot.trans.flush()
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\auth\thrift_http_client.py", line 186, in flush
    self.__resp = self.__pool.request(
                  ^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\urllib3\_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\urllib3\_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\urllib3\connectionpool.py", line 942, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\urllib3\connectionpool.py", line 940, in urlopen
    retries.sleep(response)
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\urllib3\util\retry.py", line 359, in sleep
    slept = self.sleep_for_retry(response)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\auth\retry.py", line 300, in sleep_for_retry
    time.sleep(proposed_wait)
KeyboardInterrupt

[0m20:47:22.744128 [debug] [MainThread]: Command `dbt debug` failed at 20:47:22.743130 after 121.06 seconds
[0m20:47:22.744128 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m20:47:22.744128 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C6EADCF5F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C6EB69B620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C6EE9A1790>]}
[0m20:47:22.745121 [debug] [MainThread]: Flushing usage events
[0m20:47:23.462450 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:47:31.883518 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F72A6658E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F72CBFAF90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F72D1C2FF0>]}


============================== 20:47:31.891248 | 88518666-bf5e-46db-af27-1596d9280580 ==============================
[0m20:47:31.891248 [info ] [MainThread]: Running with dbt=1.9.4
[0m20:47:31.892179 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\morsi\\.dbt', 'log_path': 'H:\\Acadmy\\My\\Projects\\Azure_Spark_DBT\\azure_dbt_spark\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt debug', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m20:47:31.904991 [info ] [MainThread]: dbt version: 1.9.4
[0m20:47:31.906413 [info ] [MainThread]: python version: 3.12.6
[0m20:47:31.906413 [info ] [MainThread]: python path: H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Scripts\python.exe
[0m20:47:31.906413 [info ] [MainThread]: os info: Windows-11-10.0.26100-SP0
[0m20:47:32.792313 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:47:32.793314 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:47:32.793314 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:47:33.373881 [info ] [MainThread]: Using profiles dir at C:\Users\morsi\.dbt
[0m20:47:33.373881 [info ] [MainThread]: Using profiles.yml file at C:\Users\morsi\.dbt\profiles.yml
[0m20:47:33.375395 [info ] [MainThread]: Using dbt_project.yml file at H:\Acadmy\My\Projects\Azure_Spark_DBT\azure_dbt_spark\dbt_project.yml
[0m20:47:33.375395 [info ] [MainThread]: adapter type: databricks
[0m20:47:33.376413 [info ] [MainThread]: adapter version: 1.10.1
[0m20:47:33.468659 [info ] [MainThread]: Configuration:
[0m20:47:33.469207 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m20:47:33.470078 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m20:47:33.470600 [info ] [MainThread]: Required dependencies:
[0m20:47:33.471615 [debug] [MainThread]: Executing "git --help"
[0m20:47:33.507947 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m20:47:33.507947 [debug] [MainThread]: STDERR: "b''"
[0m20:47:33.507947 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m20:47:33.509014 [info ] [MainThread]: Connection:
[0m20:47:33.509910 [info ] [MainThread]:   host: adb-2755154010893639.19.azuredatabricks.net
[0m20:47:33.509910 [info ] [MainThread]:   http_path: sql/protocolv1/o/2755154010893639/0521-160211-rrnl6jhk
[0m20:47:33.510909 [info ] [MainThread]:   catalog: hive_metastore
[0m20:47:33.510909 [info ] [MainThread]:   schema: saleslt
[0m20:47:33.511921 [info ] [MainThread]: Registered adapter: databricks=1.10.1
[0m20:47:33.805058 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=debug, idle-time=0s, language=None, compute-name=) - Creating connection
[0m20:47:33.805595 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m20:47:33.805595 [debug] [MainThread]: Using databricks connection "debug"
[0m20:47:33.805595 [debug] [MainThread]: On debug: select 1 as id
[0m20:47:33.805595 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:48:56.525552 [error] [MainThread]: Encountered an error:

[0m20:48:56.532067 [error] [MainThread]: Traceback (most recent call last):
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\cli\requires.py", line 153, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\cli\requires.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\cli\main.py", line 414, in debug
    results = task.run()
              ^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\task\debug.py", line 144, in run
    connection_status = self.test_connection()
                        ^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\task\debug.py", line 465, in test_connection
    connection_result = self.attempt_connection(self.profile)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\task\debug.py", line 443, in attempt_connection
    adapter.debug_query()
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\spark\impl.py", line 519, in debug_query
    self.execute("select 1 as id")
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\databricks\impl.py", line 306, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt_common\record.py", line 507, in record_replay_wrapper
    return func_to_record(*call_args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\base\impl.py", line 438, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\databricks\connections.py", line 337, in execute
    _, cursor = self.add_query(sql, auto_begin)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\databricks\connections.py", line 310, in add_query
    handle: DatabricksHandle = connection.handle
                               ^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\contracts\connection.py", line 96, in handle
    self._handle.resolve(self)
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\contracts\connection.py", line 120, in resolve
    return self.opener(connection)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\databricks\connections.py", line 471, in open
    return cls.retry_connection(
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\base\connections.py", line 237, in retry_connection
    connection.handle = connect()
                        ^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\databricks\connections.py", line 450, in connect
    conn = DatabricksHandle.from_connection_args(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\databricks\handle.py", line 211, in from_connection_args
    conn = dbsql.connect(**conn_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\__init__.py", line 90, in connect
    return Connection(server_hostname, http_path, access_token, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\client.py", line 272, in __init__
    self._open_session_resp = self.thrift_backend.open_session(
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\thrift_backend.py", line 552, in open_session
    response = self.make_request(self._client.OpenSession, open_session_req)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\thrift_backend.py", line 469, in make_request
    response_or_error_info = attempt_request(attempt)
                             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\thrift_backend.py", line 379, in attempt_request
    response = method(request)
               ^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\thrift_api\TCLIService\TCLIService.py", line 204, in OpenSession
    self.send_OpenSession(req)
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\thrift_api\TCLIService\TCLIService.py", line 213, in send_OpenSession
    self._oprot.trans.flush()
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\auth\thrift_http_client.py", line 186, in flush
    self.__resp = self.__pool.request(
                  ^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\urllib3\_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\urllib3\_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\urllib3\connectionpool.py", line 942, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\urllib3\connectionpool.py", line 940, in urlopen
    retries.sleep(response)
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\urllib3\util\retry.py", line 359, in sleep
    slept = self.sleep_for_retry(response)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\auth\retry.py", line 300, in sleep_for_retry
    time.sleep(proposed_wait)
KeyboardInterrupt

[0m20:48:56.534605 [debug] [MainThread]: Command `dbt debug` failed at 20:48:56.534605 after 84.96 seconds
[0m20:48:56.534605 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m20:48:56.535708 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F72CE10B30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F72A8DAAB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F72E729BE0>]}
[0m20:48:56.535708 [debug] [MainThread]: Flushing usage events
[0m20:49:05.669416 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C5AA3C7860>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C5A9D1EF60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C5AA1CC7A0>]}


============================== 20:49:05.679585 | 93e04b79-8a3f-4f19-b8cb-e9e8cc99721c ==============================
[0m20:49:05.679585 [info ] [MainThread]: Running with dbt=1.9.4
[0m20:49:05.681509 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\morsi\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'H:\\Acadmy\\My\\Projects\\Azure_Spark_DBT\\azure_dbt_spark\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt debug', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m20:49:05.693033 [info ] [MainThread]: dbt version: 1.9.4
[0m20:49:05.693033 [info ] [MainThread]: python version: 3.12.6
[0m20:49:05.694038 [info ] [MainThread]: python path: H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Scripts\python.exe
[0m20:49:05.694038 [info ] [MainThread]: os info: Windows-11-10.0.26100-SP0
[0m20:49:06.509383 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:49:06.509383 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:49:06.509383 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:49:07.089799 [info ] [MainThread]: Using profiles dir at C:\Users\morsi\.dbt
[0m20:49:07.090924 [info ] [MainThread]: Using profiles.yml file at C:\Users\morsi\.dbt\profiles.yml
[0m20:49:07.090924 [info ] [MainThread]: Using dbt_project.yml file at H:\Acadmy\My\Projects\Azure_Spark_DBT\azure_dbt_spark\dbt_project.yml
[0m20:49:07.092153 [info ] [MainThread]: adapter type: databricks
[0m20:49:07.092153 [info ] [MainThread]: adapter version: 1.10.1
[0m20:49:07.215122 [info ] [MainThread]: Configuration:
[0m20:49:07.216249 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m20:49:07.216957 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m20:49:07.216957 [info ] [MainThread]: Required dependencies:
[0m20:49:07.217972 [debug] [MainThread]: Executing "git --help"
[0m20:49:07.256370 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m20:49:07.257409 [debug] [MainThread]: STDERR: "b''"
[0m20:49:07.257409 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m20:49:07.258437 [info ] [MainThread]: Connection:
[0m20:49:07.258437 [info ] [MainThread]:   host: adb-2755154010893639.19.azuredatabricks.net
[0m20:49:07.259408 [info ] [MainThread]:   http_path: sql/protocolv1/o/2755154010893639/0521-160211-rrnl6jhk
[0m20:49:07.259408 [info ] [MainThread]:   catalog: hive_metastore
[0m20:49:07.260415 [info ] [MainThread]:   schema: saleslt
[0m20:49:07.260415 [info ] [MainThread]: Registered adapter: databricks=1.10.1
[0m20:49:07.611435 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=debug, idle-time=0s, language=None, compute-name=) - Creating connection
[0m20:49:07.611435 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m20:49:07.612431 [debug] [MainThread]: Using databricks connection "debug"
[0m20:49:07.612431 [debug] [MainThread]: On debug: select 1 as id
[0m20:49:07.612431 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:51:59.321872 [error] [MainThread]: Encountered an error:

[0m20:51:59.332640 [error] [MainThread]: Traceback (most recent call last):
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\cli\requires.py", line 153, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\cli\requires.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\cli\main.py", line 414, in debug
    results = task.run()
              ^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\task\debug.py", line 144, in run
    connection_status = self.test_connection()
                        ^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\task\debug.py", line 465, in test_connection
    connection_result = self.attempt_connection(self.profile)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\task\debug.py", line 443, in attempt_connection
    adapter.debug_query()
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\spark\impl.py", line 519, in debug_query
    self.execute("select 1 as id")
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\databricks\impl.py", line 306, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt_common\record.py", line 507, in record_replay_wrapper
    return func_to_record(*call_args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\base\impl.py", line 438, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\databricks\connections.py", line 337, in execute
    _, cursor = self.add_query(sql, auto_begin)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\databricks\connections.py", line 310, in add_query
    handle: DatabricksHandle = connection.handle
                               ^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\contracts\connection.py", line 96, in handle
    self._handle.resolve(self)
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\contracts\connection.py", line 120, in resolve
    return self.opener(connection)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\databricks\connections.py", line 471, in open
    return cls.retry_connection(
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\base\connections.py", line 237, in retry_connection
    connection.handle = connect()
                        ^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\databricks\connections.py", line 450, in connect
    conn = DatabricksHandle.from_connection_args(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\databricks\handle.py", line 211, in from_connection_args
    conn = dbsql.connect(**conn_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\__init__.py", line 90, in connect
    return Connection(server_hostname, http_path, access_token, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\client.py", line 272, in __init__
    self._open_session_resp = self.thrift_backend.open_session(
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\thrift_backend.py", line 552, in open_session
    response = self.make_request(self._client.OpenSession, open_session_req)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\thrift_backend.py", line 469, in make_request
    response_or_error_info = attempt_request(attempt)
                             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\thrift_backend.py", line 379, in attempt_request
    response = method(request)
               ^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\thrift_api\TCLIService\TCLIService.py", line 204, in OpenSession
    self.send_OpenSession(req)
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\thrift_api\TCLIService\TCLIService.py", line 213, in send_OpenSession
    self._oprot.trans.flush()
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\auth\thrift_http_client.py", line 186, in flush
    self.__resp = self.__pool.request(
                  ^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\urllib3\_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\urllib3\_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\urllib3\connectionpool.py", line 942, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\urllib3\connectionpool.py", line 942, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\urllib3\connectionpool.py", line 940, in urlopen
    retries.sleep(response)
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\urllib3\util\retry.py", line 359, in sleep
    slept = self.sleep_for_retry(response)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\auth\retry.py", line 300, in sleep_for_retry
    time.sleep(proposed_wait)
KeyboardInterrupt

[0m20:51:59.335640 [debug] [MainThread]: Command `dbt debug` failed at 20:51:59.334736 after 173.83 seconds
[0m20:51:59.335640 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m20:51:59.336730 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C5A81DF350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C5BDA32900>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C5AA4244D0>]}
[0m20:51:59.336730 [debug] [MainThread]: Flushing usage events
