[0m20:45:21.987473 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C6EB60F1A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C6EB60DC10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C6E8FF78C0>]}


============================== 20:45:21.994567 | 98831fb3-c2b7-47ef-b491-2e7dbc6bc67d ==============================
[0m20:45:21.994567 [info ] [MainThread]: Running with dbt=1.9.4
[0m20:45:21.995227 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\morsi\\.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': 'H:\\Acadmy\\My\\Projects\\Azure_Spark_DBT\\azure_dbt_spark\\logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt debug', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m20:45:22.007527 [info ] [MainThread]: dbt version: 1.9.4
[0m20:45:22.008533 [info ] [MainThread]: python version: 3.12.6
[0m20:45:22.008533 [info ] [MainThread]: python path: H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Scripts\python.exe
[0m20:45:22.009536 [info ] [MainThread]: os info: Windows-11-10.0.26100-SP0
[0m20:45:22.835378 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:45:22.836885 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:45:22.836885 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:45:23.446781 [info ] [MainThread]: Using profiles dir at C:\Users\morsi\.dbt
[0m20:45:23.452300 [info ] [MainThread]: Using profiles.yml file at C:\Users\morsi\.dbt\profiles.yml
[0m20:45:23.453312 [info ] [MainThread]: Using dbt_project.yml file at H:\Acadmy\My\Projects\Azure_Spark_DBT\azure_dbt_spark\dbt_project.yml
[0m20:45:23.454490 [info ] [MainThread]: adapter type: databricks
[0m20:45:23.454490 [info ] [MainThread]: adapter version: 1.10.1
[0m20:45:23.547677 [info ] [MainThread]: Configuration:
[0m20:45:23.548754 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m20:45:23.549679 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m20:45:23.549679 [info ] [MainThread]: Required dependencies:
[0m20:45:23.550713 [debug] [MainThread]: Executing "git --help"
[0m20:45:23.588117 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m20:45:23.589134 [debug] [MainThread]: STDERR: "b''"
[0m20:45:23.589134 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m20:45:23.590174 [info ] [MainThread]: Connection:
[0m20:45:23.591126 [info ] [MainThread]:   host: adb-2755154010893639.19.azuredatabricks.net
[0m20:45:23.591126 [info ] [MainThread]:   http_path: sql/protocolv1/o/2755154010893639/0521-160211-rrnl6jhk
[0m20:45:23.592140 [info ] [MainThread]:   catalog: hive_metastore
[0m20:45:23.592140 [info ] [MainThread]:   schema: saleslt
[0m20:45:23.593131 [info ] [MainThread]: Registered adapter: databricks=1.10.1
[0m20:45:23.899604 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=debug, idle-time=0s, language=None, compute-name=) - Creating connection
[0m20:45:23.899604 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m20:45:23.900504 [debug] [MainThread]: Using databricks connection "debug"
[0m20:45:23.900504 [debug] [MainThread]: On debug: select 1 as id
[0m20:45:23.900504 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:47:22.724385 [error] [MainThread]: Encountered an error:

[0m20:47:22.741038 [error] [MainThread]: Traceback (most recent call last):
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\cli\requires.py", line 153, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\cli\requires.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\cli\main.py", line 414, in debug
    results = task.run()
              ^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\task\debug.py", line 144, in run
    connection_status = self.test_connection()
                        ^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\task\debug.py", line 465, in test_connection
    connection_result = self.attempt_connection(self.profile)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\task\debug.py", line 443, in attempt_connection
    adapter.debug_query()
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\spark\impl.py", line 519, in debug_query
    self.execute("select 1 as id")
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\databricks\impl.py", line 306, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt_common\record.py", line 507, in record_replay_wrapper
    return func_to_record(*call_args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\base\impl.py", line 438, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\databricks\connections.py", line 337, in execute
    _, cursor = self.add_query(sql, auto_begin)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\databricks\connections.py", line 310, in add_query
    handle: DatabricksHandle = connection.handle
                               ^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\contracts\connection.py", line 96, in handle
    self._handle.resolve(self)
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\contracts\connection.py", line 120, in resolve
    return self.opener(connection)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\databricks\connections.py", line 471, in open
    return cls.retry_connection(
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\base\connections.py", line 237, in retry_connection
    connection.handle = connect()
                        ^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\databricks\connections.py", line 450, in connect
    conn = DatabricksHandle.from_connection_args(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\databricks\handle.py", line 211, in from_connection_args
    conn = dbsql.connect(**conn_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\__init__.py", line 90, in connect
    return Connection(server_hostname, http_path, access_token, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\client.py", line 272, in __init__
    self._open_session_resp = self.thrift_backend.open_session(
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\thrift_backend.py", line 552, in open_session
    response = self.make_request(self._client.OpenSession, open_session_req)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\thrift_backend.py", line 469, in make_request
    response_or_error_info = attempt_request(attempt)
                             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\thrift_backend.py", line 379, in attempt_request
    response = method(request)
               ^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\thrift_api\TCLIService\TCLIService.py", line 204, in OpenSession
    self.send_OpenSession(req)
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\thrift_api\TCLIService\TCLIService.py", line 213, in send_OpenSession
    self._oprot.trans.flush()
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\auth\thrift_http_client.py", line 186, in flush
    self.__resp = self.__pool.request(
                  ^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\urllib3\_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\urllib3\_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\urllib3\connectionpool.py", line 942, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\urllib3\connectionpool.py", line 940, in urlopen
    retries.sleep(response)
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\urllib3\util\retry.py", line 359, in sleep
    slept = self.sleep_for_retry(response)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\auth\retry.py", line 300, in sleep_for_retry
    time.sleep(proposed_wait)
KeyboardInterrupt

[0m20:47:22.744128 [debug] [MainThread]: Command `dbt debug` failed at 20:47:22.743130 after 121.06 seconds
[0m20:47:22.744128 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m20:47:22.744128 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C6EADCF5F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C6EB69B620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C6EE9A1790>]}
[0m20:47:22.745121 [debug] [MainThread]: Flushing usage events
[0m20:47:23.462450 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:47:31.883518 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F72A6658E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F72CBFAF90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F72D1C2FF0>]}


============================== 20:47:31.891248 | 88518666-bf5e-46db-af27-1596d9280580 ==============================
[0m20:47:31.891248 [info ] [MainThread]: Running with dbt=1.9.4
[0m20:47:31.892179 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\morsi\\.dbt', 'log_path': 'H:\\Acadmy\\My\\Projects\\Azure_Spark_DBT\\azure_dbt_spark\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt debug', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m20:47:31.904991 [info ] [MainThread]: dbt version: 1.9.4
[0m20:47:31.906413 [info ] [MainThread]: python version: 3.12.6
[0m20:47:31.906413 [info ] [MainThread]: python path: H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Scripts\python.exe
[0m20:47:31.906413 [info ] [MainThread]: os info: Windows-11-10.0.26100-SP0
[0m20:47:32.792313 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:47:32.793314 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:47:32.793314 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:47:33.373881 [info ] [MainThread]: Using profiles dir at C:\Users\morsi\.dbt
[0m20:47:33.373881 [info ] [MainThread]: Using profiles.yml file at C:\Users\morsi\.dbt\profiles.yml
[0m20:47:33.375395 [info ] [MainThread]: Using dbt_project.yml file at H:\Acadmy\My\Projects\Azure_Spark_DBT\azure_dbt_spark\dbt_project.yml
[0m20:47:33.375395 [info ] [MainThread]: adapter type: databricks
[0m20:47:33.376413 [info ] [MainThread]: adapter version: 1.10.1
[0m20:47:33.468659 [info ] [MainThread]: Configuration:
[0m20:47:33.469207 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m20:47:33.470078 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m20:47:33.470600 [info ] [MainThread]: Required dependencies:
[0m20:47:33.471615 [debug] [MainThread]: Executing "git --help"
[0m20:47:33.507947 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m20:47:33.507947 [debug] [MainThread]: STDERR: "b''"
[0m20:47:33.507947 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m20:47:33.509014 [info ] [MainThread]: Connection:
[0m20:47:33.509910 [info ] [MainThread]:   host: adb-2755154010893639.19.azuredatabricks.net
[0m20:47:33.509910 [info ] [MainThread]:   http_path: sql/protocolv1/o/2755154010893639/0521-160211-rrnl6jhk
[0m20:47:33.510909 [info ] [MainThread]:   catalog: hive_metastore
[0m20:47:33.510909 [info ] [MainThread]:   schema: saleslt
[0m20:47:33.511921 [info ] [MainThread]: Registered adapter: databricks=1.10.1
[0m20:47:33.805058 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=debug, idle-time=0s, language=None, compute-name=) - Creating connection
[0m20:47:33.805595 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m20:47:33.805595 [debug] [MainThread]: Using databricks connection "debug"
[0m20:47:33.805595 [debug] [MainThread]: On debug: select 1 as id
[0m20:47:33.805595 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:48:56.525552 [error] [MainThread]: Encountered an error:

[0m20:48:56.532067 [error] [MainThread]: Traceback (most recent call last):
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\cli\requires.py", line 153, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\cli\requires.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\cli\main.py", line 414, in debug
    results = task.run()
              ^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\task\debug.py", line 144, in run
    connection_status = self.test_connection()
                        ^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\task\debug.py", line 465, in test_connection
    connection_result = self.attempt_connection(self.profile)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\task\debug.py", line 443, in attempt_connection
    adapter.debug_query()
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\spark\impl.py", line 519, in debug_query
    self.execute("select 1 as id")
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\databricks\impl.py", line 306, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt_common\record.py", line 507, in record_replay_wrapper
    return func_to_record(*call_args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\base\impl.py", line 438, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\databricks\connections.py", line 337, in execute
    _, cursor = self.add_query(sql, auto_begin)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\databricks\connections.py", line 310, in add_query
    handle: DatabricksHandle = connection.handle
                               ^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\contracts\connection.py", line 96, in handle
    self._handle.resolve(self)
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\contracts\connection.py", line 120, in resolve
    return self.opener(connection)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\databricks\connections.py", line 471, in open
    return cls.retry_connection(
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\base\connections.py", line 237, in retry_connection
    connection.handle = connect()
                        ^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\databricks\connections.py", line 450, in connect
    conn = DatabricksHandle.from_connection_args(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\databricks\handle.py", line 211, in from_connection_args
    conn = dbsql.connect(**conn_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\__init__.py", line 90, in connect
    return Connection(server_hostname, http_path, access_token, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\client.py", line 272, in __init__
    self._open_session_resp = self.thrift_backend.open_session(
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\thrift_backend.py", line 552, in open_session
    response = self.make_request(self._client.OpenSession, open_session_req)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\thrift_backend.py", line 469, in make_request
    response_or_error_info = attempt_request(attempt)
                             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\thrift_backend.py", line 379, in attempt_request
    response = method(request)
               ^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\thrift_api\TCLIService\TCLIService.py", line 204, in OpenSession
    self.send_OpenSession(req)
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\thrift_api\TCLIService\TCLIService.py", line 213, in send_OpenSession
    self._oprot.trans.flush()
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\auth\thrift_http_client.py", line 186, in flush
    self.__resp = self.__pool.request(
                  ^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\urllib3\_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\urllib3\_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\urllib3\connectionpool.py", line 942, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\urllib3\connectionpool.py", line 940, in urlopen
    retries.sleep(response)
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\urllib3\util\retry.py", line 359, in sleep
    slept = self.sleep_for_retry(response)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\auth\retry.py", line 300, in sleep_for_retry
    time.sleep(proposed_wait)
KeyboardInterrupt

[0m20:48:56.534605 [debug] [MainThread]: Command `dbt debug` failed at 20:48:56.534605 after 84.96 seconds
[0m20:48:56.534605 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m20:48:56.535708 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F72CE10B30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F72A8DAAB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F72E729BE0>]}
[0m20:48:56.535708 [debug] [MainThread]: Flushing usage events
[0m20:49:05.669416 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C5AA3C7860>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C5A9D1EF60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C5AA1CC7A0>]}


============================== 20:49:05.679585 | 93e04b79-8a3f-4f19-b8cb-e9e8cc99721c ==============================
[0m20:49:05.679585 [info ] [MainThread]: Running with dbt=1.9.4
[0m20:49:05.681509 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\morsi\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'H:\\Acadmy\\My\\Projects\\Azure_Spark_DBT\\azure_dbt_spark\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt debug', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m20:49:05.693033 [info ] [MainThread]: dbt version: 1.9.4
[0m20:49:05.693033 [info ] [MainThread]: python version: 3.12.6
[0m20:49:05.694038 [info ] [MainThread]: python path: H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Scripts\python.exe
[0m20:49:05.694038 [info ] [MainThread]: os info: Windows-11-10.0.26100-SP0
[0m20:49:06.509383 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:49:06.509383 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:49:06.509383 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:49:07.089799 [info ] [MainThread]: Using profiles dir at C:\Users\morsi\.dbt
[0m20:49:07.090924 [info ] [MainThread]: Using profiles.yml file at C:\Users\morsi\.dbt\profiles.yml
[0m20:49:07.090924 [info ] [MainThread]: Using dbt_project.yml file at H:\Acadmy\My\Projects\Azure_Spark_DBT\azure_dbt_spark\dbt_project.yml
[0m20:49:07.092153 [info ] [MainThread]: adapter type: databricks
[0m20:49:07.092153 [info ] [MainThread]: adapter version: 1.10.1
[0m20:49:07.215122 [info ] [MainThread]: Configuration:
[0m20:49:07.216249 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m20:49:07.216957 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m20:49:07.216957 [info ] [MainThread]: Required dependencies:
[0m20:49:07.217972 [debug] [MainThread]: Executing "git --help"
[0m20:49:07.256370 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m20:49:07.257409 [debug] [MainThread]: STDERR: "b''"
[0m20:49:07.257409 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m20:49:07.258437 [info ] [MainThread]: Connection:
[0m20:49:07.258437 [info ] [MainThread]:   host: adb-2755154010893639.19.azuredatabricks.net
[0m20:49:07.259408 [info ] [MainThread]:   http_path: sql/protocolv1/o/2755154010893639/0521-160211-rrnl6jhk
[0m20:49:07.259408 [info ] [MainThread]:   catalog: hive_metastore
[0m20:49:07.260415 [info ] [MainThread]:   schema: saleslt
[0m20:49:07.260415 [info ] [MainThread]: Registered adapter: databricks=1.10.1
[0m20:49:07.611435 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=debug, idle-time=0s, language=None, compute-name=) - Creating connection
[0m20:49:07.611435 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m20:49:07.612431 [debug] [MainThread]: Using databricks connection "debug"
[0m20:49:07.612431 [debug] [MainThread]: On debug: select 1 as id
[0m20:49:07.612431 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:51:59.321872 [error] [MainThread]: Encountered an error:

[0m20:51:59.332640 [error] [MainThread]: Traceback (most recent call last):
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\cli\requires.py", line 153, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\cli\requires.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\cli\main.py", line 414, in debug
    results = task.run()
              ^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\task\debug.py", line 144, in run
    connection_status = self.test_connection()
                        ^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\task\debug.py", line 465, in test_connection
    connection_result = self.attempt_connection(self.profile)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\task\debug.py", line 443, in attempt_connection
    adapter.debug_query()
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\spark\impl.py", line 519, in debug_query
    self.execute("select 1 as id")
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\databricks\impl.py", line 306, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt_common\record.py", line 507, in record_replay_wrapper
    return func_to_record(*call_args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\base\impl.py", line 438, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\databricks\connections.py", line 337, in execute
    _, cursor = self.add_query(sql, auto_begin)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\databricks\connections.py", line 310, in add_query
    handle: DatabricksHandle = connection.handle
                               ^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\contracts\connection.py", line 96, in handle
    self._handle.resolve(self)
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\contracts\connection.py", line 120, in resolve
    return self.opener(connection)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\databricks\connections.py", line 471, in open
    return cls.retry_connection(
           ^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\base\connections.py", line 237, in retry_connection
    connection.handle = connect()
                        ^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\databricks\connections.py", line 450, in connect
    conn = DatabricksHandle.from_connection_args(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\dbt\adapters\databricks\handle.py", line 211, in from_connection_args
    conn = dbsql.connect(**conn_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\__init__.py", line 90, in connect
    return Connection(server_hostname, http_path, access_token, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\client.py", line 272, in __init__
    self._open_session_resp = self.thrift_backend.open_session(
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\thrift_backend.py", line 552, in open_session
    response = self.make_request(self._client.OpenSession, open_session_req)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\thrift_backend.py", line 469, in make_request
    response_or_error_info = attempt_request(attempt)
                             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\thrift_backend.py", line 379, in attempt_request
    response = method(request)
               ^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\thrift_api\TCLIService\TCLIService.py", line 204, in OpenSession
    self.send_OpenSession(req)
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\thrift_api\TCLIService\TCLIService.py", line 213, in send_OpenSession
    self._oprot.trans.flush()
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\auth\thrift_http_client.py", line 186, in flush
    self.__resp = self.__pool.request(
                  ^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\urllib3\_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\urllib3\_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\urllib3\connectionpool.py", line 942, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\urllib3\connectionpool.py", line 942, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\urllib3\connectionpool.py", line 940, in urlopen
    retries.sleep(response)
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\urllib3\util\retry.py", line 359, in sleep
    slept = self.sleep_for_retry(response)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "H:\Acadmy\My\Projects\Azure_Spark_DBT\venv\Lib\site-packages\databricks\sql\auth\retry.py", line 300, in sleep_for_retry
    time.sleep(proposed_wait)
KeyboardInterrupt

[0m20:51:59.335640 [debug] [MainThread]: Command `dbt debug` failed at 20:51:59.334736 after 173.83 seconds
[0m20:51:59.335640 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m20:51:59.336730 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C5A81DF350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C5BDA32900>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C5AA4244D0>]}
[0m20:51:59.336730 [debug] [MainThread]: Flushing usage events
[0m21:06:42.499524 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E20D7F5880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E20D7F6150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E20D6431A0>]}


============================== 21:06:42.507426 | 42a30b75-ba3c-40e0-89c5-5d79928ddcf1 ==============================
[0m21:06:42.507426 [info ] [MainThread]: Running with dbt=1.9.4
[0m21:06:42.508719 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'H:\\Acadmy\\My\\Projects\\Azure_Spark_DBT\\azure_dbt_spark\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\morsi\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt snapshot', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m21:06:43.364102 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m21:06:43.365100 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m21:06:43.365100 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m21:06:44.395249 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '42a30b75-ba3c-40e0-89c5-5d79928ddcf1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E20D1D6330>]}
[0m21:06:44.441153 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '42a30b75-ba3c-40e0-89c5-5d79928ddcf1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E20B60F4A0>]}
[0m21:06:44.442130 [info ] [MainThread]: Registered adapter: databricks=1.10.1
[0m21:06:44.737372 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m21:06:44.737372 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m21:06:44.737372 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '42a30b75-ba3c-40e0-89c5-5d79928ddcf1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E220EDCDD0>]}
[0m21:06:45.954099 [error] [MainThread]: Encountered an error:
Compilation Error
  Snapshot 'snapshot.azure_dbt_spark.address_snapshot' (snapshots\address.sql) depends on a source named 'saleslt.address' which was not found
[0m21:06:45.955994 [debug] [MainThread]: Command `dbt snapshot` failed at 21:06:45.955994 after 3.78 seconds
[0m21:06:45.955994 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E20D6431A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E221356FF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E20D418920>]}
[0m21:06:45.955994 [debug] [MainThread]: Flushing usage events
[0m21:06:47.345333 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:09:42.383173 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C76C595640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C76EAD72C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C76EAD4CE0>]}


============================== 21:09:42.391210 | 2b91c0ba-8d69-4fd2-8066-60ad52978a1f ==============================
[0m21:09:42.391210 [info ] [MainThread]: Running with dbt=1.9.4
[0m21:09:42.392129 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\morsi\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'H:\\Acadmy\\My\\Projects\\Azure_Spark_DBT\\azure_dbt_spark\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'invocation_command': 'dbt snapshot', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m21:09:43.142247 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m21:09:43.143236 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m21:09:43.143236 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m21:09:43.774571 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2b91c0ba-8d69-4fd2-8066-60ad52978a1f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C76F59DC70>]}
[0m21:09:43.806062 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2b91c0ba-8d69-4fd2-8066-60ad52978a1f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C702772EA0>]}
[0m21:09:43.806062 [info ] [MainThread]: Registered adapter: databricks=1.10.1
[0m21:09:44.107610 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m21:09:44.108607 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m21:09:44.109548 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '2b91c0ba-8d69-4fd2-8066-60ad52978a1f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C702772AB0>]}
[0m21:09:45.301623 [error] [MainThread]: Encountered an error:
Compilation Error
  Snapshot 'snapshot.azure_dbt_spark.address_snapshot' (snapshots\address.sql) depends on a source named 'saleslt.address' which was not found
[0m21:09:45.303627 [debug] [MainThread]: Command `dbt snapshot` failed at 21:09:45.303627 after 3.21 seconds
[0m21:09:45.304533 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C76F1F10A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C702D50C80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C702D7D9A0>]}
[0m21:09:45.304533 [debug] [MainThread]: Flushing usage events
[0m21:09:46.103403 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:10:39.308316 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BC5D28BFE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BC5CAE7590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BC5D164770>]}


============================== 21:10:39.315316 | 144b2ff8-c9e1-4d8e-8870-8a72e5d13681 ==============================
[0m21:10:39.315316 [info ] [MainThread]: Running with dbt=1.9.4
[0m21:10:39.316320 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'H:\\Acadmy\\My\\Projects\\Azure_Spark_DBT\\azure_dbt_spark\\logs', 'profiles_dir': 'C:\\Users\\morsi\\.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt snapshot', 'send_anonymous_usage_stats': 'True'}
[0m21:10:40.321180 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m21:10:40.322190 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m21:10:40.322190 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m21:10:41.198938 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '144b2ff8-c9e1-4d8e-8870-8a72e5d13681', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BC5CB3F530>]}
[0m21:10:41.258748 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '144b2ff8-c9e1-4d8e-8870-8a72e5d13681', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BC5D3B80E0>]}
[0m21:10:41.259765 [info ] [MainThread]: Registered adapter: databricks=1.10.1
[0m21:10:41.629842 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m21:10:41.630841 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m21:10:41.631840 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '144b2ff8-c9e1-4d8e-8870-8a72e5d13681', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BC70366E40>]}
[0m21:10:43.260150 [error] [MainThread]: Encountered an error:
Compilation Error
  Snapshot 'snapshot.azure_dbt_spark.customeraddress_snapshot' (snapshots\customeraddress.sql) depends on a source named 'saleslt.customeraddress' which was not found
[0m21:10:43.262051 [debug] [MainThread]: Command `dbt snapshot` failed at 21:10:43.262051 after 4.13 seconds
[0m21:10:43.262051 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BC5D1658E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BC70CAC470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BC70A75190>]}
[0m21:10:43.262051 [debug] [MainThread]: Flushing usage events
[0m21:10:43.939313 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:13:15.436383 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219B2581C40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219B4A7FFB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219B4D966F0>]}


============================== 21:13:15.441702 | c4410eeb-fd85-4f17-9011-0109919f3bc8 ==============================
[0m21:13:15.441702 [info ] [MainThread]: Running with dbt=1.9.4
[0m21:13:15.441702 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'H:\\Acadmy\\My\\Projects\\Azure_Spark_DBT\\azure_dbt_spark\\logs', 'profiles_dir': 'C:\\Users\\morsi\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt snapshot', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m21:13:16.241589 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m21:13:16.241589 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m21:13:16.241589 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m21:13:16.930197 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c4410eeb-fd85-4f17-9011-0109919f3bc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219B69169F0>]}
[0m21:13:16.981910 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c4410eeb-fd85-4f17-9011-0109919f3bc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219B790BEF0>]}
[0m21:13:16.981910 [info ] [MainThread]: Registered adapter: databricks=1.10.1
[0m21:13:17.274865 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m21:13:17.274865 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m21:13:17.290919 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'c4410eeb-fd85-4f17-9011-0109919f3bc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219C845ABD0>]}
[0m21:13:18.526874 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.azure_dbt_spark.example
[0m21:13:18.546989 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c4410eeb-fd85-4f17-9011-0109919f3bc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219C897C3B0>]}
[0m21:13:18.605235 [debug] [MainThread]: Wrote artifact WritableManifest to H:\Acadmy\My\Projects\Azure_Spark_DBT\azure_dbt_spark\target\manifest.json
[0m21:13:18.614811 [debug] [MainThread]: Wrote artifact SemanticManifest to H:\Acadmy\My\Projects\Azure_Spark_DBT\azure_dbt_spark\target\semantic_manifest.json
[0m21:13:18.636507 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c4410eeb-fd85-4f17-9011-0109919f3bc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219C8A95A30>]}
[0m21:13:18.636507 [info ] [MainThread]: Found 7 snapshots, 9 sources, 660 macros
[0m21:13:18.636507 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c4410eeb-fd85-4f17-9011-0109919f3bc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219C897C560>]}
[0m21:13:18.650630 [info ] [MainThread]: 
[0m21:13:18.652273 [info ] [MainThread]: Concurrency: 2 threads (target='dev')
[0m21:13:18.652273 [info ] [MainThread]: 
[0m21:13:18.652273 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m21:13:18.652273 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m21:13:18.652273 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_hive_metastore, idle-time=0s, language=None, compute-name=) - Creating connection
[0m21:13:18.652273 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m21:13:18.652273 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m21:13:18.652273 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m21:13:18.652273 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:13:20.250397 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=bb6db4c9-6684-4648-a506-f323ac16f014) - Created
[0m21:13:21.134795 [debug] [ThreadPool]: SQL status: OK in 2.480 seconds
[0m21:13:21.134795 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=bb6db4c9-6684-4648-a506-f323ac16f014, command-id=a935c838-4172-4b88-b324-04b95f76ff65) - Closing
[0m21:13:21.146621 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now create_hive_metastore_snapshots)
[0m21:13:21.146621 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=bb6db4c9-6684-4648-a506-f323ac16f014, name=create_hive_metastore_snapshots, idle-time=0.0s, language=None, compute-name=) - Reusing connection previously named list_hive_metastore
[0m21:13:21.146621 [debug] [ThreadPool]: Creating schema "database: "hive_metastore"
schema: "snapshots"
"
[0m21:13:21.146621 [debug] [ThreadPool]: Using databricks connection "create_hive_metastore_snapshots"
[0m21:13:21.146621 [debug] [ThreadPool]: On create_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "connection_name": "create_hive_metastore_snapshots"} */
create schema if not exists `hive_metastore`.`snapshots`
  
[0m21:13:23.503639 [debug] [ThreadPool]: SQL status: OK in 2.360 seconds
[0m21:13:23.503639 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=bb6db4c9-6684-4648-a506-f323ac16f014, command-id=4316749c-0001-4871-9d93-e59db5e61f5a) - Closing
[0m21:13:23.503639 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_hive_metastore_snapshots, idle-time=0s, language=None, compute-name=) - Creating connection
[0m21:13:23.503639 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_snapshots'
[0m21:13:23.503639 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m21:13:23.503639 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots)
[0m21:13:23.503639 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:13:24.964398 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=365ed37c-062a-4788-a9d3-00eab3c04d44) - Created
[0m21:13:25.640853 [debug] [ThreadPool]: SQL status: OK in 2.140 seconds
[0m21:13:25.654561 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=365ed37c-062a-4788-a9d3-00eab3c04d44, command-id=6ac325fd-34ae-4c0f-a06a-f1d3c116df1e) - Closing
[0m21:13:25.654561 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c4410eeb-fd85-4f17-9011-0109919f3bc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219C8A8FF80>]}
[0m21:13:25.686129 [debug] [Thread-2 (]: Began running node snapshot.azure_dbt_spark.customer_snapshot
[0m21:13:25.686129 [debug] [Thread-1 (]: Began running node snapshot.azure_dbt_spark.address_snapshot
[0m21:13:25.686129 [info ] [Thread-2 (]: 2 of 7 START snapshot snapshots.customer_snapshot .............................. [RUN]
[0m21:13:25.686129 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=snapshot.azure_dbt_spark.customer_snapshot, idle-time=0s, language=None, compute-name=) - Creating connection
[0m21:13:25.686129 [debug] [Thread-2 (]: Acquiring new databricks connection 'snapshot.azure_dbt_spark.customer_snapshot'
[0m21:13:25.686129 [debug] [Thread-2 (]: Began compiling node snapshot.azure_dbt_spark.customer_snapshot
[0m21:13:25.686129 [debug] [Thread-2 (]: Began executing node snapshot.azure_dbt_spark.customer_snapshot
[0m21:13:25.686129 [info ] [Thread-1 (]: 1 of 7 START snapshot snapshots.address_snapshot ............................... [RUN]
[0m21:13:25.719164 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=snapshot.azure_dbt_spark.address_snapshot, idle-time=0s, language=None, compute-name=) - Creating connection
[0m21:13:25.734135 [debug] [Thread-1 (]: Acquiring new databricks connection 'snapshot.azure_dbt_spark.address_snapshot'
[0m21:13:25.734135 [debug] [Thread-1 (]: Began compiling node snapshot.azure_dbt_spark.address_snapshot
[0m21:13:25.738141 [debug] [Thread-1 (]: Began executing node snapshot.azure_dbt_spark.address_snapshot
[0m21:13:25.884521 [debug] [Thread-1 (]: Using databricks connection "snapshot.azure_dbt_spark.address_snapshot"
[0m21:13:25.884521 [debug] [Thread-1 (]: On snapshot.azure_dbt_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.address_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(AddressID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        with source_data as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`saleslt`.`address`
)
select *
from source_data
    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

[0m21:13:25.884521 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m21:13:25.884521 [debug] [Thread-2 (]: Using databricks connection "snapshot.azure_dbt_spark.customer_snapshot"
[0m21:13:25.884521 [debug] [Thread-2 (]: On snapshot.azure_dbt_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.customer_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(CustomerId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        with source_data as (
    select
        CustomerId,
        NameStyle,
        Title,
        FirstName,
        MiddleName,
        LastName,
        Suffix,
        CompanyName,
        SalesPerson,
        EmailAddress,
        Phone,
        PasswordHash,
        PasswordSalt
    from `hive_metastore`.`saleslt`.`customer`
)
select *
from source_data
    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

[0m21:13:25.884521 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:13:26.767108 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=56201866-6ea7-41a5-bf4b-6491fe9397c1) - Created
[0m21:13:27.000650 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=626bbb08-5484-4e7f-bab5-ad0994c50a47) - Created
[0m21:13:28.070372 [debug] [Thread-2 (]: SQL status: OK in 2.190 seconds
[0m21:13:28.070372 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=626bbb08-5484-4e7f-bab5-ad0994c50a47, command-id=57055ef5-f9ed-48a0-95f5-5ff23baa46ae) - Closing
[0m21:13:28.070372 [debug] [Thread-2 (]: Using databricks connection "snapshot.azure_dbt_spark.customer_snapshot"
[0m21:13:28.070372 [debug] [Thread-2 (]: On snapshot.azure_dbt_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.customer_snapshot"} */
select * from (
        select 
    current_timestamp()
 as dbt_snapshot_time
    ) as __dbt_sbq
    where false
    limit 0

[0m21:13:28.290960 [debug] [Thread-1 (]: SQL status: OK in 2.410 seconds
[0m21:13:28.290960 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=56201866-6ea7-41a5-bf4b-6491fe9397c1, command-id=efd82394-5422-4605-93cf-c7c3897dca9c) - Closing
[0m21:13:28.303205 [debug] [Thread-1 (]: Using databricks connection "snapshot.azure_dbt_spark.address_snapshot"
[0m21:13:28.303205 [debug] [Thread-1 (]: On snapshot.azure_dbt_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.address_snapshot"} */
select * from (
        select 
    current_timestamp()
 as dbt_snapshot_time
    ) as __dbt_sbq
    where false
    limit 0

[0m21:13:28.368399 [debug] [Thread-2 (]: SQL status: OK in 0.300 seconds
[0m21:13:28.368399 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=626bbb08-5484-4e7f-bab5-ad0994c50a47, command-id=d0684fbf-930c-4b9a-be83-bd1b394fa502) - Closing
[0m21:13:28.368399 [debug] [Thread-2 (]: Writing runtime sql for node "snapshot.azure_dbt_spark.customer_snapshot"
[0m21:13:28.383012 [debug] [Thread-2 (]: Using databricks connection "snapshot.azure_dbt_spark.customer_snapshot"
[0m21:13:28.383012 [debug] [Thread-2 (]: On snapshot.azure_dbt_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.customer_snapshot"} */

      
  
    
        create or replace table `hive_metastore`.`snapshots`.`customer_snapshot`
      
      using delta
      
      
      
      
      
    location '/mnt/silver/customer/customer_snapshot'
      
      
      as
      
    

    select *,
        md5(coalesce(cast(CustomerId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        with source_data as (
    select
        CustomerId,
        NameStyle,
        Title,
        FirstName,
        MiddleName,
        LastName,
        Suffix,
        CompanyName,
        SalesPerson,
        EmailAddress,
        Phone,
        PasswordHash,
        PasswordSalt
    from `hive_metastore`.`saleslt`.`customer`
)
select *
from source_data
    ) sbq



  
  
[0m21:13:28.596816 [debug] [Thread-1 (]: SQL status: OK in 0.290 seconds
[0m21:13:28.596816 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=56201866-6ea7-41a5-bf4b-6491fe9397c1, command-id=0a4acb62-2842-429c-8437-afc96118fe66) - Closing
[0m21:13:28.596816 [debug] [Thread-1 (]: Writing runtime sql for node "snapshot.azure_dbt_spark.address_snapshot"
[0m21:13:28.608866 [debug] [Thread-1 (]: Using databricks connection "snapshot.azure_dbt_spark.address_snapshot"
[0m21:13:28.608866 [debug] [Thread-1 (]: On snapshot.azure_dbt_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.address_snapshot"} */

      
  
    
        create or replace table `hive_metastore`.`snapshots`.`address_snapshot`
      
      using delta
      
      
      
      
      
    location '/mnt/silver/address/address_snapshot'
      
      
      as
      
    

    select *,
        md5(coalesce(cast(AddressID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        with source_data as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`saleslt`.`address`
)
select *
from source_data
    ) sbq



  
  
[0m21:13:46.357228 [debug] [Thread-1 (]: SQL status: OK in 17.750 seconds
[0m21:13:46.357228 [debug] [Thread-2 (]: SQL status: OK in 17.970 seconds
[0m21:13:46.357228 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=56201866-6ea7-41a5-bf4b-6491fe9397c1, command-id=846fa46c-93ba-4481-bee8-a2240d4bbc6b) - Closing
[0m21:13:46.360769 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=626bbb08-5484-4e7f-bab5-ad0994c50a47, command-id=4042511d-66d4-4475-b320-d1e7b26e4d08) - Closing
[0m21:13:46.698730 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c4410eeb-fd85-4f17-9011-0109919f3bc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219B46DC350>]}
[0m21:13:46.699723 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c4410eeb-fd85-4f17-9011-0109919f3bc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219C8B43F80>]}
[0m21:13:46.699723 [info ] [Thread-1 (]: 1 of 7 OK snapshotted snapshots.address_snapshot ............................... [[32mOK[0m in 20.98s]
[0m21:13:46.701184 [debug] [Thread-1 (]: Finished running node snapshot.azure_dbt_spark.address_snapshot
[0m21:13:46.701184 [info ] [Thread-2 (]: 2 of 7 OK snapshotted snapshots.customer_snapshot .............................. [[32mOK[0m in 21.01s]
[0m21:13:46.702707 [debug] [Thread-1 (]: Began running node snapshot.azure_dbt_spark.customeraddress_snapshot
[0m21:13:46.703722 [debug] [Thread-2 (]: Finished running node snapshot.azure_dbt_spark.customer_snapshot
[0m21:13:46.703722 [debug] [Thread-2 (]: Began running node snapshot.azure_dbt_spark.product_snapshot
[0m21:13:46.703722 [info ] [Thread-1 (]: 3 of 7 START snapshot snapshots.customeraddress_snapshot ....................... [RUN]
[0m21:13:46.704822 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.azure_dbt_spark.address_snapshot, now snapshot.azure_dbt_spark.customeraddress_snapshot)
[0m21:13:46.704822 [info ] [Thread-2 (]: 4 of 7 START snapshot snapshots.product_snapshot ............................... [RUN]
[0m21:13:46.707661 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=56201866-6ea7-41a5-bf4b-6491fe9397c1, name=snapshot.azure_dbt_spark.customeraddress_snapshot, idle-time=0.011903762817382812s, language=sql, compute-name=) - Reusing connection previously named snapshot.azure_dbt_spark.address_snapshot
[0m21:13:46.708999 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly snapshot.azure_dbt_spark.customer_snapshot, now snapshot.azure_dbt_spark.product_snapshot)
[0m21:13:46.708999 [debug] [Thread-1 (]: Began compiling node snapshot.azure_dbt_spark.customeraddress_snapshot
[0m21:13:46.710184 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=626bbb08-5484-4e7f-bab5-ad0994c50a47, name=snapshot.azure_dbt_spark.product_snapshot, idle-time=0.012423276901245117s, language=sql, compute-name=) - Reusing connection previously named snapshot.azure_dbt_spark.customer_snapshot
[0m21:13:46.713900 [debug] [Thread-1 (]: Began executing node snapshot.azure_dbt_spark.customeraddress_snapshot
[0m21:13:46.715824 [debug] [Thread-2 (]: Began compiling node snapshot.azure_dbt_spark.product_snapshot
[0m21:13:46.726122 [debug] [Thread-1 (]: Using databricks connection "snapshot.azure_dbt_spark.customeraddress_snapshot"
[0m21:13:46.729124 [debug] [Thread-2 (]: Began executing node snapshot.azure_dbt_spark.product_snapshot
[0m21:13:46.729124 [debug] [Thread-1 (]: On snapshot.azure_dbt_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.customeraddress_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(CustomerId||'-'||AddressId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        with source_data as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`saleslt`.`customeraddress`
)
select *
from source_data
    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

[0m21:13:46.735791 [debug] [Thread-2 (]: Using databricks connection "snapshot.azure_dbt_spark.product_snapshot"
[0m21:13:46.737747 [debug] [Thread-2 (]: On snapshot.azure_dbt_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.product_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(ProductID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        with product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
)

select * from product_snapshot
    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

[0m21:13:47.286184 [debug] [Thread-1 (]: SQL status: OK in 0.550 seconds
[0m21:13:47.286184 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=56201866-6ea7-41a5-bf4b-6491fe9397c1, command-id=8efd86b3-0afb-45a9-bce4-9a2083321212) - Closing
[0m21:13:47.290662 [debug] [Thread-1 (]: Using databricks connection "snapshot.azure_dbt_spark.customeraddress_snapshot"
[0m21:13:47.290662 [debug] [Thread-1 (]: On snapshot.azure_dbt_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.customeraddress_snapshot"} */
select * from (
        select 
    current_timestamp()
 as dbt_snapshot_time
    ) as __dbt_sbq
    where false
    limit 0

[0m21:13:47.477345 [debug] [Thread-2 (]: SQL status: OK in 0.740 seconds
[0m21:13:47.478348 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=626bbb08-5484-4e7f-bab5-ad0994c50a47, command-id=bd2c06c4-8ce7-4cb9-acfc-6165aceaad0a) - Closing
[0m21:13:47.479345 [debug] [Thread-2 (]: Using databricks connection "snapshot.azure_dbt_spark.product_snapshot"
[0m21:13:47.480365 [debug] [Thread-2 (]: On snapshot.azure_dbt_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.product_snapshot"} */
select * from (
        select 
    current_timestamp()
 as dbt_snapshot_time
    ) as __dbt_sbq
    where false
    limit 0

[0m21:13:47.560432 [debug] [Thread-1 (]: SQL status: OK in 0.270 seconds
[0m21:13:47.565912 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=56201866-6ea7-41a5-bf4b-6491fe9397c1, command-id=409b9a15-6a17-46e3-8bad-96ba8818b506) - Closing
[0m21:13:47.568228 [debug] [Thread-1 (]: Writing runtime sql for node "snapshot.azure_dbt_spark.customeraddress_snapshot"
[0m21:13:47.571757 [debug] [Thread-1 (]: Using databricks connection "snapshot.azure_dbt_spark.customeraddress_snapshot"
[0m21:13:47.572819 [debug] [Thread-1 (]: On snapshot.azure_dbt_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.customeraddress_snapshot"} */

      
  
    
        create or replace table `hive_metastore`.`snapshots`.`customeraddress_snapshot`
      
      using delta
      
      
      
      
      
    location '/mnt/silver/customeraddress/customeraddress_snapshot'
      
      
      as
      
    

    select *,
        md5(coalesce(cast(CustomerId||'-'||AddressId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        with source_data as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`saleslt`.`customeraddress`
)
select *
from source_data
    ) sbq



  
  
[0m21:13:47.901530 [debug] [Thread-2 (]: SQL status: OK in 0.420 seconds
[0m21:13:47.901530 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=626bbb08-5484-4e7f-bab5-ad0994c50a47, command-id=a47ecad2-a689-4eac-9681-0273e571016b) - Closing
[0m21:13:47.902529 [debug] [Thread-2 (]: Writing runtime sql for node "snapshot.azure_dbt_spark.product_snapshot"
[0m21:13:47.904040 [debug] [Thread-2 (]: Using databricks connection "snapshot.azure_dbt_spark.product_snapshot"
[0m21:13:47.905054 [debug] [Thread-2 (]: On snapshot.azure_dbt_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.product_snapshot"} */

      
  
    
        create or replace table `hive_metastore`.`snapshots`.`product_snapshot`
      
      using delta
      
      
      
      
      
    location '/mnt/silver/product/product_snapshot'
      
      
      as
      
    

    select *,
        md5(coalesce(cast(ProductID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        with product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
)

select * from product_snapshot
    ) sbq



  
  
[0m21:13:52.285051 [debug] [Thread-1 (]: SQL status: OK in 4.710 seconds
[0m21:13:52.286769 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=56201866-6ea7-41a5-bf4b-6491fe9397c1, command-id=52bb8a11-8c3e-42a3-8aaa-bd7fa7de8285) - Closing
[0m21:13:52.289470 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c4410eeb-fd85-4f17-9011-0109919f3bc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219C88DBDA0>]}
[0m21:13:52.290974 [info ] [Thread-1 (]: 3 of 7 OK snapshotted snapshots.customeraddress_snapshot ....................... [[32mOK[0m in 5.58s]
[0m21:13:52.291978 [debug] [Thread-1 (]: Finished running node snapshot.azure_dbt_spark.customeraddress_snapshot
[0m21:13:52.291978 [debug] [Thread-1 (]: Began running node snapshot.azure_dbt_spark.productmodel_snapshot
[0m21:13:52.292977 [info ] [Thread-1 (]: 5 of 7 START snapshot snapshots.productmodel_snapshot .......................... [RUN]
[0m21:13:52.294218 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.azure_dbt_spark.customeraddress_snapshot, now snapshot.azure_dbt_spark.productmodel_snapshot)
[0m21:13:52.294218 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=56201866-6ea7-41a5-bf4b-6491fe9397c1, name=snapshot.azure_dbt_spark.productmodel_snapshot, idle-time=0.004747867584228516s, language=sql, compute-name=) - Reusing connection previously named snapshot.azure_dbt_spark.customeraddress_snapshot
[0m21:13:52.294218 [debug] [Thread-1 (]: Began compiling node snapshot.azure_dbt_spark.productmodel_snapshot
[0m21:13:52.298212 [debug] [Thread-1 (]: Began executing node snapshot.azure_dbt_spark.productmodel_snapshot
[0m21:13:52.306334 [debug] [Thread-1 (]: Using databricks connection "snapshot.azure_dbt_spark.productmodel_snapshot"
[0m21:13:52.307392 [debug] [Thread-1 (]: On snapshot.azure_dbt_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.productmodel_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(ProductModelID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        with product_snapshot as (
    SELECT
        ProductModelID,
        Name,
        CatalogDescription
    FROM `hive_metastore`.`saleslt`.`productmodel`
)

select * from product_snapshot
    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

[0m21:13:52.310416 [debug] [Thread-2 (]: SQL status: OK in 4.400 seconds
[0m21:13:52.312443 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=626bbb08-5484-4e7f-bab5-ad0994c50a47, command-id=f2caa8c6-c6f7-4e79-84a7-6365cf497887) - Closing
[0m21:13:52.315431 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c4410eeb-fd85-4f17-9011-0109919f3bc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219C89588F0>]}
[0m21:13:52.315431 [info ] [Thread-2 (]: 4 of 7 OK snapshotted snapshots.product_snapshot ............................... [[32mOK[0m in 5.61s]
[0m21:13:52.317432 [debug] [Thread-2 (]: Finished running node snapshot.azure_dbt_spark.product_snapshot
[0m21:13:52.318941 [debug] [Thread-2 (]: Began running node snapshot.azure_dbt_spark.salesorderdetail_snapshot
[0m21:13:52.320506 [info ] [Thread-2 (]: 6 of 7 START snapshot snapshots.salesorderdetail_snapshot ...................... [RUN]
[0m21:13:52.321518 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly snapshot.azure_dbt_spark.product_snapshot, now snapshot.azure_dbt_spark.salesorderdetail_snapshot)
[0m21:13:52.322520 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=626bbb08-5484-4e7f-bab5-ad0994c50a47, name=snapshot.azure_dbt_spark.salesorderdetail_snapshot, idle-time=0.007089138031005859s, language=sql, compute-name=) - Reusing connection previously named snapshot.azure_dbt_spark.product_snapshot
[0m21:13:52.323519 [debug] [Thread-2 (]: Began compiling node snapshot.azure_dbt_spark.salesorderdetail_snapshot
[0m21:13:52.327258 [debug] [Thread-2 (]: Began executing node snapshot.azure_dbt_spark.salesorderdetail_snapshot
[0m21:13:52.331882 [debug] [Thread-2 (]: Using databricks connection "snapshot.azure_dbt_spark.salesorderdetail_snapshot"
[0m21:13:52.332943 [debug] [Thread-2 (]: On snapshot.azure_dbt_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.salesorderdetail_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(SalesOrderDetailID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`saleslt`.`salesorderdetail`
)

select * from salesorderdetail_snapshot
    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

[0m21:13:52.845222 [debug] [Thread-2 (]: SQL status: OK in 0.510 seconds
[0m21:13:52.846169 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=626bbb08-5484-4e7f-bab5-ad0994c50a47, command-id=5b0d08ae-f4d4-42d3-acc7-ae71843bc648) - Closing
[0m21:13:52.847842 [debug] [Thread-2 (]: Using databricks connection "snapshot.azure_dbt_spark.salesorderdetail_snapshot"
[0m21:13:52.848548 [debug] [Thread-2 (]: On snapshot.azure_dbt_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.salesorderdetail_snapshot"} */
select * from (
        select 
    current_timestamp()
 as dbt_snapshot_time
    ) as __dbt_sbq
    where false
    limit 0

[0m21:13:52.896684 [debug] [Thread-1 (]: SQL status: OK in 0.590 seconds
[0m21:13:52.898686 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=56201866-6ea7-41a5-bf4b-6491fe9397c1, command-id=da4e39b0-b60f-4150-a37f-fa32b3a0282b) - Closing
[0m21:13:52.900596 [debug] [Thread-1 (]: Using databricks connection "snapshot.azure_dbt_spark.productmodel_snapshot"
[0m21:13:52.901718 [debug] [Thread-1 (]: On snapshot.azure_dbt_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.productmodel_snapshot"} */
select * from (
        select 
    current_timestamp()
 as dbt_snapshot_time
    ) as __dbt_sbq
    where false
    limit 0

[0m21:13:53.122307 [debug] [Thread-2 (]: SQL status: OK in 0.270 seconds
[0m21:13:53.123305 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=626bbb08-5484-4e7f-bab5-ad0994c50a47, command-id=fdc5514e-906d-443f-a3be-9f630bcbedb0) - Closing
[0m21:13:53.124296 [debug] [Thread-2 (]: Writing runtime sql for node "snapshot.azure_dbt_spark.salesorderdetail_snapshot"
[0m21:13:53.125207 [debug] [Thread-2 (]: Using databricks connection "snapshot.azure_dbt_spark.salesorderdetail_snapshot"
[0m21:13:53.126259 [debug] [Thread-2 (]: On snapshot.azure_dbt_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.salesorderdetail_snapshot"} */

      
  
    
        create or replace table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
      
      using delta
      
      
      
      
      
    location '/mnt/silver/salesorderdetail/salesorderdetail_snapshot'
      
      
      as
      
    

    select *,
        md5(coalesce(cast(SalesOrderDetailID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`saleslt`.`salesorderdetail`
)

select * from salesorderdetail_snapshot
    ) sbq



  
  
[0m21:13:53.173504 [debug] [Thread-1 (]: SQL status: OK in 0.270 seconds
[0m21:13:53.174528 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=56201866-6ea7-41a5-bf4b-6491fe9397c1, command-id=952b421c-6a4e-4817-ba7c-0da02c6461ef) - Closing
[0m21:13:53.175450 [debug] [Thread-1 (]: Writing runtime sql for node "snapshot.azure_dbt_spark.productmodel_snapshot"
[0m21:13:53.176518 [debug] [Thread-1 (]: Using databricks connection "snapshot.azure_dbt_spark.productmodel_snapshot"
[0m21:13:53.177527 [debug] [Thread-1 (]: On snapshot.azure_dbt_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.productmodel_snapshot"} */

      
  
    
        create or replace table `hive_metastore`.`snapshots`.`productmodel_snapshot`
      
      using delta
      
      
      
      
      
    location '/mnt/silver/productmodel/productmodel_snapshot'
      
      
      as
      
    

    select *,
        md5(coalesce(cast(ProductModelID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        with product_snapshot as (
    SELECT
        ProductModelID,
        Name,
        CatalogDescription
    FROM `hive_metastore`.`saleslt`.`productmodel`
)

select * from product_snapshot
    ) sbq



  
  
[0m21:13:56.842797 [debug] [Thread-2 (]: SQL status: OK in 3.720 seconds
[0m21:13:56.845735 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=626bbb08-5484-4e7f-bab5-ad0994c50a47, command-id=a850903a-b8ed-434f-ba1e-cadb7b95b6f3) - Closing
[0m21:13:56.850712 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c4410eeb-fd85-4f17-9011-0109919f3bc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219C8914320>]}
[0m21:13:56.852903 [info ] [Thread-2 (]: 6 of 7 OK snapshotted snapshots.salesorderdetail_snapshot ...................... [[32mOK[0m in 4.53s]
[0m21:13:56.853903 [debug] [Thread-2 (]: Finished running node snapshot.azure_dbt_spark.salesorderdetail_snapshot
[0m21:13:56.853903 [debug] [Thread-2 (]: Began running node snapshot.azure_dbt_spark.salesorderheader_snapshot
[0m21:13:56.855219 [info ] [Thread-2 (]: 7 of 7 START snapshot snapshots.salesorderheader_snapshot ...................... [RUN]
[0m21:13:56.856573 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly snapshot.azure_dbt_spark.salesorderdetail_snapshot, now snapshot.azure_dbt_spark.salesorderheader_snapshot)
[0m21:13:56.857545 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=626bbb08-5484-4e7f-bab5-ad0994c50a47, name=snapshot.azure_dbt_spark.salesorderheader_snapshot, idle-time=0.006833314895629883s, language=sql, compute-name=) - Reusing connection previously named snapshot.azure_dbt_spark.salesorderdetail_snapshot
[0m21:13:56.857545 [debug] [Thread-2 (]: Began compiling node snapshot.azure_dbt_spark.salesorderheader_snapshot
[0m21:13:56.862246 [debug] [Thread-2 (]: Began executing node snapshot.azure_dbt_spark.salesorderheader_snapshot
[0m21:13:56.868163 [debug] [Thread-2 (]: Using databricks connection "snapshot.azure_dbt_spark.salesorderheader_snapshot"
[0m21:13:56.869167 [debug] [Thread-2 (]: On snapshot.azure_dbt_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.salesorderheader_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(SalesOrderID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        with salesorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
)

select * from salesorderheader_snapshot
    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

[0m21:13:57.338981 [debug] [Thread-2 (]: SQL status: OK in 0.470 seconds
[0m21:13:57.338981 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=626bbb08-5484-4e7f-bab5-ad0994c50a47, command-id=b7f6fdda-7f42-4e4e-a94c-f5d00f57b1dc) - Closing
[0m21:13:57.340579 [debug] [Thread-2 (]: Using databricks connection "snapshot.azure_dbt_spark.salesorderheader_snapshot"
[0m21:13:57.341634 [debug] [Thread-2 (]: On snapshot.azure_dbt_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.salesorderheader_snapshot"} */
select * from (
        select 
    current_timestamp()
 as dbt_snapshot_time
    ) as __dbt_sbq
    where false
    limit 0

[0m21:13:57.627775 [debug] [Thread-2 (]: SQL status: OK in 0.290 seconds
[0m21:13:57.627775 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=626bbb08-5484-4e7f-bab5-ad0994c50a47, command-id=69c0f6a6-17e4-4a2a-8860-030226eab17b) - Closing
[0m21:13:57.628769 [debug] [Thread-2 (]: Writing runtime sql for node "snapshot.azure_dbt_spark.salesorderheader_snapshot"
[0m21:13:57.630738 [debug] [Thread-2 (]: Using databricks connection "snapshot.azure_dbt_spark.salesorderheader_snapshot"
[0m21:13:57.630738 [debug] [Thread-2 (]: On snapshot.azure_dbt_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.salesorderheader_snapshot"} */

      
  
    
        create or replace table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
      
      using delta
      
      
      
      
      
    location '/mnt/silver/salesorderheader/salesorderheader_snapshot'
      
      
      as
      
    

    select *,
        md5(coalesce(cast(SalesOrderID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        with salesorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
)

select * from salesorderheader_snapshot
    ) sbq



  
  
[0m21:13:57.635937 [debug] [Thread-1 (]: SQL status: OK in 4.460 seconds
[0m21:13:57.638049 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=56201866-6ea7-41a5-bf4b-6491fe9397c1, command-id=e9fc69ea-17cb-457e-9d59-7dbb9ba86db1) - Closing
[0m21:13:57.642783 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c4410eeb-fd85-4f17-9011-0109919f3bc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219C8877D70>]}
[0m21:13:57.644795 [info ] [Thread-1 (]: 5 of 7 OK snapshotted snapshots.productmodel_snapshot .......................... [[32mOK[0m in 5.35s]
[0m21:13:57.645787 [debug] [Thread-1 (]: Finished running node snapshot.azure_dbt_spark.productmodel_snapshot
[0m21:14:00.967170 [debug] [Thread-2 (]: SQL status: OK in 3.330 seconds
[0m21:14:00.969239 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=626bbb08-5484-4e7f-bab5-ad0994c50a47, command-id=2d89fad4-46be-489e-b2f7-df8ee84c4312) - Closing
[0m21:14:00.973960 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c4410eeb-fd85-4f17-9011-0109919f3bc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219C8710200>]}
[0m21:14:00.975962 [info ] [Thread-2 (]: 7 of 7 OK snapshotted snapshots.salesorderheader_snapshot ...................... [[32mOK[0m in 4.12s]
[0m21:14:00.978005 [debug] [Thread-2 (]: Finished running node snapshot.azure_dbt_spark.salesorderheader_snapshot
[0m21:14:00.980489 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=35.32592797279358s, language=None, compute-name=) - Reusing connection previously named master
[0m21:14:00.981512 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:14:00.981512 [debug] [MainThread]: Connection 'create_hive_metastore_snapshots' was properly closed.
[0m21:14:00.981512 [debug] [MainThread]: On create_hive_metastore_snapshots: Close
[0m21:14:00.982504 [debug] [MainThread]: Databricks adapter: Connection(session-id=bb6db4c9-6684-4648-a506-f323ac16f014) - Closing
[0m21:14:01.430235 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m21:14:01.431776 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m21:14:01.432802 [debug] [MainThread]: Databricks adapter: Connection(session-id=365ed37c-062a-4788-a9d3-00eab3c04d44) - Closing
[0m21:14:01.644373 [debug] [MainThread]: Connection 'snapshot.azure_dbt_spark.salesorderheader_snapshot' was properly closed.
[0m21:14:01.645368 [debug] [MainThread]: On snapshot.azure_dbt_spark.salesorderheader_snapshot: Close
[0m21:14:01.646366 [debug] [MainThread]: Databricks adapter: Connection(session-id=626bbb08-5484-4e7f-bab5-ad0994c50a47) - Closing
[0m21:14:01.865381 [debug] [MainThread]: Connection 'snapshot.azure_dbt_spark.productmodel_snapshot' was properly closed.
[0m21:14:01.866378 [debug] [MainThread]: On snapshot.azure_dbt_spark.productmodel_snapshot: Close
[0m21:14:01.867374 [debug] [MainThread]: Databricks adapter: Connection(session-id=56201866-6ea7-41a5-bf4b-6491fe9397c1) - Closing
[0m21:14:02.067691 [info ] [MainThread]: 
[0m21:14:02.068689 [info ] [MainThread]: Finished running 7 snapshots in 0 hours 0 minutes and 43.41 seconds (43.41s).
[0m21:14:02.070784 [debug] [MainThread]: Command end result
[0m21:14:02.108827 [debug] [MainThread]: Wrote artifact WritableManifest to H:\Acadmy\My\Projects\Azure_Spark_DBT\azure_dbt_spark\target\manifest.json
[0m21:14:02.112536 [debug] [MainThread]: Wrote artifact SemanticManifest to H:\Acadmy\My\Projects\Azure_Spark_DBT\azure_dbt_spark\target\semantic_manifest.json
[0m21:14:02.120661 [debug] [MainThread]: Wrote artifact RunExecutionResult to H:\Acadmy\My\Projects\Azure_Spark_DBT\azure_dbt_spark\target\run_results.json
[0m21:14:02.121672 [info ] [MainThread]: 
[0m21:14:02.121672 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:14:02.122672 [info ] [MainThread]: 
[0m21:14:02.123672 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m21:14:02.124720 [debug] [MainThread]: Command `dbt snapshot` succeeded at 21:14:02.124720 after 46.99 seconds
[0m21:14:02.124720 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219B4D37AD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219B4D36F60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219B4AA3500>]}
[0m21:14:02.125668 [debug] [MainThread]: Flushing usage events
[0m21:14:02.960582 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:19:47.243845 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B132D64860>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B132E08830>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B132B05430>]}


============================== 21:19:47.252528 | 5ffdccfb-3c00-4464-99af-1dbd6988c517 ==============================
[0m21:19:47.252528 [info ] [MainThread]: Running with dbt=1.9.4
[0m21:19:47.254550 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\morsi\\.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': 'H:\\Acadmy\\My\\Projects\\Azure_Spark_DBT\\azure_dbt_spark\\logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'invocation_command': 'dbt snapshot', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m21:19:48.210768 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m21:19:48.210768 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m21:19:48.211839 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m21:19:48.925903 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5ffdccfb-3c00-4464-99af-1dbd6988c517', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B145C80F50>]}
[0m21:19:48.976917 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5ffdccfb-3c00-4464-99af-1dbd6988c517', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B14608CAD0>]}
[0m21:19:48.978295 [info ] [MainThread]: Registered adapter: databricks=1.10.1
[0m21:19:49.267239 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m21:19:49.391117 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:19:49.391117 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:19:49.397820 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.azure_dbt_spark.example
[0m21:19:49.424157 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5ffdccfb-3c00-4464-99af-1dbd6988c517', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B1461F1430>]}
[0m21:19:49.488452 [debug] [MainThread]: Wrote artifact WritableManifest to H:\Acadmy\My\Projects\Azure_Spark_DBT\azure_dbt_spark\target\manifest.json
[0m21:19:49.491453 [debug] [MainThread]: Wrote artifact SemanticManifest to H:\Acadmy\My\Projects\Azure_Spark_DBT\azure_dbt_spark\target\semantic_manifest.json
[0m21:19:49.512929 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5ffdccfb-3c00-4464-99af-1dbd6988c517', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B1463E22A0>]}
[0m21:19:49.513951 [info ] [MainThread]: Found 7 snapshots, 9 sources, 660 macros
[0m21:19:49.514930 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5ffdccfb-3c00-4464-99af-1dbd6988c517', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B1319BE210>]}
[0m21:19:49.516445 [info ] [MainThread]: 
[0m21:19:49.517453 [info ] [MainThread]: Concurrency: 2 threads (target='dev')
[0m21:19:49.518346 [info ] [MainThread]: 
[0m21:19:49.518851 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m21:19:49.518851 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m21:19:49.524601 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_hive_metastore, idle-time=0s, language=None, compute-name=) - Creating connection
[0m21:19:49.524601 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m21:19:49.524601 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m21:19:49.525978 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m21:19:49.525978 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:19:50.449052 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=0e82b395-938b-4662-83af-fd7678d185d9) - Created
[0m21:19:50.676052 [debug] [ThreadPool]: SQL status: OK in 1.150 seconds
[0m21:19:50.678321 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0e82b395-938b-4662-83af-fd7678d185d9, command-id=b1acc4e9-b30d-42c9-b965-bac8a62f9d75) - Closing
[0m21:19:50.680328 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_hive_metastore_snapshots, idle-time=0s, language=None, compute-name=) - Creating connection
[0m21:19:50.681327 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_snapshots'
[0m21:19:50.681327 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m21:19:50.681327 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots)
[0m21:19:50.682325 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:19:51.464414 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=41641183-cfb6-4379-af43-47a36da7784a) - Created
[0m21:19:51.741184 [debug] [ThreadPool]: SQL status: OK in 1.060 seconds
[0m21:19:51.742187 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=41641183-cfb6-4379-af43-47a36da7784a, command-id=3b137336-50d7-4388-83b8-42726e345461) - Closing
[0m21:19:51.754864 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m21:19:51.754864 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

    
SELECT current_catalog()

  
[0m21:19:52.656878 [debug] [ThreadPool]: SQL status: OK in 0.900 seconds
[0m21:19:52.672599 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=41641183-cfb6-4379-af43-47a36da7784a, command-id=7bf8e475-f9bc-4178-944b-8e659a86fdf0) - Closing
[0m21:19:52.682948 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m21:19:52.682948 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

    
SHOW VIEWS IN `hive_metastore`.`snapshots`

  
[0m21:19:53.040458 [debug] [ThreadPool]: SQL status: OK in 0.360 seconds
[0m21:19:53.045345 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=41641183-cfb6-4379-af43-47a36da7784a, command-id=55f8fade-1eef-4c73-a650-bc3abbf17129) - Closing
[0m21:19:53.049428 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5ffdccfb-3c00-4464-99af-1dbd6988c517', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B1301BD760>]}
[0m21:19:53.055768 [debug] [Thread-1 (]: Began running node snapshot.azure_dbt_spark.address_snapshot
[0m21:19:53.055768 [debug] [Thread-2 (]: Began running node snapshot.azure_dbt_spark.customer_snapshot
[0m21:19:53.056775 [info ] [Thread-1 (]: 1 of 7 START snapshot snapshots.address_snapshot ............................... [RUN]
[0m21:19:53.057775 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=snapshot.azure_dbt_spark.address_snapshot, idle-time=0s, language=None, compute-name=) - Creating connection
[0m21:19:53.056775 [info ] [Thread-2 (]: 2 of 7 START snapshot snapshots.customer_snapshot .............................. [RUN]
[0m21:19:53.057775 [debug] [Thread-1 (]: Acquiring new databricks connection 'snapshot.azure_dbt_spark.address_snapshot'
[0m21:19:53.058776 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=snapshot.azure_dbt_spark.customer_snapshot, idle-time=0s, language=None, compute-name=) - Creating connection
[0m21:19:53.058776 [debug] [Thread-1 (]: Began compiling node snapshot.azure_dbt_spark.address_snapshot
[0m21:19:53.059773 [debug] [Thread-2 (]: Acquiring new databricks connection 'snapshot.azure_dbt_spark.customer_snapshot'
[0m21:19:53.068774 [debug] [Thread-1 (]: Began executing node snapshot.azure_dbt_spark.address_snapshot
[0m21:19:53.070850 [debug] [Thread-2 (]: Began compiling node snapshot.azure_dbt_spark.customer_snapshot
[0m21:19:53.087268 [debug] [Thread-2 (]: Began executing node snapshot.azure_dbt_spark.customer_snapshot
[0m21:19:53.137186 [debug] [Thread-2 (]: Using databricks connection "snapshot.azure_dbt_spark.customer_snapshot"
[0m21:19:53.137186 [debug] [Thread-1 (]: Using databricks connection "snapshot.azure_dbt_spark.address_snapshot"
[0m21:19:53.137186 [debug] [Thread-2 (]: On snapshot.azure_dbt_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.customer_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customer_snapshot`
  
[0m21:19:53.138179 [debug] [Thread-1 (]: On snapshot.azure_dbt_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.address_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m21:19:53.138179 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:19:53.138179 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m21:19:53.906996 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=48cd769e-da6e-4612-ac0a-66b1570a6680) - Created
[0m21:19:53.909983 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=f40e06b6-1372-4e2a-8180-e860a3ae3429) - Created
[0m21:19:54.469871 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.address_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`address_snapshot`
  
: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/address/address_snapshot doesn't exist, or is not a Delta table.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_PATH_DOES_NOT_EXIST] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/address/address_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:840)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:662)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:737)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:91)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:195)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:483)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:469)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:519)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/address/address_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:810)
	... 42 more
, operation-id=9f91ebdb-cb93-4c75-b7a7-2bdbcc480895
[0m21:19:54.470867 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
macro get_columns_in_relation_raw
: Database Error
  [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/address/address_snapshot doesn't exist, or is not a Delta table.
[0m21:19:54.473867 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.customer_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customer_snapshot`
  
: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customer/customer_snapshot doesn't exist, or is not a Delta table.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_PATH_DOES_NOT_EXIST] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customer/customer_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:840)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:662)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:737)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:91)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:195)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:483)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:469)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:519)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customer/customer_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:810)
	... 42 more
, operation-id=bbc67918-021c-49d8-9913-2315822f96cc
[0m21:19:54.474868 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
macro get_columns_in_relation_raw
: Database Error
  [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customer/customer_snapshot doesn't exist, or is not a Delta table.
[0m21:19:54.497709 [debug] [Thread-1 (]: Database Error in snapshot address_snapshot (snapshots\address.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/address/address_snapshot doesn't exist, or is not a Delta table.
[0m21:19:54.499740 [debug] [Thread-2 (]: Database Error in snapshot customer_snapshot (snapshots\customer.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customer/customer_snapshot doesn't exist, or is not a Delta table.
[0m21:19:54.502748 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5ffdccfb-3c00-4464-99af-1dbd6988c517', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B1466BA570>]}
[0m21:19:54.502748 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5ffdccfb-3c00-4464-99af-1dbd6988c517', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B146AF4350>]}
[0m21:19:54.503761 [error] [Thread-1 (]: 1 of 7 ERROR snapshotting snapshots.address_snapshot ........................... [[31mERROR[0m in 1.44s]
[0m21:19:54.504756 [debug] [Thread-1 (]: Finished running node snapshot.azure_dbt_spark.address_snapshot
[0m21:19:54.504756 [error] [Thread-2 (]: 2 of 7 ERROR snapshotting snapshots.customer_snapshot .......................... [[31mERROR[0m in 1.44s]
[0m21:19:54.505831 [debug] [Thread-1 (]: Began running node snapshot.azure_dbt_spark.customeraddress_snapshot
[0m21:19:54.505831 [debug] [Thread-5 (]: Marking all children of 'snapshot.azure_dbt_spark.address_snapshot' to be skipped because of status 'error'.  Reason: Database Error in snapshot address_snapshot (snapshots\address.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/address/address_snapshot doesn't exist, or is not a Delta table..
[0m21:19:54.506828 [debug] [Thread-2 (]: Finished running node snapshot.azure_dbt_spark.customer_snapshot
[0m21:19:54.506828 [info ] [Thread-1 (]: 3 of 7 START snapshot snapshots.customeraddress_snapshot ....................... [RUN]
[0m21:19:54.507838 [debug] [Thread-2 (]: Began running node snapshot.azure_dbt_spark.product_snapshot
[0m21:19:54.508828 [debug] [Thread-5 (]: Marking all children of 'snapshot.azure_dbt_spark.customer_snapshot' to be skipped because of status 'error'.  Reason: Database Error in snapshot customer_snapshot (snapshots\customer.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customer/customer_snapshot doesn't exist, or is not a Delta table..
[0m21:19:54.508828 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.azure_dbt_spark.address_snapshot, now snapshot.azure_dbt_spark.customeraddress_snapshot)
[0m21:19:54.509745 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=48cd769e-da6e-4612-ac0a-66b1570a6680, name=snapshot.azure_dbt_spark.customeraddress_snapshot, idle-time=0.009001016616821289s, language=sql, compute-name=) - Reusing connection previously named snapshot.azure_dbt_spark.address_snapshot
[0m21:19:54.509745 [info ] [Thread-2 (]: 4 of 7 START snapshot snapshots.product_snapshot ............................... [RUN]
[0m21:19:54.510796 [debug] [Thread-1 (]: Began compiling node snapshot.azure_dbt_spark.customeraddress_snapshot
[0m21:19:54.510796 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly snapshot.azure_dbt_spark.customer_snapshot, now snapshot.azure_dbt_spark.product_snapshot)
[0m21:19:54.516405 [debug] [Thread-1 (]: Began executing node snapshot.azure_dbt_spark.customeraddress_snapshot
[0m21:19:54.517404 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=f40e06b6-1372-4e2a-8180-e860a3ae3429, name=snapshot.azure_dbt_spark.product_snapshot, idle-time=0.014658689498901367s, language=sql, compute-name=) - Reusing connection previously named snapshot.azure_dbt_spark.customer_snapshot
[0m21:19:54.523415 [debug] [Thread-1 (]: Using databricks connection "snapshot.azure_dbt_spark.customeraddress_snapshot"
[0m21:19:54.524412 [debug] [Thread-2 (]: Began compiling node snapshot.azure_dbt_spark.product_snapshot
[0m21:19:54.524412 [debug] [Thread-1 (]: On snapshot.azure_dbt_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.customeraddress_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m21:19:54.528657 [debug] [Thread-2 (]: Began executing node snapshot.azure_dbt_spark.product_snapshot
[0m21:19:54.533638 [debug] [Thread-2 (]: Using databricks connection "snapshot.azure_dbt_spark.product_snapshot"
[0m21:19:54.534649 [debug] [Thread-2 (]: On snapshot.azure_dbt_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.product_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`product_snapshot`
  
[0m21:19:54.870472 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.customeraddress_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customeraddress/customeraddress_snapshot doesn't exist, or is not a Delta table.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_PATH_DOES_NOT_EXIST] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customeraddress/customeraddress_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:840)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:662)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:737)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:91)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:195)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:483)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:469)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:519)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customeraddress/customeraddress_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:810)
	... 42 more
, operation-id=cb5b68f2-19f0-4a6e-808d-d056725b06e6
[0m21:19:54.874541 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.product_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`product_snapshot`
  
: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/product/product_snapshot doesn't exist, or is not a Delta table.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_PATH_DOES_NOT_EXIST] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/product/product_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:840)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:662)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:737)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:91)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:195)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:483)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:469)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:519)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/product/product_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:810)
	... 42 more
, operation-id=e3075aa2-71b4-48b8-b45e-b51a0634f517
[0m21:19:54.874541 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
macro get_columns_in_relation_raw
: Database Error
  [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customeraddress/customeraddress_snapshot doesn't exist, or is not a Delta table.
[0m21:19:54.875507 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
macro get_columns_in_relation_raw
: Database Error
  [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/product/product_snapshot doesn't exist, or is not a Delta table.
[0m21:19:54.879939 [debug] [Thread-1 (]: Database Error in snapshot customeraddress_snapshot (snapshots\customeraddress.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customeraddress/customeraddress_snapshot doesn't exist, or is not a Delta table.
[0m21:19:54.881973 [debug] [Thread-2 (]: Database Error in snapshot product_snapshot (snapshots\product.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/product/product_snapshot doesn't exist, or is not a Delta table.
[0m21:19:54.882964 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5ffdccfb-3c00-4464-99af-1dbd6988c517', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B1466F4E00>]}
[0m21:19:54.882964 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5ffdccfb-3c00-4464-99af-1dbd6988c517', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B1466F4A70>]}
[0m21:19:54.883962 [error] [Thread-1 (]: 3 of 7 ERROR snapshotting snapshots.customeraddress_snapshot ................... [[31mERROR[0m in 0.37s]
[0m21:19:54.884889 [debug] [Thread-1 (]: Finished running node snapshot.azure_dbt_spark.customeraddress_snapshot
[0m21:19:54.884889 [error] [Thread-2 (]: 4 of 7 ERROR snapshotting snapshots.product_snapshot ........................... [[31mERROR[0m in 0.37s]
[0m21:19:54.885882 [debug] [Thread-1 (]: Began running node snapshot.azure_dbt_spark.productmodel_snapshot
[0m21:19:54.885882 [debug] [Thread-5 (]: Marking all children of 'snapshot.azure_dbt_spark.customeraddress_snapshot' to be skipped because of status 'error'.  Reason: Database Error in snapshot customeraddress_snapshot (snapshots\customeraddress.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customeraddress/customeraddress_snapshot doesn't exist, or is not a Delta table..
[0m21:19:54.886971 [debug] [Thread-2 (]: Finished running node snapshot.azure_dbt_spark.product_snapshot
[0m21:19:54.886971 [debug] [Thread-2 (]: Began running node snapshot.azure_dbt_spark.salesorderdetail_snapshot
[0m21:19:54.886971 [info ] [Thread-1 (]: 5 of 7 START snapshot snapshots.productmodel_snapshot .......................... [RUN]
[0m21:19:54.887936 [debug] [Thread-5 (]: Marking all children of 'snapshot.azure_dbt_spark.product_snapshot' to be skipped because of status 'error'.  Reason: Database Error in snapshot product_snapshot (snapshots\product.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/product/product_snapshot doesn't exist, or is not a Delta table..
[0m21:19:54.888943 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.azure_dbt_spark.customeraddress_snapshot, now snapshot.azure_dbt_spark.productmodel_snapshot)
[0m21:19:54.887936 [info ] [Thread-2 (]: 6 of 7 START snapshot snapshots.salesorderdetail_snapshot ...................... [RUN]
[0m21:19:54.888943 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=48cd769e-da6e-4612-ac0a-66b1570a6680, name=snapshot.azure_dbt_spark.productmodel_snapshot, idle-time=0.0069696903228759766s, language=sql, compute-name=) - Reusing connection previously named snapshot.azure_dbt_spark.customeraddress_snapshot
[0m21:19:54.888943 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly snapshot.azure_dbt_spark.product_snapshot, now snapshot.azure_dbt_spark.salesorderdetail_snapshot)
[0m21:19:54.889976 [debug] [Thread-1 (]: Began compiling node snapshot.azure_dbt_spark.productmodel_snapshot
[0m21:19:54.889976 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=f40e06b6-1372-4e2a-8180-e860a3ae3429, name=snapshot.azure_dbt_spark.salesorderdetail_snapshot, idle-time=0.007012367248535156s, language=sql, compute-name=) - Reusing connection previously named snapshot.azure_dbt_spark.product_snapshot
[0m21:19:54.892962 [debug] [Thread-1 (]: Began executing node snapshot.azure_dbt_spark.productmodel_snapshot
[0m21:19:54.893877 [debug] [Thread-2 (]: Began compiling node snapshot.azure_dbt_spark.salesorderdetail_snapshot
[0m21:19:54.897878 [debug] [Thread-1 (]: Using databricks connection "snapshot.azure_dbt_spark.productmodel_snapshot"
[0m21:19:54.905642 [debug] [Thread-2 (]: Began executing node snapshot.azure_dbt_spark.salesorderdetail_snapshot
[0m21:19:54.906627 [debug] [Thread-1 (]: On snapshot.azure_dbt_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.productmodel_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m21:19:54.911624 [debug] [Thread-2 (]: Using databricks connection "snapshot.azure_dbt_spark.salesorderdetail_snapshot"
[0m21:19:54.912619 [debug] [Thread-2 (]: On snapshot.azure_dbt_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.salesorderdetail_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m21:19:55.217169 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.productmodel_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/productmodel/productmodel_snapshot doesn't exist, or is not a Delta table.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_PATH_DOES_NOT_EXIST] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/productmodel/productmodel_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:840)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:662)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:737)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:91)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:195)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:483)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:469)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:519)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/productmodel/productmodel_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:810)
	... 42 more
, operation-id=2aa64654-7958-423b-899d-b257f3763c10
[0m21:19:55.219157 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
macro get_columns_in_relation_raw
: Database Error
  [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/productmodel/productmodel_snapshot doesn't exist, or is not a Delta table.
[0m21:19:55.227179 [debug] [Thread-1 (]: Database Error in snapshot productmodel_snapshot (snapshots\productmodel.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/productmodel/productmodel_snapshot doesn't exist, or is not a Delta table.
[0m21:19:55.227179 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5ffdccfb-3c00-4464-99af-1dbd6988c517', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B147B86540>]}
[0m21:19:55.228264 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.salesorderdetail_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderdetail/salesorderdetail_snapshot doesn't exist, or is not a Delta table.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_PATH_DOES_NOT_EXIST] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderdetail/salesorderdetail_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:840)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:662)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:737)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:91)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:195)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:483)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:469)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:519)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderdetail/salesorderdetail_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:810)
	... 42 more
, operation-id=f22ed87b-f469-42e3-ad66-0e8ccacd202d
[0m21:19:55.230271 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
macro get_columns_in_relation_raw
: Database Error
  [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderdetail/salesorderdetail_snapshot doesn't exist, or is not a Delta table.
[0m21:19:55.229264 [error] [Thread-1 (]: 5 of 7 ERROR snapshotting snapshots.productmodel_snapshot ...................... [[31mERROR[0m in 0.34s]
[0m21:19:55.231179 [debug] [Thread-1 (]: Finished running node snapshot.azure_dbt_spark.productmodel_snapshot
[0m21:19:55.232181 [debug] [Thread-1 (]: Began running node snapshot.azure_dbt_spark.salesorderheader_snapshot
[0m21:19:55.232181 [debug] [Thread-5 (]: Marking all children of 'snapshot.azure_dbt_spark.productmodel_snapshot' to be skipped because of status 'error'.  Reason: Database Error in snapshot productmodel_snapshot (snapshots\productmodel.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/productmodel/productmodel_snapshot doesn't exist, or is not a Delta table..
[0m21:19:55.233273 [info ] [Thread-1 (]: 7 of 7 START snapshot snapshots.salesorderheader_snapshot ...................... [RUN]
[0m21:19:55.233273 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.azure_dbt_spark.productmodel_snapshot, now snapshot.azure_dbt_spark.salesorderheader_snapshot)
[0m21:19:55.234258 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=48cd769e-da6e-4612-ac0a-66b1570a6680, name=snapshot.azure_dbt_spark.salesorderheader_snapshot, idle-time=0.007079362869262695s, language=sql, compute-name=) - Reusing connection previously named snapshot.azure_dbt_spark.productmodel_snapshot
[0m21:19:55.234258 [debug] [Thread-1 (]: Began compiling node snapshot.azure_dbt_spark.salesorderheader_snapshot
[0m21:19:55.238269 [debug] [Thread-1 (]: Began executing node snapshot.azure_dbt_spark.salesorderheader_snapshot
[0m21:19:55.243173 [debug] [Thread-1 (]: Using databricks connection "snapshot.azure_dbt_spark.salesorderheader_snapshot"
[0m21:19:55.244173 [debug] [Thread-1 (]: On snapshot.azure_dbt_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.salesorderheader_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m21:19:55.248178 [debug] [Thread-2 (]: Database Error in snapshot salesorderdetail_snapshot (snapshots\salesorderdetail.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderdetail/salesorderdetail_snapshot doesn't exist, or is not a Delta table.
[0m21:19:55.249179 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5ffdccfb-3c00-4464-99af-1dbd6988c517', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B1466FAC00>]}
[0m21:19:55.250176 [error] [Thread-2 (]: 6 of 7 ERROR snapshotting snapshots.salesorderdetail_snapshot .................. [[31mERROR[0m in 0.36s]
[0m21:19:55.251278 [debug] [Thread-2 (]: Finished running node snapshot.azure_dbt_spark.salesorderdetail_snapshot
[0m21:19:55.251278 [debug] [Thread-5 (]: Marking all children of 'snapshot.azure_dbt_spark.salesorderdetail_snapshot' to be skipped because of status 'error'.  Reason: Database Error in snapshot salesorderdetail_snapshot (snapshots\salesorderdetail.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderdetail/salesorderdetail_snapshot doesn't exist, or is not a Delta table..
[0m21:19:55.543447 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.salesorderheader_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderheader/salesorderheader_snapshot doesn't exist, or is not a Delta table.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_PATH_DOES_NOT_EXIST] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderheader/salesorderheader_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:840)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:662)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:737)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:91)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:195)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:483)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:469)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:519)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderheader/salesorderheader_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:810)
	... 42 more
, operation-id=32583e3e-34ec-4417-9a8f-98bcb40c6519
[0m21:19:55.545447 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
macro get_columns_in_relation_raw
: Database Error
  [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderheader/salesorderheader_snapshot doesn't exist, or is not a Delta table.
[0m21:19:55.555958 [debug] [Thread-1 (]: Database Error in snapshot salesorderheader_snapshot (snapshots\salesorderheader.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderheader/salesorderheader_snapshot doesn't exist, or is not a Delta table.
[0m21:19:55.556954 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5ffdccfb-3c00-4464-99af-1dbd6988c517', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B147B80A40>]}
[0m21:19:55.557960 [error] [Thread-1 (]: 7 of 7 ERROR snapshotting snapshots.salesorderheader_snapshot .................. [[31mERROR[0m in 0.32s]
[0m21:19:55.559143 [debug] [Thread-1 (]: Finished running node snapshot.azure_dbt_spark.salesorderheader_snapshot
[0m21:19:55.560163 [debug] [Thread-5 (]: Marking all children of 'snapshot.azure_dbt_spark.salesorderheader_snapshot' to be skipped because of status 'error'.  Reason: Database Error in snapshot salesorderheader_snapshot (snapshots\salesorderheader.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderheader/salesorderheader_snapshot doesn't exist, or is not a Delta table..
[0m21:19:55.562175 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=2.512746572494507s, language=None, compute-name=) - Reusing connection previously named master
[0m21:19:55.563280 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:19:55.564270 [debug] [MainThread]: Connection 'list_hive_metastore' was properly closed.
[0m21:19:55.566157 [debug] [MainThread]: On list_hive_metastore: Close
[0m21:19:55.566157 [debug] [MainThread]: Databricks adapter: Connection(session-id=0e82b395-938b-4662-83af-fd7678d185d9) - Closing
[0m21:19:55.754992 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m21:19:55.755993 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m21:19:55.756994 [debug] [MainThread]: Databricks adapter: Connection(session-id=41641183-cfb6-4379-af43-47a36da7784a) - Closing
[0m21:19:55.944822 [debug] [MainThread]: Connection 'snapshot.azure_dbt_spark.salesorderheader_snapshot' was properly closed.
[0m21:19:55.945818 [debug] [MainThread]: On snapshot.azure_dbt_spark.salesorderheader_snapshot: Close
[0m21:19:55.946817 [debug] [MainThread]: Databricks adapter: Connection(session-id=48cd769e-da6e-4612-ac0a-66b1570a6680) - Closing
[0m21:19:56.140197 [debug] [MainThread]: Connection 'snapshot.azure_dbt_spark.salesorderdetail_snapshot' was properly closed.
[0m21:19:56.141197 [debug] [MainThread]: On snapshot.azure_dbt_spark.salesorderdetail_snapshot: Close
[0m21:19:56.142198 [debug] [MainThread]: Databricks adapter: Connection(session-id=f40e06b6-1372-4e2a-8180-e860a3ae3429) - Closing
[0m21:19:56.354856 [info ] [MainThread]: 
[0m21:19:56.355865 [info ] [MainThread]: Finished running 7 snapshots in 0 hours 0 minutes and 6.84 seconds (6.84s).
[0m21:19:56.358966 [debug] [MainThread]: Command end result
[0m21:19:56.387132 [debug] [MainThread]: Wrote artifact WritableManifest to H:\Acadmy\My\Projects\Azure_Spark_DBT\azure_dbt_spark\target\manifest.json
[0m21:19:56.389134 [debug] [MainThread]: Wrote artifact SemanticManifest to H:\Acadmy\My\Projects\Azure_Spark_DBT\azure_dbt_spark\target\semantic_manifest.json
[0m21:19:56.395248 [debug] [MainThread]: Wrote artifact RunExecutionResult to H:\Acadmy\My\Projects\Azure_Spark_DBT\azure_dbt_spark\target\run_results.json
[0m21:19:56.395248 [info ] [MainThread]: 
[0m21:19:56.396140 [info ] [MainThread]: [31mCompleted with 7 errors, 0 partial successes, and 0 warnings:[0m
[0m21:19:56.397133 [info ] [MainThread]: 
[0m21:19:56.397133 [error] [MainThread]:   Database Error in snapshot address_snapshot (snapshots\address.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/address/address_snapshot doesn't exist, or is not a Delta table.
[0m21:19:56.398138 [info ] [MainThread]: 
[0m21:19:56.398138 [error] [MainThread]:   Database Error in snapshot customer_snapshot (snapshots\customer.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customer/customer_snapshot doesn't exist, or is not a Delta table.
[0m21:19:56.399139 [info ] [MainThread]: 
[0m21:19:56.400215 [error] [MainThread]:   Database Error in snapshot customeraddress_snapshot (snapshots\customeraddress.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customeraddress/customeraddress_snapshot doesn't exist, or is not a Delta table.
[0m21:19:56.401139 [info ] [MainThread]: 
[0m21:19:56.402134 [error] [MainThread]:   Database Error in snapshot product_snapshot (snapshots\product.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/product/product_snapshot doesn't exist, or is not a Delta table.
[0m21:19:56.403146 [info ] [MainThread]: 
[0m21:19:56.404381 [error] [MainThread]:   Database Error in snapshot productmodel_snapshot (snapshots\productmodel.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/productmodel/productmodel_snapshot doesn't exist, or is not a Delta table.
[0m21:19:56.405382 [info ] [MainThread]: 
[0m21:19:56.405382 [error] [MainThread]:   Database Error in snapshot salesorderdetail_snapshot (snapshots\salesorderdetail.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderdetail/salesorderdetail_snapshot doesn't exist, or is not a Delta table.
[0m21:19:56.406380 [info ] [MainThread]: 
[0m21:19:56.406380 [error] [MainThread]:   Database Error in snapshot salesorderheader_snapshot (snapshots\salesorderheader.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderheader/salesorderheader_snapshot doesn't exist, or is not a Delta table.
[0m21:19:56.406380 [info ] [MainThread]: 
[0m21:19:56.407378 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=7 SKIP=0 TOTAL=7
[0m21:19:56.408397 [debug] [MainThread]: Command `dbt snapshot` failed at 21:19:56.408397 after 9.59 seconds
[0m21:19:56.408397 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B132BE1040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B131C32D50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B132C9C8F0>]}
[0m21:19:56.408397 [debug] [MainThread]: Flushing usage events
[0m21:19:57.340935 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:26:33.657702 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019EB0C05550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019EB363CEC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019EB3A8B1D0>]}


============================== 21:26:33.664698 | 0233c983-4209-4194-bc63-2b043bf2801d ==============================
[0m21:26:33.664698 [info ] [MainThread]: Running with dbt=1.9.4
[0m21:26:33.665698 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\morsi\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'H:\\Acadmy\\My\\Projects\\Azure_Spark_DBT\\azure_dbt_spark\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt snapshot', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m21:26:34.432924 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m21:26:34.432924 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m21:26:34.432924 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m21:26:35.089236 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0233c983-4209-4194-bc63-2b043bf2801d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019EB3388560>]}
[0m21:26:35.135311 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0233c983-4209-4194-bc63-2b043bf2801d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019EC6A80A40>]}
[0m21:26:35.136288 [info ] [MainThread]: Registered adapter: databricks=1.10.1
[0m21:26:35.409352 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m21:26:35.520967 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:26:35.520967 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:26:35.527379 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.azure_dbt_spark.example
[0m21:26:35.553519 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0233c983-4209-4194-bc63-2b043bf2801d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019EC72438C0>]}
[0m21:26:35.610086 [debug] [MainThread]: Wrote artifact WritableManifest to H:\Acadmy\My\Projects\Azure_Spark_DBT\azure_dbt_spark\target\manifest.json
[0m21:26:35.612085 [debug] [MainThread]: Wrote artifact SemanticManifest to H:\Acadmy\My\Projects\Azure_Spark_DBT\azure_dbt_spark\target\semantic_manifest.json
[0m21:26:35.633391 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0233c983-4209-4194-bc63-2b043bf2801d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019EC713CB60>]}
[0m21:26:35.633391 [info ] [MainThread]: Found 7 snapshots, 9 sources, 660 macros
[0m21:26:35.634282 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0233c983-4209-4194-bc63-2b043bf2801d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019EC6ABFB30>]}
[0m21:26:35.635374 [info ] [MainThread]: 
[0m21:26:35.636711 [info ] [MainThread]: Concurrency: 2 threads (target='dev')
[0m21:26:35.636711 [info ] [MainThread]: 
[0m21:26:35.637720 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m21:26:35.637720 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m21:26:35.644726 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_hive_metastore, idle-time=0s, language=None, compute-name=) - Creating connection
[0m21:26:35.645730 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m21:26:35.645730 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m21:26:35.646728 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m21:26:35.646728 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:26:36.497514 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=574dd3c5-9417-4f12-9ffa-bebdf489c54f) - Created
[0m21:26:36.715187 [debug] [ThreadPool]: SQL status: OK in 1.070 seconds
[0m21:26:36.716186 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=574dd3c5-9417-4f12-9ffa-bebdf489c54f, command-id=94683e11-3bd1-4512-bf1c-0f9fea51407c) - Closing
[0m21:26:36.718716 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_hive_metastore_snapshots, idle-time=0s, language=None, compute-name=) - Creating connection
[0m21:26:36.718716 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_snapshots'
[0m21:26:36.718716 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m21:26:36.719723 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots)
[0m21:26:36.719723 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:26:37.418822 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=a369cbaa-1247-486b-9e70-42803f0c3fc5) - Created
[0m21:26:37.709796 [debug] [ThreadPool]: SQL status: OK in 0.990 seconds
[0m21:26:37.711709 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=a369cbaa-1247-486b-9e70-42803f0c3fc5, command-id=684e30a8-0d97-48a2-a3ae-c1b821150023) - Closing
[0m21:26:37.722797 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m21:26:37.722797 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

    
SELECT current_catalog()

  
[0m21:26:38.037860 [debug] [ThreadPool]: SQL status: OK in 0.310 seconds
[0m21:26:38.044838 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=a369cbaa-1247-486b-9e70-42803f0c3fc5, command-id=94dd1534-d038-4c0d-b136-dee976a7138a) - Closing
[0m21:26:38.050267 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m21:26:38.051267 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

    
SHOW VIEWS IN `hive_metastore`.`snapshots`

  
[0m21:26:38.352670 [debug] [ThreadPool]: SQL status: OK in 0.300 seconds
[0m21:26:38.354693 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=a369cbaa-1247-486b-9e70-42803f0c3fc5, command-id=3cb6a60a-2e73-4211-89c8-0629da1bd653) - Closing
[0m21:26:38.356584 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0233c983-4209-4194-bc63-2b043bf2801d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019EB0DED880>]}
[0m21:26:38.364797 [debug] [Thread-1 (]: Began running node snapshot.azure_dbt_spark.address_snapshot
[0m21:26:38.365712 [debug] [Thread-2 (]: Began running node snapshot.azure_dbt_spark.customer_snapshot
[0m21:26:38.364797 [info ] [Thread-1 (]: 1 of 7 START snapshot snapshots.address_snapshot ............................... [RUN]
[0m21:26:38.366765 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=snapshot.azure_dbt_spark.address_snapshot, idle-time=0s, language=None, compute-name=) - Creating connection
[0m21:26:38.366765 [debug] [Thread-1 (]: Acquiring new databricks connection 'snapshot.azure_dbt_spark.address_snapshot'
[0m21:26:38.367711 [debug] [Thread-1 (]: Began compiling node snapshot.azure_dbt_spark.address_snapshot
[0m21:26:38.375715 [debug] [Thread-1 (]: Began executing node snapshot.azure_dbt_spark.address_snapshot
[0m21:26:38.402777 [debug] [Thread-1 (]: Using databricks connection "snapshot.azure_dbt_spark.address_snapshot"
[0m21:26:38.402777 [debug] [Thread-1 (]: On snapshot.azure_dbt_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.address_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m21:26:38.402777 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m21:26:38.365712 [info ] [Thread-2 (]: 2 of 7 START snapshot snapshots.customer_snapshot .............................. [RUN]
[0m21:26:38.404782 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=snapshot.azure_dbt_spark.customer_snapshot, idle-time=0s, language=None, compute-name=) - Creating connection
[0m21:26:38.404782 [debug] [Thread-2 (]: Acquiring new databricks connection 'snapshot.azure_dbt_spark.customer_snapshot'
[0m21:26:38.404782 [debug] [Thread-2 (]: Began compiling node snapshot.azure_dbt_spark.customer_snapshot
[0m21:26:38.454420 [debug] [Thread-2 (]: Began executing node snapshot.azure_dbt_spark.customer_snapshot
[0m21:26:38.463421 [debug] [Thread-2 (]: Using databricks connection "snapshot.azure_dbt_spark.customer_snapshot"
[0m21:26:38.465419 [debug] [Thread-2 (]: On snapshot.azure_dbt_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.customer_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customer_snapshot`
  
[0m21:26:38.465419 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:26:39.239676 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=47f10b0d-6fd2-462c-9a08-70ccabcb93ab) - Created
[0m21:26:39.271672 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=9363bd0d-c867-4643-b859-ae136a9ebd67) - Created
[0m21:26:39.608335 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.address_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`address_snapshot`
  
: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/address/address_snapshot doesn't exist, or is not a Delta table.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_PATH_DOES_NOT_EXIST] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/address/address_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:840)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:662)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:737)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:91)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:195)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:483)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:469)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:519)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/address/address_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:810)
	... 42 more
, operation-id=65900539-a62e-46ca-8d28-ed6784e0cad4
[0m21:26:39.610334 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
macro get_columns_in_relation_raw
: Database Error
  [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/address/address_snapshot doesn't exist, or is not a Delta table.
[0m21:26:39.616327 [debug] [Thread-1 (]: Database Error in snapshot address_snapshot (snapshots\address.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/address/address_snapshot doesn't exist, or is not a Delta table.
[0m21:26:39.617327 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0233c983-4209-4194-bc63-2b043bf2801d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019EC7345010>]}
[0m21:26:39.618335 [error] [Thread-1 (]: 1 of 7 ERROR snapshotting snapshots.address_snapshot ........................... [[31mERROR[0m in 1.25s]
[0m21:26:39.619385 [debug] [Thread-1 (]: Finished running node snapshot.azure_dbt_spark.address_snapshot
[0m21:26:39.619385 [debug] [Thread-1 (]: Began running node snapshot.azure_dbt_spark.customeraddress_snapshot
[0m21:26:39.620632 [debug] [Thread-5 (]: Marking all children of 'snapshot.azure_dbt_spark.address_snapshot' to be skipped because of status 'error'.  Reason: Database Error in snapshot address_snapshot (snapshots\address.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/address/address_snapshot doesn't exist, or is not a Delta table..
[0m21:26:39.620632 [info ] [Thread-1 (]: 3 of 7 START snapshot snapshots.customeraddress_snapshot ....................... [RUN]
[0m21:26:39.622111 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.azure_dbt_spark.address_snapshot, now snapshot.azure_dbt_spark.customeraddress_snapshot)
[0m21:26:39.622111 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=47f10b0d-6fd2-462c-9a08-70ccabcb93ab, name=snapshot.azure_dbt_spark.customeraddress_snapshot, idle-time=0.005784273147583008s, language=sql, compute-name=) - Reusing connection previously named snapshot.azure_dbt_spark.address_snapshot
[0m21:26:39.622111 [debug] [Thread-1 (]: Began compiling node snapshot.azure_dbt_spark.customeraddress_snapshot
[0m21:26:39.628395 [debug] [Thread-1 (]: Began executing node snapshot.azure_dbt_spark.customeraddress_snapshot
[0m21:26:39.632341 [debug] [Thread-1 (]: Using databricks connection "snapshot.azure_dbt_spark.customeraddress_snapshot"
[0m21:26:39.632341 [debug] [Thread-1 (]: On snapshot.azure_dbt_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.customeraddress_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m21:26:39.647258 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.customer_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customer_snapshot`
  
: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customer/customer_snapshot doesn't exist, or is not a Delta table.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_PATH_DOES_NOT_EXIST] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customer/customer_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:840)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:662)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:737)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:91)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:195)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:483)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:469)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:519)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customer/customer_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:810)
	... 42 more
, operation-id=8163a13f-0ab3-46e6-b873-6a8c0f14731e
[0m21:26:39.648256 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
macro get_columns_in_relation_raw
: Database Error
  [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customer/customer_snapshot doesn't exist, or is not a Delta table.
[0m21:26:39.653193 [debug] [Thread-2 (]: Database Error in snapshot customer_snapshot (snapshots\customer.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customer/customer_snapshot doesn't exist, or is not a Delta table.
[0m21:26:39.653193 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0233c983-4209-4194-bc63-2b043bf2801d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019EC871CEC0>]}
[0m21:26:39.654193 [error] [Thread-2 (]: 2 of 7 ERROR snapshotting snapshots.customer_snapshot .......................... [[31mERROR[0m in 1.25s]
[0m21:26:39.655106 [debug] [Thread-2 (]: Finished running node snapshot.azure_dbt_spark.customer_snapshot
[0m21:26:39.655106 [debug] [Thread-2 (]: Began running node snapshot.azure_dbt_spark.product_snapshot
[0m21:26:39.656134 [debug] [Thread-5 (]: Marking all children of 'snapshot.azure_dbt_spark.customer_snapshot' to be skipped because of status 'error'.  Reason: Database Error in snapshot customer_snapshot (snapshots\customer.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customer/customer_snapshot doesn't exist, or is not a Delta table..
[0m21:26:39.656134 [info ] [Thread-2 (]: 4 of 7 START snapshot snapshots.product_snapshot ............................... [RUN]
[0m21:26:39.657212 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly snapshot.azure_dbt_spark.customer_snapshot, now snapshot.azure_dbt_spark.product_snapshot)
[0m21:26:39.657212 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=9363bd0d-c867-4643-b859-ae136a9ebd67, name=snapshot.azure_dbt_spark.product_snapshot, idle-time=0.004019260406494141s, language=sql, compute-name=) - Reusing connection previously named snapshot.azure_dbt_spark.customer_snapshot
[0m21:26:39.658187 [debug] [Thread-2 (]: Began compiling node snapshot.azure_dbt_spark.product_snapshot
[0m21:26:39.661190 [debug] [Thread-2 (]: Began executing node snapshot.azure_dbt_spark.product_snapshot
[0m21:26:39.666152 [debug] [Thread-2 (]: Using databricks connection "snapshot.azure_dbt_spark.product_snapshot"
[0m21:26:39.666152 [debug] [Thread-2 (]: On snapshot.azure_dbt_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.product_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`product_snapshot`
  
[0m21:26:39.930836 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.customeraddress_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customeraddress/customeraddress_snapshot doesn't exist, or is not a Delta table.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_PATH_DOES_NOT_EXIST] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customeraddress/customeraddress_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:840)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:662)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:737)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:91)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:195)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:483)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:469)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:519)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customeraddress/customeraddress_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:810)
	... 42 more
, operation-id=e2cbf122-974a-4af2-9fa9-6ddce13349f8
[0m21:26:39.932833 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
macro get_columns_in_relation_raw
: Database Error
  [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customeraddress/customeraddress_snapshot doesn't exist, or is not a Delta table.
[0m21:26:39.938841 [debug] [Thread-1 (]: Database Error in snapshot customeraddress_snapshot (snapshots\customeraddress.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customeraddress/customeraddress_snapshot doesn't exist, or is not a Delta table.
[0m21:26:39.938841 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0233c983-4209-4194-bc63-2b043bf2801d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019EC7339D60>]}
[0m21:26:39.939837 [error] [Thread-1 (]: 3 of 7 ERROR snapshotting snapshots.customeraddress_snapshot ................... [[31mERROR[0m in 0.32s]
[0m21:26:39.941829 [debug] [Thread-1 (]: Finished running node snapshot.azure_dbt_spark.customeraddress_snapshot
[0m21:26:39.941829 [debug] [Thread-1 (]: Began running node snapshot.azure_dbt_spark.productmodel_snapshot
[0m21:26:39.942740 [debug] [Thread-5 (]: Marking all children of 'snapshot.azure_dbt_spark.customeraddress_snapshot' to be skipped because of status 'error'.  Reason: Database Error in snapshot customeraddress_snapshot (snapshots\customeraddress.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customeraddress/customeraddress_snapshot doesn't exist, or is not a Delta table..
[0m21:26:39.943227 [info ] [Thread-1 (]: 5 of 7 START snapshot snapshots.productmodel_snapshot .......................... [RUN]
[0m21:26:39.943830 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.azure_dbt_spark.customeraddress_snapshot, now snapshot.azure_dbt_spark.productmodel_snapshot)
[0m21:26:39.943830 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=47f10b0d-6fd2-462c-9a08-70ccabcb93ab, name=snapshot.azure_dbt_spark.productmodel_snapshot, idle-time=0.0049896240234375s, language=sql, compute-name=) - Reusing connection previously named snapshot.azure_dbt_spark.customeraddress_snapshot
[0m21:26:39.944923 [debug] [Thread-1 (]: Began compiling node snapshot.azure_dbt_spark.productmodel_snapshot
[0m21:26:39.947923 [debug] [Thread-1 (]: Began executing node snapshot.azure_dbt_spark.productmodel_snapshot
[0m21:26:39.951357 [debug] [Thread-1 (]: Using databricks connection "snapshot.azure_dbt_spark.productmodel_snapshot"
[0m21:26:39.952360 [debug] [Thread-1 (]: On snapshot.azure_dbt_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.productmodel_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m21:26:39.969442 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.product_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`product_snapshot`
  
: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/product/product_snapshot doesn't exist, or is not a Delta table.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_PATH_DOES_NOT_EXIST] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/product/product_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:840)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:662)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:737)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:91)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:195)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:483)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:469)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:519)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/product/product_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:810)
	... 42 more
, operation-id=27bedd8b-1b6c-44ba-9a7f-8800ad37660c
[0m21:26:39.970532 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
macro get_columns_in_relation_raw
: Database Error
  [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/product/product_snapshot doesn't exist, or is not a Delta table.
[0m21:26:39.974528 [debug] [Thread-2 (]: Database Error in snapshot product_snapshot (snapshots\product.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/product/product_snapshot doesn't exist, or is not a Delta table.
[0m21:26:39.974839 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0233c983-4209-4194-bc63-2b043bf2801d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019EC87E1B20>]}
[0m21:26:39.974839 [error] [Thread-2 (]: 4 of 7 ERROR snapshotting snapshots.product_snapshot ........................... [[31mERROR[0m in 0.32s]
[0m21:26:39.976127 [debug] [Thread-2 (]: Finished running node snapshot.azure_dbt_spark.product_snapshot
[0m21:26:39.976127 [debug] [Thread-2 (]: Began running node snapshot.azure_dbt_spark.salesorderdetail_snapshot
[0m21:26:39.977134 [debug] [Thread-5 (]: Marking all children of 'snapshot.azure_dbt_spark.product_snapshot' to be skipped because of status 'error'.  Reason: Database Error in snapshot product_snapshot (snapshots\product.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/product/product_snapshot doesn't exist, or is not a Delta table..
[0m21:26:39.977134 [info ] [Thread-2 (]: 6 of 7 START snapshot snapshots.salesorderdetail_snapshot ...................... [RUN]
[0m21:26:39.978141 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly snapshot.azure_dbt_spark.product_snapshot, now snapshot.azure_dbt_spark.salesorderdetail_snapshot)
[0m21:26:39.978141 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=9363bd0d-c867-4643-b859-ae136a9ebd67, name=snapshot.azure_dbt_spark.salesorderdetail_snapshot, idle-time=0.003301858901977539s, language=sql, compute-name=) - Reusing connection previously named snapshot.azure_dbt_spark.product_snapshot
[0m21:26:39.979157 [debug] [Thread-2 (]: Began compiling node snapshot.azure_dbt_spark.salesorderdetail_snapshot
[0m21:26:39.983151 [debug] [Thread-2 (]: Began executing node snapshot.azure_dbt_spark.salesorderdetail_snapshot
[0m21:26:39.989221 [debug] [Thread-2 (]: Using databricks connection "snapshot.azure_dbt_spark.salesorderdetail_snapshot"
[0m21:26:39.989221 [debug] [Thread-2 (]: On snapshot.azure_dbt_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.salesorderdetail_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m21:26:40.248186 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.productmodel_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/productmodel/productmodel_snapshot doesn't exist, or is not a Delta table.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_PATH_DOES_NOT_EXIST] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/productmodel/productmodel_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:840)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:662)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:737)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:91)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:195)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:483)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:469)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:519)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/productmodel/productmodel_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:810)
	... 42 more
, operation-id=cc10f455-5fad-4929-a29b-7ac395febb35
[0m21:26:40.249095 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
macro get_columns_in_relation_raw
: Database Error
  [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/productmodel/productmodel_snapshot doesn't exist, or is not a Delta table.
[0m21:26:40.252268 [debug] [Thread-1 (]: Database Error in snapshot productmodel_snapshot (snapshots\productmodel.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/productmodel/productmodel_snapshot doesn't exist, or is not a Delta table.
[0m21:26:40.253354 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0233c983-4209-4194-bc63-2b043bf2801d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019EC8809DF0>]}
[0m21:26:40.254356 [error] [Thread-1 (]: 5 of 7 ERROR snapshotting snapshots.productmodel_snapshot ...................... [[31mERROR[0m in 0.31s]
[0m21:26:40.255271 [debug] [Thread-1 (]: Finished running node snapshot.azure_dbt_spark.productmodel_snapshot
[0m21:26:40.256259 [debug] [Thread-1 (]: Began running node snapshot.azure_dbt_spark.salesorderheader_snapshot
[0m21:26:40.256909 [debug] [Thread-5 (]: Marking all children of 'snapshot.azure_dbt_spark.productmodel_snapshot' to be skipped because of status 'error'.  Reason: Database Error in snapshot productmodel_snapshot (snapshots\productmodel.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/productmodel/productmodel_snapshot doesn't exist, or is not a Delta table..
[0m21:26:40.256259 [info ] [Thread-1 (]: 7 of 7 START snapshot snapshots.salesorderheader_snapshot ...................... [RUN]
[0m21:26:40.257566 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.azure_dbt_spark.productmodel_snapshot, now snapshot.azure_dbt_spark.salesorderheader_snapshot)
[0m21:26:40.257566 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=47f10b0d-6fd2-462c-9a08-70ccabcb93ab, name=snapshot.azure_dbt_spark.salesorderheader_snapshot, idle-time=0.004211902618408203s, language=sql, compute-name=) - Reusing connection previously named snapshot.azure_dbt_spark.productmodel_snapshot
[0m21:26:40.257566 [debug] [Thread-1 (]: Began compiling node snapshot.azure_dbt_spark.salesorderheader_snapshot
[0m21:26:40.260572 [debug] [Thread-1 (]: Began executing node snapshot.azure_dbt_spark.salesorderheader_snapshot
[0m21:26:40.265576 [debug] [Thread-1 (]: Using databricks connection "snapshot.azure_dbt_spark.salesorderheader_snapshot"
[0m21:26:40.266578 [debug] [Thread-1 (]: On snapshot.azure_dbt_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.salesorderheader_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m21:26:40.299366 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.salesorderdetail_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderdetail/salesorderdetail_snapshot doesn't exist, or is not a Delta table.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_PATH_DOES_NOT_EXIST] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderdetail/salesorderdetail_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:840)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:662)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:737)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:91)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:195)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:483)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:469)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:519)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderdetail/salesorderdetail_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:810)
	... 42 more
, operation-id=7b4cb559-3672-4158-924a-f78152002332
[0m21:26:40.300267 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
macro get_columns_in_relation_raw
: Database Error
  [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderdetail/salesorderdetail_snapshot doesn't exist, or is not a Delta table.
[0m21:26:40.305352 [debug] [Thread-2 (]: Database Error in snapshot salesorderdetail_snapshot (snapshots\salesorderdetail.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderdetail/salesorderdetail_snapshot doesn't exist, or is not a Delta table.
[0m21:26:40.305352 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0233c983-4209-4194-bc63-2b043bf2801d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019EC732AD80>]}
[0m21:26:40.306368 [error] [Thread-2 (]: 6 of 7 ERROR snapshotting snapshots.salesorderdetail_snapshot .................. [[31mERROR[0m in 0.33s]
[0m21:26:40.307267 [debug] [Thread-2 (]: Finished running node snapshot.azure_dbt_spark.salesorderdetail_snapshot
[0m21:26:40.307267 [debug] [Thread-5 (]: Marking all children of 'snapshot.azure_dbt_spark.salesorderdetail_snapshot' to be skipped because of status 'error'.  Reason: Database Error in snapshot salesorderdetail_snapshot (snapshots\salesorderdetail.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderdetail/salesorderdetail_snapshot doesn't exist, or is not a Delta table..
[0m21:26:40.735950 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.salesorderheader_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderheader/salesorderheader_snapshot doesn't exist, or is not a Delta table.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_PATH_DOES_NOT_EXIST] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderheader/salesorderheader_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:840)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:662)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:737)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:91)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:195)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:483)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:469)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:519)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderheader/salesorderheader_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:810)
	... 42 more
, operation-id=3fc4db61-51df-45ec-a188-3f7e075a8a45
[0m21:26:40.737948 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
macro get_columns_in_relation_raw
: Database Error
  [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderheader/salesorderheader_snapshot doesn't exist, or is not a Delta table.
[0m21:26:40.740944 [debug] [Thread-1 (]: Database Error in snapshot salesorderheader_snapshot (snapshots\salesorderheader.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderheader/salesorderheader_snapshot doesn't exist, or is not a Delta table.
[0m21:26:40.741934 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0233c983-4209-4194-bc63-2b043bf2801d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019EC732AD80>]}
[0m21:26:40.741934 [error] [Thread-1 (]: 7 of 7 ERROR snapshotting snapshots.salesorderheader_snapshot .................. [[31mERROR[0m in 0.48s]
[0m21:26:40.743098 [debug] [Thread-1 (]: Finished running node snapshot.azure_dbt_spark.salesorderheader_snapshot
[0m21:26:40.744105 [debug] [Thread-5 (]: Marking all children of 'snapshot.azure_dbt_spark.salesorderheader_snapshot' to be skipped because of status 'error'.  Reason: Database Error in snapshot salesorderheader_snapshot (snapshots\salesorderheader.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderheader/salesorderheader_snapshot doesn't exist, or is not a Delta table..
[0m21:26:40.745105 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=2.3875176906585693s, language=None, compute-name=) - Reusing connection previously named master
[0m21:26:40.745105 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:26:40.746190 [debug] [MainThread]: Connection 'list_hive_metastore' was properly closed.
[0m21:26:40.746190 [debug] [MainThread]: On list_hive_metastore: Close
[0m21:26:40.746190 [debug] [MainThread]: Databricks adapter: Connection(session-id=574dd3c5-9417-4f12-9ffa-bebdf489c54f) - Closing
[0m21:26:40.952156 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m21:26:40.953156 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m21:26:40.954159 [debug] [MainThread]: Databricks adapter: Connection(session-id=a369cbaa-1247-486b-9e70-42803f0c3fc5) - Closing
[0m21:26:41.154340 [debug] [MainThread]: Connection 'snapshot.azure_dbt_spark.salesorderheader_snapshot' was properly closed.
[0m21:26:41.155340 [debug] [MainThread]: On snapshot.azure_dbt_spark.salesorderheader_snapshot: Close
[0m21:26:41.155340 [debug] [MainThread]: Databricks adapter: Connection(session-id=47f10b0d-6fd2-462c-9a08-70ccabcb93ab) - Closing
[0m21:26:41.368420 [debug] [MainThread]: Connection 'snapshot.azure_dbt_spark.salesorderdetail_snapshot' was properly closed.
[0m21:26:41.369415 [debug] [MainThread]: On snapshot.azure_dbt_spark.salesorderdetail_snapshot: Close
[0m21:26:41.370413 [debug] [MainThread]: Databricks adapter: Connection(session-id=9363bd0d-c867-4643-b859-ae136a9ebd67) - Closing
[0m21:26:41.570768 [info ] [MainThread]: 
[0m21:26:41.571765 [info ] [MainThread]: Finished running 7 snapshots in 0 hours 0 minutes and 5.93 seconds (5.93s).
[0m21:26:41.573856 [debug] [MainThread]: Command end result
[0m21:26:41.600105 [debug] [MainThread]: Wrote artifact WritableManifest to H:\Acadmy\My\Projects\Azure_Spark_DBT\azure_dbt_spark\target\manifest.json
[0m21:26:41.601100 [debug] [MainThread]: Wrote artifact SemanticManifest to H:\Acadmy\My\Projects\Azure_Spark_DBT\azure_dbt_spark\target\semantic_manifest.json
[0m21:26:41.607099 [debug] [MainThread]: Wrote artifact RunExecutionResult to H:\Acadmy\My\Projects\Azure_Spark_DBT\azure_dbt_spark\target\run_results.json
[0m21:26:41.607099 [info ] [MainThread]: 
[0m21:26:41.607801 [info ] [MainThread]: [31mCompleted with 7 errors, 0 partial successes, and 0 warnings:[0m
[0m21:26:41.608673 [info ] [MainThread]: 
[0m21:26:41.608673 [error] [MainThread]:   Database Error in snapshot address_snapshot (snapshots\address.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/address/address_snapshot doesn't exist, or is not a Delta table.
[0m21:26:41.609679 [info ] [MainThread]: 
[0m21:26:41.610680 [error] [MainThread]:   Database Error in snapshot customer_snapshot (snapshots\customer.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customer/customer_snapshot doesn't exist, or is not a Delta table.
[0m21:26:41.610680 [info ] [MainThread]: 
[0m21:26:41.611679 [error] [MainThread]:   Database Error in snapshot customeraddress_snapshot (snapshots\customeraddress.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customeraddress/customeraddress_snapshot doesn't exist, or is not a Delta table.
[0m21:26:41.612685 [info ] [MainThread]: 
[0m21:26:41.612685 [error] [MainThread]:   Database Error in snapshot product_snapshot (snapshots\product.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/product/product_snapshot doesn't exist, or is not a Delta table.
[0m21:26:41.613682 [info ] [MainThread]: 
[0m21:26:41.614685 [error] [MainThread]:   Database Error in snapshot productmodel_snapshot (snapshots\productmodel.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/productmodel/productmodel_snapshot doesn't exist, or is not a Delta table.
[0m21:26:41.614685 [info ] [MainThread]: 
[0m21:26:41.615683 [error] [MainThread]:   Database Error in snapshot salesorderdetail_snapshot (snapshots\salesorderdetail.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderdetail/salesorderdetail_snapshot doesn't exist, or is not a Delta table.
[0m21:26:41.616339 [info ] [MainThread]: 
[0m21:26:41.617365 [error] [MainThread]:   Database Error in snapshot salesorderheader_snapshot (snapshots\salesorderheader.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderheader/salesorderheader_snapshot doesn't exist, or is not a Delta table.
[0m21:26:41.617365 [info ] [MainThread]: 
[0m21:26:41.618403 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=7 SKIP=0 TOTAL=7
[0m21:26:41.620359 [debug] [MainThread]: Command `dbt snapshot` failed at 21:26:41.619361 after 8.23 seconds
[0m21:26:41.620359 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019EB363CEC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019EB3C0CC80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019EC67C26F0>]}
[0m21:26:41.620359 [debug] [MainThread]: Flushing usage events
[0m21:26:42.576999 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:33:42.684204 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283C7D599D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283CA2EAF00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283CAB68080>]}


============================== 21:33:42.690261 | 5306f488-79be-45cd-ac3a-1e19ebeb1d25 ==============================
[0m21:33:42.690261 [info ] [MainThread]: Running with dbt=1.9.4
[0m21:33:42.691171 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\morsi\\.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': 'H:\\Acadmy\\My\\Projects\\Azure_Spark_DBT\\azure_dbt_spark\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt snapshot', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m21:33:43.448818 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m21:33:43.449817 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m21:33:43.449817 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m21:33:44.106427 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5306f488-79be-45cd-ac3a-1e19ebeb1d25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283DD4B17C0>]}
[0m21:33:44.151211 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5306f488-79be-45cd-ac3a-1e19ebeb1d25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283C9655280>]}
[0m21:33:44.152219 [info ] [MainThread]: Registered adapter: databricks=1.10.1
[0m21:33:44.418055 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m21:33:44.532766 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:33:44.532766 [debug] [MainThread]: Partial parsing: updated file: azure_dbt_spark://snapshots\address.sql
[0m21:33:44.730639 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.azure_dbt_spark.example
[0m21:33:44.740736 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5306f488-79be-45cd-ac3a-1e19ebeb1d25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283DE493230>]}
[0m21:33:44.799880 [debug] [MainThread]: Wrote artifact WritableManifest to H:\Acadmy\My\Projects\Azure_Spark_DBT\azure_dbt_spark\target\manifest.json
[0m21:33:44.801882 [debug] [MainThread]: Wrote artifact SemanticManifest to H:\Acadmy\My\Projects\Azure_Spark_DBT\azure_dbt_spark\target\semantic_manifest.json
[0m21:33:44.823891 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5306f488-79be-45cd-ac3a-1e19ebeb1d25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283DE13A180>]}
[0m21:33:44.824882 [info ] [MainThread]: Found 7 snapshots, 9 sources, 660 macros
[0m21:33:44.825422 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5306f488-79be-45cd-ac3a-1e19ebeb1d25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283DE4D34A0>]}
[0m21:33:44.827819 [info ] [MainThread]: 
[0m21:33:44.828818 [info ] [MainThread]: Concurrency: 2 threads (target='dev')
[0m21:33:44.829316 [info ] [MainThread]: 
[0m21:33:44.829751 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m21:33:44.829751 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m21:33:44.835759 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_default, idle-time=0s, language=None, compute-name=) - Creating connection
[0m21:33:44.835759 [debug] [ThreadPool]: Acquiring new databricks connection 'list_default'
[0m21:33:44.836758 [debug] [ThreadPool]: Using databricks connection "list_default"
[0m21:33:44.836758 [debug] [ThreadPool]: On list_default: GetSchemas(database=default, schema=None)
[0m21:33:44.836758 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:33:44.843762 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_hive_metastore, idle-time=0s, language=None, compute-name=) - Creating connection
[0m21:33:44.860422 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m21:33:44.861877 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m21:33:44.862885 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m21:33:44.862885 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:33:45.672883 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=608f02c0-a286-4920-99ab-045ea2c5249f) - Created
[0m21:33:45.678439 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=3d6a8833-f2b6-4a85-852f-709f9ae11404) - Created
[0m21:33:45.967883 [debug] [ThreadPool]: SQL status: OK in 1.100 seconds
[0m21:33:45.968877 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=3d6a8833-f2b6-4a85-852f-709f9ae11404, command-id=b84dfa4f-c2a8-42fd-918d-952a1d47bf49) - Closing
[0m21:33:46.196843 [debug] [ThreadPool]: SQL status: OK in 1.360 seconds
[0m21:33:46.205029 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=608f02c0-a286-4920-99ab-045ea2c5249f, command-id=692a9c3b-265d-4512-8a7f-eac27b57e9b3) - Closing
[0m21:33:46.205029 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now create_default_snapshots)
[0m21:33:46.205029 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=3d6a8833-f2b6-4a85-852f-709f9ae11404, name=create_default_snapshots, idle-time=0.23516082763671875s, language=None, compute-name=) - Reusing connection previously named list_hive_metastore
[0m21:33:46.205029 [debug] [ThreadPool]: Creating schema "database: "default"
schema: "snapshots"
"
[0m21:33:46.216834 [debug] [ThreadPool]: Using databricks connection "create_default_snapshots"
[0m21:33:46.216834 [debug] [ThreadPool]: On create_default_snapshots: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "connection_name": "create_default_snapshots"} */
create schema if not exists `default`.`snapshots`
  
[0m21:33:46.645041 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "connection_name": "create_default_snapshots"} */
create schema if not exists `default`.`snapshots`
  
: [REQUIRES_SINGLE_PART_NAMESPACE] spark_catalog requires a single-part namespace, but got `default`.`snapshots`. SQLSTATE: 42K05
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [REQUIRES_SINGLE_PART_NAMESPACE] org.apache.spark.sql.AnalysisException: [REQUIRES_SINGLE_PART_NAMESPACE] spark_catalog requires a single-part namespace, but got `default`.`snapshots`. SQLSTATE: 42K05
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:840)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:662)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:737)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:91)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:195)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:483)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:469)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:519)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [REQUIRES_SINGLE_PART_NAMESPACE] spark_catalog requires a single-part namespace, but got `default`.`snapshots`. SQLSTATE: 42K05
	at org.apache.spark.sql.errors.QueryCompilationErrors$.requiresSinglePartNamespaceError(QueryCompilationErrors.scala:1944)
	at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$DatabaseNameInSessionCatalog$.unapply(ResolveSessionCatalog.scala:1333)
	at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$$anonfun$apply$1.applyOrElse(ResolveSessionCatalog.scala:456)
	at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$$anonfun$apply$1.applyOrElse(ResolveSessionCatalog.scala:79)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:436)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:40)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:40)
	at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.apply(ResolveSessionCatalog.scala:79)
	at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.apply(ResolveSessionCatalog.scala:73)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:327)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:327)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:324)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:307)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$9(RuleExecutor.scala:411)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$9$adapted(RuleExecutor.scala:411)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:411)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:270)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:444)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:437)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:350)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:437)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:369)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:262)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:194)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:262)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:422)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:443)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:421)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:290)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:489)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:653)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:653)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1219)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:652)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:648)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1210)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:648)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:284)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:283)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:265)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:656)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1210)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:628)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:618)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:628)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:706)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:582)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:706)
	... 42 more
, operation-id=68d74786-277d-4bf0-87eb-4dc8dda80a14
[0m21:33:46.647631 [debug] [ThreadPool]: Databricks adapter: Exception while trying to execute query
macro create_schema
: Database Error
  [REQUIRES_SINGLE_PART_NAMESPACE] spark_catalog requires a single-part namespace, but got `default`.`snapshots`. SQLSTATE: 42K05
[0m21:33:46.648572 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:33:46.649606 [debug] [MainThread]: Connection 'list_default' was properly closed.
[0m21:33:46.649606 [debug] [MainThread]: On list_default: Close
[0m21:33:46.650533 [debug] [MainThread]: Databricks adapter: Connection(session-id=608f02c0-a286-4920-99ab-045ea2c5249f) - Closing
[0m21:33:46.838642 [debug] [MainThread]: Connection 'create_default_snapshots' was properly closed.
[0m21:33:46.839639 [debug] [MainThread]: On create_default_snapshots: Close
[0m21:33:46.840642 [debug] [MainThread]: Databricks adapter: Connection(session-id=3d6a8833-f2b6-4a85-852f-709f9ae11404) - Closing
[0m21:33:47.023597 [info ] [MainThread]: 
[0m21:33:47.025606 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 2.19 seconds (2.19s).
[0m21:33:47.026691 [error] [MainThread]: Encountered an error:
Database Error
  Database Error
    [REQUIRES_SINGLE_PART_NAMESPACE] spark_catalog requires a single-part namespace, but got `default`.`snapshots`. SQLSTATE: 42K05
[0m21:33:47.027121 [debug] [MainThread]: Command `dbt snapshot` failed at 21:33:47.027121 after 4.65 seconds
[0m21:33:47.028139 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283C9F80A10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283DE581CD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283DE4D3E30>]}
[0m21:33:47.029137 [debug] [MainThread]: Flushing usage events
[0m21:33:47.707162 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:34:02.271957 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D2BF1C170>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D2BE65970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D2BD66F30>]}


============================== 21:34:02.278349 | a0e2051b-568c-4629-8ee1-a3299f64618e ==============================
[0m21:34:02.278349 [info ] [MainThread]: Running with dbt=1.9.4
[0m21:34:02.279462 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'H:\\Acadmy\\My\\Projects\\Azure_Spark_DBT\\azure_dbt_spark\\logs', 'profiles_dir': 'C:\\Users\\morsi\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt snapshot', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m21:34:03.020313 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m21:34:03.021313 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m21:34:03.021313 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m21:34:03.656526 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a0e2051b-568c-4629-8ee1-a3299f64618e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D2C20C530>]}
[0m21:34:03.703071 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a0e2051b-568c-4629-8ee1-a3299f64618e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D3E6CEBA0>]}
[0m21:34:03.703071 [info ] [MainThread]: Registered adapter: databricks=1.10.1
[0m21:34:03.973244 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m21:34:04.083964 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:34:04.084958 [debug] [MainThread]: Partial parsing: updated file: azure_dbt_spark://snapshots\address.sql
[0m21:34:04.285027 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.azure_dbt_spark.example
[0m21:34:04.295922 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a0e2051b-568c-4629-8ee1-a3299f64618e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D3F9990D0>]}
[0m21:34:04.355107 [debug] [MainThread]: Wrote artifact WritableManifest to H:\Acadmy\My\Projects\Azure_Spark_DBT\azure_dbt_spark\target\manifest.json
[0m21:34:04.357103 [debug] [MainThread]: Wrote artifact SemanticManifest to H:\Acadmy\My\Projects\Azure_Spark_DBT\azure_dbt_spark\target\semantic_manifest.json
[0m21:34:04.379015 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a0e2051b-568c-4629-8ee1-a3299f64618e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D3F700CE0>]}
[0m21:34:04.379015 [info ] [MainThread]: Found 7 snapshots, 9 sources, 660 macros
[0m21:34:04.379960 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a0e2051b-568c-4629-8ee1-a3299f64618e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D3F790D40>]}
[0m21:34:04.381062 [info ] [MainThread]: 
[0m21:34:04.382376 [info ] [MainThread]: Concurrency: 2 threads (target='dev')
[0m21:34:04.382975 [info ] [MainThread]: 
[0m21:34:04.383981 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m21:34:04.383981 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m21:34:04.389067 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_hive_metastore, idle-time=0s, language=None, compute-name=) - Creating connection
[0m21:34:04.390028 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m21:34:04.390028 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m21:34:04.390028 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m21:34:04.390980 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:34:05.120210 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=52d2a03c-54ac-4272-b76b-286b07a0ebbd) - Created
[0m21:34:05.339590 [debug] [ThreadPool]: SQL status: OK in 0.950 seconds
[0m21:34:05.342584 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=52d2a03c-54ac-4272-b76b-286b07a0ebbd, command-id=f4dd88bf-58f2-4eb7-b34a-70f60161844b) - Closing
[0m21:34:05.345588 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_hive_metastore_snapshots, idle-time=0s, language=None, compute-name=) - Creating connection
[0m21:34:05.345588 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_snapshots'
[0m21:34:05.345588 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m21:34:05.346588 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots)
[0m21:34:05.346588 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:34:06.065732 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=b3cc07b7-c381-4e55-ae25-bcf1b2ed978a) - Created
[0m21:34:06.325837 [debug] [ThreadPool]: SQL status: OK in 0.980 seconds
[0m21:34:06.327744 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=b3cc07b7-c381-4e55-ae25-bcf1b2ed978a, command-id=2d991da2-9134-4814-b286-2af55d2b6fcc) - Closing
[0m21:34:06.337850 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m21:34:06.337850 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

    
SELECT current_catalog()

  
[0m21:34:06.660712 [debug] [ThreadPool]: SQL status: OK in 0.320 seconds
[0m21:34:06.665081 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=b3cc07b7-c381-4e55-ae25-bcf1b2ed978a, command-id=56b20c43-f5a6-4c44-bff9-dc7a087fddcd) - Closing
[0m21:34:06.671080 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m21:34:06.672080 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

    
SHOW VIEWS IN `hive_metastore`.`snapshots`

  
[0m21:34:06.967439 [debug] [ThreadPool]: SQL status: OK in 0.290 seconds
[0m21:34:06.970521 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=b3cc07b7-c381-4e55-ae25-bcf1b2ed978a, command-id=57ebf953-ce58-4f97-a379-3429c3e8760c) - Closing
[0m21:34:06.973906 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a0e2051b-568c-4629-8ee1-a3299f64618e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D3F49CCE0>]}
[0m21:34:06.981024 [debug] [Thread-1 (]: Began running node snapshot.azure_dbt_spark.address_snapshot
[0m21:34:06.982111 [debug] [Thread-2 (]: Began running node snapshot.azure_dbt_spark.customer_snapshot
[0m21:34:06.982111 [info ] [Thread-1 (]: 1 of 7 START snapshot snapshots.address_snapshot ............................... [RUN]
[0m21:34:06.984109 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=snapshot.azure_dbt_spark.address_snapshot, idle-time=0s, language=None, compute-name=) - Creating connection
[0m21:34:06.984109 [debug] [Thread-1 (]: Acquiring new databricks connection 'snapshot.azure_dbt_spark.address_snapshot'
[0m21:34:06.984109 [debug] [Thread-1 (]: Began compiling node snapshot.azure_dbt_spark.address_snapshot
[0m21:34:06.991119 [debug] [Thread-1 (]: Began executing node snapshot.azure_dbt_spark.address_snapshot
[0m21:34:07.018113 [debug] [Thread-1 (]: Using databricks connection "snapshot.azure_dbt_spark.address_snapshot"
[0m21:34:07.018113 [debug] [Thread-1 (]: On snapshot.azure_dbt_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.address_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m21:34:07.019114 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m21:34:06.983070 [info ] [Thread-2 (]: 2 of 7 START snapshot snapshots.customer_snapshot .............................. [RUN]
[0m21:34:07.019114 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=snapshot.azure_dbt_spark.customer_snapshot, idle-time=0s, language=None, compute-name=) - Creating connection
[0m21:34:07.020113 [debug] [Thread-2 (]: Acquiring new databricks connection 'snapshot.azure_dbt_spark.customer_snapshot'
[0m21:34:07.020113 [debug] [Thread-2 (]: Began compiling node snapshot.azure_dbt_spark.customer_snapshot
[0m21:34:07.023595 [debug] [Thread-2 (]: Began executing node snapshot.azure_dbt_spark.customer_snapshot
[0m21:34:07.027188 [debug] [Thread-2 (]: Using databricks connection "snapshot.azure_dbt_spark.customer_snapshot"
[0m21:34:07.027188 [debug] [Thread-2 (]: On snapshot.azure_dbt_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.customer_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customer_snapshot`
  
[0m21:34:07.028293 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:34:07.793235 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=f1610eab-728e-4ab1-8786-c4f39da0e598) - Created
[0m21:34:07.795283 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=640abe9a-0574-46b8-a86e-ec17691886a0) - Created
[0m21:34:08.165312 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.address_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`address_snapshot`
  
: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/address/address_snapshot doesn't exist, or is not a Delta table.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_PATH_DOES_NOT_EXIST] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/address/address_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:840)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:662)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:737)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:91)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:195)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:483)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:469)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:519)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/address/address_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:810)
	... 42 more
, operation-id=9340cebb-751b-44e3-8f5d-cc6933dcf293
[0m21:34:08.167310 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
macro get_columns_in_relation_raw
: Database Error
  [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/address/address_snapshot doesn't exist, or is not a Delta table.
[0m21:34:08.174644 [debug] [Thread-1 (]: Database Error in snapshot address_snapshot (snapshots\address.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/address/address_snapshot doesn't exist, or is not a Delta table.
[0m21:34:08.175645 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a0e2051b-568c-4629-8ee1-a3299f64618e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D40E79370>]}
[0m21:34:08.176649 [error] [Thread-1 (]: 1 of 7 ERROR snapshotting snapshots.address_snapshot ........................... [[31mERROR[0m in 1.19s]
[0m21:34:08.177643 [debug] [Thread-1 (]: Finished running node snapshot.azure_dbt_spark.address_snapshot
[0m21:34:08.178563 [debug] [Thread-1 (]: Began running node snapshot.azure_dbt_spark.customeraddress_snapshot
[0m21:34:08.178563 [info ] [Thread-1 (]: 3 of 7 START snapshot snapshots.customeraddress_snapshot ....................... [RUN]
[0m21:34:08.179561 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.azure_dbt_spark.address_snapshot, now snapshot.azure_dbt_spark.customeraddress_snapshot)
[0m21:34:08.179561 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=640abe9a-0574-46b8-a86e-ec17691886a0, name=snapshot.azure_dbt_spark.customeraddress_snapshot, idle-time=0.0049169063568115234s, language=sql, compute-name=) - Reusing connection previously named snapshot.azure_dbt_spark.address_snapshot
[0m21:34:08.180562 [debug] [Thread-1 (]: Began compiling node snapshot.azure_dbt_spark.customeraddress_snapshot
[0m21:34:08.183559 [debug] [Thread-1 (]: Began executing node snapshot.azure_dbt_spark.customeraddress_snapshot
[0m21:34:08.187559 [debug] [Thread-1 (]: Using databricks connection "snapshot.azure_dbt_spark.customeraddress_snapshot"
[0m21:34:08.188560 [debug] [Thread-1 (]: On snapshot.azure_dbt_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.customeraddress_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m21:34:08.190622 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.customer_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customer_snapshot`
  
: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customer/customer_snapshot doesn't exist, or is not a Delta table.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_PATH_DOES_NOT_EXIST] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customer/customer_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:840)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:662)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:737)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:91)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:195)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:483)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:469)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:519)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customer/customer_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:810)
	... 42 more
, operation-id=fce677ab-c36f-4702-be52-79cba53361b5
[0m21:34:08.190622 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
macro get_columns_in_relation_raw
: Database Error
  [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customer/customer_snapshot doesn't exist, or is not a Delta table.
[0m21:34:08.191557 [debug] [Thread-5 (]: Marking all children of 'snapshot.azure_dbt_spark.address_snapshot' to be skipped because of status 'error'.  Reason: Database Error in snapshot address_snapshot (snapshots\address.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/address/address_snapshot doesn't exist, or is not a Delta table..
[0m21:34:08.195556 [debug] [Thread-2 (]: Database Error in snapshot customer_snapshot (snapshots\customer.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customer/customer_snapshot doesn't exist, or is not a Delta table.
[0m21:34:08.195556 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a0e2051b-568c-4629-8ee1-a3299f64618e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D40E7EF30>]}
[0m21:34:08.196557 [error] [Thread-2 (]: 2 of 7 ERROR snapshotting snapshots.customer_snapshot .......................... [[31mERROR[0m in 1.18s]
[0m21:34:08.196557 [debug] [Thread-2 (]: Finished running node snapshot.azure_dbt_spark.customer_snapshot
[0m21:34:08.196557 [debug] [Thread-2 (]: Began running node snapshot.azure_dbt_spark.product_snapshot
[0m21:34:08.199163 [debug] [Thread-5 (]: Marking all children of 'snapshot.azure_dbt_spark.customer_snapshot' to be skipped because of status 'error'.  Reason: Database Error in snapshot customer_snapshot (snapshots\customer.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customer/customer_snapshot doesn't exist, or is not a Delta table..
[0m21:34:08.196557 [info ] [Thread-2 (]: 4 of 7 START snapshot snapshots.product_snapshot ............................... [RUN]
[0m21:34:08.200452 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly snapshot.azure_dbt_spark.customer_snapshot, now snapshot.azure_dbt_spark.product_snapshot)
[0m21:34:08.200452 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=f1610eab-728e-4ab1-8786-c4f39da0e598, name=snapshot.azure_dbt_spark.product_snapshot, idle-time=0.004895687103271484s, language=sql, compute-name=) - Reusing connection previously named snapshot.azure_dbt_spark.customer_snapshot
[0m21:34:08.201733 [debug] [Thread-2 (]: Began compiling node snapshot.azure_dbt_spark.product_snapshot
[0m21:34:08.204927 [debug] [Thread-2 (]: Began executing node snapshot.azure_dbt_spark.product_snapshot
[0m21:34:08.211933 [debug] [Thread-2 (]: Using databricks connection "snapshot.azure_dbt_spark.product_snapshot"
[0m21:34:08.212935 [debug] [Thread-2 (]: On snapshot.azure_dbt_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.product_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`product_snapshot`
  
[0m21:34:08.530708 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.customeraddress_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customeraddress/customeraddress_snapshot doesn't exist, or is not a Delta table.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_PATH_DOES_NOT_EXIST] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customeraddress/customeraddress_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:840)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:662)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:737)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:91)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:195)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:483)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:469)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:519)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customeraddress/customeraddress_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:810)
	... 42 more
, operation-id=3ed57036-508c-4a65-b647-db67d9a2e5d3
[0m21:34:08.531705 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
macro get_columns_in_relation_raw
: Database Error
  [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customeraddress/customeraddress_snapshot doesn't exist, or is not a Delta table.
[0m21:34:08.534704 [debug] [Thread-1 (]: Database Error in snapshot customeraddress_snapshot (snapshots\customeraddress.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customeraddress/customeraddress_snapshot doesn't exist, or is not a Delta table.
[0m21:34:08.535704 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a0e2051b-568c-4629-8ee1-a3299f64618e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D40E72240>]}
[0m21:34:08.535704 [error] [Thread-1 (]: 3 of 7 ERROR snapshotting snapshots.customeraddress_snapshot ................... [[31mERROR[0m in 0.36s]
[0m21:34:08.536894 [debug] [Thread-1 (]: Finished running node snapshot.azure_dbt_spark.customeraddress_snapshot
[0m21:34:08.536894 [debug] [Thread-1 (]: Began running node snapshot.azure_dbt_spark.productmodel_snapshot
[0m21:34:08.537996 [debug] [Thread-5 (]: Marking all children of 'snapshot.azure_dbt_spark.customeraddress_snapshot' to be skipped because of status 'error'.  Reason: Database Error in snapshot customeraddress_snapshot (snapshots\customeraddress.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customeraddress/customeraddress_snapshot doesn't exist, or is not a Delta table..
[0m21:34:08.537996 [info ] [Thread-1 (]: 5 of 7 START snapshot snapshots.productmodel_snapshot .......................... [RUN]
[0m21:34:08.538909 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.azure_dbt_spark.customeraddress_snapshot, now snapshot.azure_dbt_spark.productmodel_snapshot)
[0m21:34:08.538909 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=640abe9a-0574-46b8-a86e-ec17691886a0, name=snapshot.azure_dbt_spark.productmodel_snapshot, idle-time=0.0032050609588623047s, language=sql, compute-name=) - Reusing connection previously named snapshot.azure_dbt_spark.customeraddress_snapshot
[0m21:34:08.539989 [debug] [Thread-1 (]: Began compiling node snapshot.azure_dbt_spark.productmodel_snapshot
[0m21:34:08.542901 [debug] [Thread-1 (]: Began executing node snapshot.azure_dbt_spark.productmodel_snapshot
[0m21:34:08.545987 [debug] [Thread-1 (]: Using databricks connection "snapshot.azure_dbt_spark.productmodel_snapshot"
[0m21:34:08.546998 [debug] [Thread-1 (]: On snapshot.azure_dbt_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.productmodel_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m21:34:08.564445 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.product_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`product_snapshot`
  
: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/product/product_snapshot doesn't exist, or is not a Delta table.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_PATH_DOES_NOT_EXIST] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/product/product_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:840)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:662)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:737)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:91)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:195)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:483)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:469)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:519)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/product/product_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:810)
	... 42 more
, operation-id=3f2b1fbe-9cf4-4802-9649-0a2712e981c2
[0m21:34:08.565525 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
macro get_columns_in_relation_raw
: Database Error
  [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/product/product_snapshot doesn't exist, or is not a Delta table.
[0m21:34:08.568521 [debug] [Thread-2 (]: Database Error in snapshot product_snapshot (snapshots\product.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/product/product_snapshot doesn't exist, or is not a Delta table.
[0m21:34:08.569519 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a0e2051b-568c-4629-8ee1-a3299f64618e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D40E01460>]}
[0m21:34:08.569519 [error] [Thread-2 (]: 4 of 7 ERROR snapshotting snapshots.product_snapshot ........................... [[31mERROR[0m in 0.37s]
[0m21:34:08.570504 [debug] [Thread-2 (]: Finished running node snapshot.azure_dbt_spark.product_snapshot
[0m21:34:08.571526 [debug] [Thread-2 (]: Began running node snapshot.azure_dbt_spark.salesorderdetail_snapshot
[0m21:34:08.572442 [debug] [Thread-5 (]: Marking all children of 'snapshot.azure_dbt_spark.product_snapshot' to be skipped because of status 'error'.  Reason: Database Error in snapshot product_snapshot (snapshots\product.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/product/product_snapshot doesn't exist, or is not a Delta table..
[0m21:34:08.571526 [info ] [Thread-2 (]: 6 of 7 START snapshot snapshots.salesorderdetail_snapshot ...................... [RUN]
[0m21:34:08.572999 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly snapshot.azure_dbt_spark.product_snapshot, now snapshot.azure_dbt_spark.salesorderdetail_snapshot)
[0m21:34:08.572999 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=f1610eab-728e-4ab1-8786-c4f39da0e598, name=snapshot.azure_dbt_spark.salesorderdetail_snapshot, idle-time=0.004477262496948242s, language=sql, compute-name=) - Reusing connection previously named snapshot.azure_dbt_spark.product_snapshot
[0m21:34:08.572999 [debug] [Thread-2 (]: Began compiling node snapshot.azure_dbt_spark.salesorderdetail_snapshot
[0m21:34:08.577008 [debug] [Thread-2 (]: Began executing node snapshot.azure_dbt_spark.salesorderdetail_snapshot
[0m21:34:08.583008 [debug] [Thread-2 (]: Using databricks connection "snapshot.azure_dbt_spark.salesorderdetail_snapshot"
[0m21:34:08.583008 [debug] [Thread-2 (]: On snapshot.azure_dbt_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.salesorderdetail_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m21:34:08.871240 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.productmodel_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/productmodel/productmodel_snapshot doesn't exist, or is not a Delta table.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_PATH_DOES_NOT_EXIST] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/productmodel/productmodel_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:840)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:662)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:737)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:91)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:195)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:483)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:469)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:519)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/productmodel/productmodel_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:810)
	... 42 more
, operation-id=48c84234-e2c2-4b67-86c3-5a05f21add23
[0m21:34:08.873824 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
macro get_columns_in_relation_raw
: Database Error
  [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/productmodel/productmodel_snapshot doesn't exist, or is not a Delta table.
[0m21:34:08.879822 [debug] [Thread-1 (]: Database Error in snapshot productmodel_snapshot (snapshots\productmodel.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/productmodel/productmodel_snapshot doesn't exist, or is not a Delta table.
[0m21:34:08.879822 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a0e2051b-568c-4629-8ee1-a3299f64618e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D40F8B650>]}
[0m21:34:08.880814 [error] [Thread-1 (]: 5 of 7 ERROR snapshotting snapshots.productmodel_snapshot ...................... [[31mERROR[0m in 0.34s]
[0m21:34:08.881778 [debug] [Thread-1 (]: Finished running node snapshot.azure_dbt_spark.productmodel_snapshot
[0m21:34:08.881778 [debug] [Thread-1 (]: Began running node snapshot.azure_dbt_spark.salesorderheader_snapshot
[0m21:34:08.882944 [debug] [Thread-5 (]: Marking all children of 'snapshot.azure_dbt_spark.productmodel_snapshot' to be skipped because of status 'error'.  Reason: Database Error in snapshot productmodel_snapshot (snapshots\productmodel.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/productmodel/productmodel_snapshot doesn't exist, or is not a Delta table..
[0m21:34:08.881778 [info ] [Thread-1 (]: 7 of 7 START snapshot snapshots.salesorderheader_snapshot ...................... [RUN]
[0m21:34:08.882944 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.azure_dbt_spark.productmodel_snapshot, now snapshot.azure_dbt_spark.salesorderheader_snapshot)
[0m21:34:08.884037 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=640abe9a-0574-46b8-a86e-ec17691886a0, name=snapshot.azure_dbt_spark.salesorderheader_snapshot, idle-time=0.004215717315673828s, language=sql, compute-name=) - Reusing connection previously named snapshot.azure_dbt_spark.productmodel_snapshot
[0m21:34:08.884037 [debug] [Thread-1 (]: Began compiling node snapshot.azure_dbt_spark.salesorderheader_snapshot
[0m21:34:08.970023 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.salesorderdetail_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderdetail/salesorderdetail_snapshot doesn't exist, or is not a Delta table.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_PATH_DOES_NOT_EXIST] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderdetail/salesorderdetail_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:840)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:662)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:737)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:91)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:195)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:483)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:469)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:519)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderdetail/salesorderdetail_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:810)
	... 42 more
, operation-id=4cc088bb-8ae9-4301-8420-981f11819ee5
[0m21:34:08.971015 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
macro get_columns_in_relation_raw
: Database Error
  [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderdetail/salesorderdetail_snapshot doesn't exist, or is not a Delta table.
[0m21:34:08.974002 [debug] [Thread-1 (]: Began executing node snapshot.azure_dbt_spark.salesorderheader_snapshot
[0m21:34:08.977917 [debug] [Thread-1 (]: Using databricks connection "snapshot.azure_dbt_spark.salesorderheader_snapshot"
[0m21:34:08.979011 [debug] [Thread-1 (]: On snapshot.azure_dbt_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.salesorderheader_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m21:34:08.982913 [debug] [Thread-2 (]: Database Error in snapshot salesorderdetail_snapshot (snapshots\salesorderdetail.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderdetail/salesorderdetail_snapshot doesn't exist, or is not a Delta table.
[0m21:34:08.982913 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a0e2051b-568c-4629-8ee1-a3299f64618e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D40F8B380>]}
[0m21:34:08.983914 [error] [Thread-2 (]: 6 of 7 ERROR snapshotting snapshots.salesorderdetail_snapshot .................. [[31mERROR[0m in 0.41s]
[0m21:34:08.984460 [debug] [Thread-2 (]: Finished running node snapshot.azure_dbt_spark.salesorderdetail_snapshot
[0m21:34:08.984460 [debug] [Thread-5 (]: Marking all children of 'snapshot.azure_dbt_spark.salesorderdetail_snapshot' to be skipped because of status 'error'.  Reason: Database Error in snapshot salesorderdetail_snapshot (snapshots\salesorderdetail.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderdetail/salesorderdetail_snapshot doesn't exist, or is not a Delta table..
[0m21:34:09.267457 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.salesorderheader_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderheader/salesorderheader_snapshot doesn't exist, or is not a Delta table.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_PATH_DOES_NOT_EXIST] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderheader/salesorderheader_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:840)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:662)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:737)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:91)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:195)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:483)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:469)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:519)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderheader/salesorderheader_snapshot doesn't exist, or is not a Delta table.
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:810)
	... 42 more
, operation-id=848d9699-7ade-43e5-bdab-347dfd6dd8f6
[0m21:34:09.268447 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
macro get_columns_in_relation_raw
: Database Error
  [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderheader/salesorderheader_snapshot doesn't exist, or is not a Delta table.
[0m21:34:09.271875 [debug] [Thread-1 (]: Database Error in snapshot salesorderheader_snapshot (snapshots\salesorderheader.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderheader/salesorderheader_snapshot doesn't exist, or is not a Delta table.
[0m21:34:09.271875 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a0e2051b-568c-4629-8ee1-a3299f64618e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D3F8B16D0>]}
[0m21:34:09.272957 [error] [Thread-1 (]: 7 of 7 ERROR snapshotting snapshots.salesorderheader_snapshot .................. [[31mERROR[0m in 0.39s]
[0m21:34:09.273869 [debug] [Thread-1 (]: Finished running node snapshot.azure_dbt_spark.salesorderheader_snapshot
[0m21:34:09.274957 [debug] [Thread-5 (]: Marking all children of 'snapshot.azure_dbt_spark.salesorderheader_snapshot' to be skipped because of status 'error'.  Reason: Database Error in snapshot salesorderheader_snapshot (snapshots\salesorderheader.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderheader/salesorderheader_snapshot doesn't exist, or is not a Delta table..
[0m21:34:09.275873 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=2.3019661903381348s, language=None, compute-name=) - Reusing connection previously named master
[0m21:34:09.276942 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:34:09.276942 [debug] [MainThread]: Connection 'list_hive_metastore' was properly closed.
[0m21:34:09.276942 [debug] [MainThread]: On list_hive_metastore: Close
[0m21:34:09.277869 [debug] [MainThread]: Databricks adapter: Connection(session-id=52d2a03c-54ac-4272-b76b-286b07a0ebbd) - Closing
[0m21:34:09.463794 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m21:34:09.464796 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m21:34:09.464796 [debug] [MainThread]: Databricks adapter: Connection(session-id=b3cc07b7-c381-4e55-ae25-bcf1b2ed978a) - Closing
[0m21:34:09.646888 [debug] [MainThread]: Connection 'snapshot.azure_dbt_spark.salesorderheader_snapshot' was properly closed.
[0m21:34:09.647889 [debug] [MainThread]: On snapshot.azure_dbt_spark.salesorderheader_snapshot: Close
[0m21:34:09.647889 [debug] [MainThread]: Databricks adapter: Connection(session-id=640abe9a-0574-46b8-a86e-ec17691886a0) - Closing
[0m21:34:09.840684 [debug] [MainThread]: Connection 'snapshot.azure_dbt_spark.salesorderdetail_snapshot' was properly closed.
[0m21:34:09.841683 [debug] [MainThread]: On snapshot.azure_dbt_spark.salesorderdetail_snapshot: Close
[0m21:34:09.842684 [debug] [MainThread]: Databricks adapter: Connection(session-id=f1610eab-728e-4ab1-8786-c4f39da0e598) - Closing
[0m21:34:10.036823 [info ] [MainThread]: 
[0m21:34:10.038882 [info ] [MainThread]: Finished running 7 snapshots in 0 hours 0 minutes and 5.65 seconds (5.65s).
[0m21:34:10.043927 [debug] [MainThread]: Command end result
[0m21:34:10.071193 [debug] [MainThread]: Wrote artifact WritableManifest to H:\Acadmy\My\Projects\Azure_Spark_DBT\azure_dbt_spark\target\manifest.json
[0m21:34:10.073600 [debug] [MainThread]: Wrote artifact SemanticManifest to H:\Acadmy\My\Projects\Azure_Spark_DBT\azure_dbt_spark\target\semantic_manifest.json
[0m21:34:10.079521 [debug] [MainThread]: Wrote artifact RunExecutionResult to H:\Acadmy\My\Projects\Azure_Spark_DBT\azure_dbt_spark\target\run_results.json
[0m21:34:10.079521 [info ] [MainThread]: 
[0m21:34:10.080618 [info ] [MainThread]: [31mCompleted with 7 errors, 0 partial successes, and 0 warnings:[0m
[0m21:34:10.080618 [info ] [MainThread]: 
[0m21:34:10.081513 [error] [MainThread]:   Database Error in snapshot address_snapshot (snapshots\address.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/address/address_snapshot doesn't exist, or is not a Delta table.
[0m21:34:10.082555 [info ] [MainThread]: 
[0m21:34:10.083534 [error] [MainThread]:   Database Error in snapshot customer_snapshot (snapshots\customer.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customer/customer_snapshot doesn't exist, or is not a Delta table.
[0m21:34:10.084550 [info ] [MainThread]: 
[0m21:34:10.085521 [error] [MainThread]:   Database Error in snapshot customeraddress_snapshot (snapshots\customeraddress.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/customeraddress/customeraddress_snapshot doesn't exist, or is not a Delta table.
[0m21:34:10.086520 [info ] [MainThread]: 
[0m21:34:10.086520 [error] [MainThread]:   Database Error in snapshot product_snapshot (snapshots\product.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/product/product_snapshot doesn't exist, or is not a Delta table.
[0m21:34:10.087520 [info ] [MainThread]: 
[0m21:34:10.088519 [error] [MainThread]:   Database Error in snapshot productmodel_snapshot (snapshots\productmodel.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/productmodel/productmodel_snapshot doesn't exist, or is not a Delta table.
[0m21:34:10.088519 [info ] [MainThread]: 
[0m21:34:10.089517 [error] [MainThread]:   Database Error in snapshot salesorderdetail_snapshot (snapshots\salesorderdetail.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderdetail/salesorderdetail_snapshot doesn't exist, or is not a Delta table.
[0m21:34:10.089517 [info ] [MainThread]: 
[0m21:34:10.090544 [error] [MainThread]:   Database Error in snapshot salesorderheader_snapshot (snapshots\salesorderheader.sql)
  Database Error
    [DELTA_PATH_DOES_NOT_EXIST] dbfs:/mnt/silver/salesorderheader/salesorderheader_snapshot doesn't exist, or is not a Delta table.
[0m21:34:10.091517 [info ] [MainThread]: 
[0m21:34:10.091517 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=7 SKIP=0 TOTAL=7
[0m21:34:10.093517 [debug] [MainThread]: Command `dbt snapshot` failed at 21:34:10.092515 after 7.98 seconds
[0m21:34:10.093517 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D2BE10500>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D2BF1C170>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D40F8E9C0>]}
[0m21:34:10.094529 [debug] [MainThread]: Flushing usage events
[0m21:34:10.903366 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:38:58.210881 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0E3ED2FC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0E3ABA510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0E3AB8A70>]}


============================== 21:38:58.218095 | f7fbda05-c48e-4eea-b16a-1b6ae19784e6 ==============================
[0m21:38:58.218095 [info ] [MainThread]: Running with dbt=1.9.4
[0m21:38:58.219092 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'H:\\Acadmy\\My\\Projects\\Azure_Spark_DBT\\azure_dbt_spark\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\morsi\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt snapshot', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m21:38:59.044570 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m21:38:59.045503 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m21:38:59.045503 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m21:38:59.818495 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f7fbda05-c48e-4eea-b16a-1b6ae19784e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0F775E540>]}
[0m21:38:59.869493 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f7fbda05-c48e-4eea-b16a-1b6ae19784e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0F732AEA0>]}
[0m21:38:59.869493 [info ] [MainThread]: Registered adapter: databricks=1.10.1
[0m21:39:00.190270 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m21:39:00.341388 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:39:00.341388 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:39:00.348597 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.azure_dbt_spark.example
[0m21:39:00.376926 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f7fbda05-c48e-4eea-b16a-1b6ae19784e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0F7BDCE90>]}
[0m21:39:00.445466 [debug] [MainThread]: Wrote artifact WritableManifest to H:\Acadmy\My\Projects\Azure_Spark_DBT\azure_dbt_spark\target\manifest.json
[0m21:39:00.447748 [debug] [MainThread]: Wrote artifact SemanticManifest to H:\Acadmy\My\Projects\Azure_Spark_DBT\azure_dbt_spark\target\semantic_manifest.json
[0m21:39:00.479037 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f7fbda05-c48e-4eea-b16a-1b6ae19784e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0F7E502C0>]}
[0m21:39:00.480039 [info ] [MainThread]: Found 7 snapshots, 9 sources, 660 macros
[0m21:39:00.480039 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f7fbda05-c48e-4eea-b16a-1b6ae19784e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0F7B6AAB0>]}
[0m21:39:00.482040 [info ] [MainThread]: 
[0m21:39:00.483069 [info ] [MainThread]: Concurrency: 2 threads (target='dev')
[0m21:39:00.483069 [info ] [MainThread]: 
[0m21:39:00.484039 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m21:39:00.484039 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m21:39:00.491048 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_hive_metastore, idle-time=0s, language=None, compute-name=) - Creating connection
[0m21:39:00.492041 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m21:39:00.492041 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m21:39:00.493041 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m21:39:00.494040 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:39:02.158385 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=a9a11716-b0d5-4867-8b95-6b593cac72df) - Created
[0m21:39:03.150451 [debug] [ThreadPool]: SQL status: OK in 2.660 seconds
[0m21:39:03.153460 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=a9a11716-b0d5-4867-8b95-6b593cac72df, command-id=bad071f4-d6b4-4282-b534-c838360a320c) - Closing
[0m21:39:03.154457 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now create_hive_metastore_snapshots)
[0m21:39:03.154457 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=a9a11716-b0d5-4867-8b95-6b593cac72df, name=create_hive_metastore_snapshots, idle-time=0.0009965896606445312s, language=None, compute-name=) - Reusing connection previously named list_hive_metastore
[0m21:39:03.154457 [debug] [ThreadPool]: Creating schema "database: "hive_metastore"
schema: "snapshots"
"
[0m21:39:03.165144 [debug] [ThreadPool]: Using databricks connection "create_hive_metastore_snapshots"
[0m21:39:03.165144 [debug] [ThreadPool]: On create_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "connection_name": "create_hive_metastore_snapshots"} */
create schema if not exists `hive_metastore`.`snapshots`
  
[0m21:39:03.732401 [debug] [ThreadPool]: SQL status: OK in 0.570 seconds
[0m21:39:03.733399 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=a9a11716-b0d5-4867-8b95-6b593cac72df, command-id=846945a3-2002-4bb8-8a3d-4e0dd4ce83d7) - Closing
[0m21:39:03.734501 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_hive_metastore_snapshots, idle-time=0s, language=None, compute-name=) - Creating connection
[0m21:39:03.736322 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_snapshots'
[0m21:39:03.736322 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m21:39:03.736322 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots)
[0m21:39:03.736322 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:39:04.565963 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=664413d3-d8b9-45e2-9ab4-1ecc21e2c7ef) - Created
[0m21:39:04.820079 [debug] [ThreadPool]: SQL status: OK in 1.080 seconds
[0m21:39:04.821506 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=664413d3-d8b9-45e2-9ab4-1ecc21e2c7ef, command-id=cfed1ffa-90a2-48cc-b143-010c390b1e7e) - Closing
[0m21:39:04.822501 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f7fbda05-c48e-4eea-b16a-1b6ae19784e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0F7FDFD40>]}
[0m21:39:04.828927 [debug] [Thread-1 (]: Began running node snapshot.azure_dbt_spark.address_snapshot
[0m21:39:04.829937 [debug] [Thread-2 (]: Began running node snapshot.azure_dbt_spark.customer_snapshot
[0m21:39:04.829937 [info ] [Thread-1 (]: 1 of 7 START snapshot snapshots.address_snapshot ............................... [RUN]
[0m21:39:04.830982 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=snapshot.azure_dbt_spark.address_snapshot, idle-time=0s, language=None, compute-name=) - Creating connection
[0m21:39:04.830982 [debug] [Thread-1 (]: Acquiring new databricks connection 'snapshot.azure_dbt_spark.address_snapshot'
[0m21:39:04.831936 [debug] [Thread-1 (]: Began compiling node snapshot.azure_dbt_spark.address_snapshot
[0m21:39:04.838936 [debug] [Thread-1 (]: Began executing node snapshot.azure_dbt_spark.address_snapshot
[0m21:39:04.829937 [info ] [Thread-2 (]: 2 of 7 START snapshot snapshots.customer_snapshot .............................. [RUN]
[0m21:39:04.887692 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=snapshot.azure_dbt_spark.customer_snapshot, idle-time=0s, language=None, compute-name=) - Creating connection
[0m21:39:04.888660 [debug] [Thread-2 (]: Acquiring new databricks connection 'snapshot.azure_dbt_spark.customer_snapshot'
[0m21:39:04.888660 [debug] [Thread-2 (]: Began compiling node snapshot.azure_dbt_spark.customer_snapshot
[0m21:39:04.893607 [debug] [Thread-2 (]: Began executing node snapshot.azure_dbt_spark.customer_snapshot
[0m21:39:04.961314 [debug] [Thread-2 (]: Using databricks connection "snapshot.azure_dbt_spark.customer_snapshot"
[0m21:39:04.963229 [debug] [Thread-1 (]: Using databricks connection "snapshot.azure_dbt_spark.address_snapshot"
[0m21:39:04.964081 [debug] [Thread-2 (]: On snapshot.azure_dbt_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.customer_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(CustomerId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        with source_data as (
    select
        CustomerId,
        NameStyle,
        Title,
        FirstName,
        MiddleName,
        LastName,
        Suffix,
        CompanyName,
        SalesPerson,
        EmailAddress,
        Phone,
        PasswordHash,
        PasswordSalt
    from `hive_metastore`.`saleslt`.`customer`
)
select *
from source_data
    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

[0m21:39:04.964603 [debug] [Thread-1 (]: On snapshot.azure_dbt_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.address_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(AddressID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        with source_data as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`saleslt`.`address`
)
select *
from source_data
    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

[0m21:39:04.965117 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:39:04.965577 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m21:39:05.922547 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=8d2be102-71b3-41bd-af46-bee961ec614c) - Created
[0m21:39:06.395510 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=b5b74f1b-44aa-4836-a4f1-046099b9dacf) - Created
[0m21:39:06.796181 [debug] [Thread-1 (]: SQL status: OK in 1.830 seconds
[0m21:39:06.796181 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b5b74f1b-44aa-4836-a4f1-046099b9dacf, command-id=a0499f06-2f25-4c76-98bd-5ce8fcfb5a1b) - Closing
[0m21:39:06.799181 [debug] [Thread-1 (]: Using databricks connection "snapshot.azure_dbt_spark.address_snapshot"
[0m21:39:06.800295 [debug] [Thread-1 (]: On snapshot.azure_dbt_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.address_snapshot"} */
select * from (
        select 
    current_timestamp()
 as dbt_snapshot_time
    ) as __dbt_sbq
    where false
    limit 0

[0m21:39:07.009372 [debug] [Thread-2 (]: SQL status: OK in 2.040 seconds
[0m21:39:07.009372 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=8d2be102-71b3-41bd-af46-bee961ec614c, command-id=d1257301-e4e3-4a36-8520-34fff2e03b9b) - Closing
[0m21:39:07.010371 [debug] [Thread-2 (]: Using databricks connection "snapshot.azure_dbt_spark.customer_snapshot"
[0m21:39:07.011377 [debug] [Thread-2 (]: On snapshot.azure_dbt_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.customer_snapshot"} */
select * from (
        select 
    current_timestamp()
 as dbt_snapshot_time
    ) as __dbt_sbq
    where false
    limit 0

[0m21:39:07.065594 [debug] [Thread-1 (]: SQL status: OK in 0.260 seconds
[0m21:39:07.066591 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b5b74f1b-44aa-4836-a4f1-046099b9dacf, command-id=e4240fb7-4457-45c1-92fa-48ca2c0f0c4c) - Closing
[0m21:39:07.067594 [debug] [Thread-1 (]: Writing runtime sql for node "snapshot.azure_dbt_spark.address_snapshot"
[0m21:39:07.068582 [debug] [Thread-1 (]: Using databricks connection "snapshot.azure_dbt_spark.address_snapshot"
[0m21:39:07.069502 [debug] [Thread-1 (]: On snapshot.azure_dbt_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.address_snapshot"} */

      
  
    
        create or replace table `hive_metastore`.`snapshots`.`address_snapshot`
      
      using delta
      
      
      
      
      
    location '/mnt/silver/address/address_snapshot'
      
      
      as
      
    

    select *,
        md5(coalesce(cast(AddressID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        with source_data as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`saleslt`.`address`
)
select *
from source_data
    ) sbq



  
  
[0m21:39:07.265875 [debug] [Thread-2 (]: SQL status: OK in 0.250 seconds
[0m21:39:07.266876 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=8d2be102-71b3-41bd-af46-bee961ec614c, command-id=7d9a69a8-6bab-4d6a-842c-57d5f95655ac) - Closing
[0m21:39:07.267888 [debug] [Thread-2 (]: Writing runtime sql for node "snapshot.azure_dbt_spark.customer_snapshot"
[0m21:39:07.268873 [debug] [Thread-2 (]: Using databricks connection "snapshot.azure_dbt_spark.customer_snapshot"
[0m21:39:07.268873 [debug] [Thread-2 (]: On snapshot.azure_dbt_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.customer_snapshot"} */

      
  
    
        create or replace table `hive_metastore`.`snapshots`.`customer_snapshot`
      
      using delta
      
      
      
      
      
    location '/mnt/silver/customer/customer_snapshot'
      
      
      as
      
    

    select *,
        md5(coalesce(cast(CustomerId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        with source_data as (
    select
        CustomerId,
        NameStyle,
        Title,
        FirstName,
        MiddleName,
        LastName,
        Suffix,
        CompanyName,
        SalesPerson,
        EmailAddress,
        Phone,
        PasswordHash,
        PasswordSalt
    from `hive_metastore`.`saleslt`.`customer`
)
select *
from source_data
    ) sbq



  
  
[0m21:39:12.949454 [debug] [Thread-1 (]: SQL status: OK in 5.880 seconds
[0m21:39:12.951032 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b5b74f1b-44aa-4836-a4f1-046099b9dacf, command-id=9675080f-229b-4b25-94d2-008b7ddf05ef) - Closing
[0m21:39:13.170675 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f7fbda05-c48e-4eea-b16a-1b6ae19784e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0E3E9C3B0>]}
[0m21:39:13.171650 [info ] [Thread-1 (]: 1 of 7 OK snapshotted snapshots.address_snapshot ............................... [[32mOK[0m in 8.34s]
[0m21:39:13.172681 [debug] [Thread-1 (]: Finished running node snapshot.azure_dbt_spark.address_snapshot
[0m21:39:13.173649 [debug] [Thread-1 (]: Began running node snapshot.azure_dbt_spark.customeraddress_snapshot
[0m21:39:13.175603 [debug] [Thread-2 (]: SQL status: OK in 5.910 seconds
[0m21:39:13.176853 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=8d2be102-71b3-41bd-af46-bee961ec614c, command-id=aa059e60-bcc5-4f86-a288-e293da83b6e5) - Closing
[0m21:39:13.173649 [info ] [Thread-1 (]: 3 of 7 START snapshot snapshots.customeraddress_snapshot ....................... [RUN]
[0m21:39:13.177859 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.azure_dbt_spark.address_snapshot, now snapshot.azure_dbt_spark.customeraddress_snapshot)
[0m21:39:13.177859 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=b5b74f1b-44aa-4836-a4f1-046099b9dacf, name=snapshot.azure_dbt_spark.customeraddress_snapshot, idle-time=0.008314132690429688s, language=sql, compute-name=) - Reusing connection previously named snapshot.azure_dbt_spark.address_snapshot
[0m21:39:13.177859 [debug] [Thread-1 (]: Began compiling node snapshot.azure_dbt_spark.customeraddress_snapshot
[0m21:39:13.182461 [debug] [Thread-1 (]: Began executing node snapshot.azure_dbt_spark.customeraddress_snapshot
[0m21:39:13.186462 [debug] [Thread-1 (]: Using databricks connection "snapshot.azure_dbt_spark.customeraddress_snapshot"
[0m21:39:13.186462 [debug] [Thread-1 (]: On snapshot.azure_dbt_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.customeraddress_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(CustomerId||'-'||AddressId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        with source_data as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`saleslt`.`customeraddress`
)
select *
from source_data
    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

[0m21:39:13.409648 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f7fbda05-c48e-4eea-b16a-1b6ae19784e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0F811A240>]}
[0m21:39:13.411712 [info ] [Thread-2 (]: 2 of 7 OK snapshotted snapshots.customer_snapshot .............................. [[32mOK[0m in 8.52s]
[0m21:39:13.412266 [debug] [Thread-2 (]: Finished running node snapshot.azure_dbt_spark.customer_snapshot
[0m21:39:13.413427 [debug] [Thread-2 (]: Began running node snapshot.azure_dbt_spark.product_snapshot
[0m21:39:13.413427 [info ] [Thread-2 (]: 4 of 7 START snapshot snapshots.product_snapshot ............................... [RUN]
[0m21:39:13.414273 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly snapshot.azure_dbt_spark.customer_snapshot, now snapshot.azure_dbt_spark.product_snapshot)
[0m21:39:13.414273 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=8d2be102-71b3-41bd-af46-bee961ec614c, name=snapshot.azure_dbt_spark.product_snapshot, idle-time=0.005628108978271484s, language=sql, compute-name=) - Reusing connection previously named snapshot.azure_dbt_spark.customer_snapshot
[0m21:39:13.414273 [debug] [Thread-2 (]: Began compiling node snapshot.azure_dbt_spark.product_snapshot
[0m21:39:13.417366 [debug] [Thread-2 (]: Began executing node snapshot.azure_dbt_spark.product_snapshot
[0m21:39:13.424756 [debug] [Thread-2 (]: Using databricks connection "snapshot.azure_dbt_spark.product_snapshot"
[0m21:39:13.425762 [debug] [Thread-2 (]: On snapshot.azure_dbt_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.product_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(ProductID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        with product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
)

select * from product_snapshot
    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

[0m21:39:13.515587 [debug] [Thread-1 (]: SQL status: OK in 0.330 seconds
[0m21:39:13.516587 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b5b74f1b-44aa-4836-a4f1-046099b9dacf, command-id=8beda440-f9bf-405b-90f7-9a92742e9d0f) - Closing
[0m21:39:13.517586 [debug] [Thread-1 (]: Using databricks connection "snapshot.azure_dbt_spark.customeraddress_snapshot"
[0m21:39:13.517586 [debug] [Thread-1 (]: On snapshot.azure_dbt_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.customeraddress_snapshot"} */
select * from (
        select 
    current_timestamp()
 as dbt_snapshot_time
    ) as __dbt_sbq
    where false
    limit 0

[0m21:39:13.793831 [debug] [Thread-2 (]: SQL status: OK in 0.370 seconds
[0m21:39:13.794874 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=8d2be102-71b3-41bd-af46-bee961ec614c, command-id=6a65e5d0-e303-4386-a472-8352139d122c) - Closing
[0m21:39:13.795878 [debug] [Thread-2 (]: Using databricks connection "snapshot.azure_dbt_spark.product_snapshot"
[0m21:39:13.796871 [debug] [Thread-2 (]: On snapshot.azure_dbt_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.product_snapshot"} */
select * from (
        select 
    current_timestamp()
 as dbt_snapshot_time
    ) as __dbt_sbq
    where false
    limit 0

[0m21:39:13.800968 [debug] [Thread-1 (]: SQL status: OK in 0.280 seconds
[0m21:39:13.801967 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b5b74f1b-44aa-4836-a4f1-046099b9dacf, command-id=cf8d3cae-cfd4-49c8-94a5-8077250a1340) - Closing
[0m21:39:13.801967 [debug] [Thread-1 (]: Writing runtime sql for node "snapshot.azure_dbt_spark.customeraddress_snapshot"
[0m21:39:13.803966 [debug] [Thread-1 (]: Using databricks connection "snapshot.azure_dbt_spark.customeraddress_snapshot"
[0m21:39:13.803966 [debug] [Thread-1 (]: On snapshot.azure_dbt_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.customeraddress_snapshot"} */

      
  
    
        create or replace table `hive_metastore`.`snapshots`.`customeraddress_snapshot`
      
      using delta
      
      
      
      
      
    location '/mnt/silver/customeraddress/customeraddress_snapshot'
      
      
      as
      
    

    select *,
        md5(coalesce(cast(CustomerId||'-'||AddressId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        with source_data as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`saleslt`.`customeraddress`
)
select *
from source_data
    ) sbq



  
  
[0m21:39:14.021660 [debug] [Thread-2 (]: SQL status: OK in 0.220 seconds
[0m21:39:14.022767 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=8d2be102-71b3-41bd-af46-bee961ec614c, command-id=f57d4684-f168-430b-8e3f-4b05aba3562a) - Closing
[0m21:39:14.023763 [debug] [Thread-2 (]: Writing runtime sql for node "snapshot.azure_dbt_spark.product_snapshot"
[0m21:39:14.025676 [debug] [Thread-2 (]: Using databricks connection "snapshot.azure_dbt_spark.product_snapshot"
[0m21:39:14.025676 [debug] [Thread-2 (]: On snapshot.azure_dbt_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.product_snapshot"} */

      
  
    
        create or replace table `hive_metastore`.`snapshots`.`product_snapshot`
      
      using delta
      
      
      
      
      
    location '/mnt/silver/product/product_snapshot'
      
      
      as
      
    

    select *,
        md5(coalesce(cast(ProductID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        with product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
)

select * from product_snapshot
    ) sbq



  
  
[0m21:39:18.523413 [debug] [Thread-2 (]: SQL status: OK in 4.500 seconds
[0m21:39:18.526313 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=8d2be102-71b3-41bd-af46-bee961ec614c, command-id=ea76305e-6b03-4812-826a-6d8c66c88230) - Closing
[0m21:39:18.530012 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f7fbda05-c48e-4eea-b16a-1b6ae19784e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0F94F6450>]}
[0m21:39:18.531114 [info ] [Thread-2 (]: 4 of 7 OK snapshotted snapshots.product_snapshot ............................... [[32mOK[0m in 5.12s]
[0m21:39:18.533116 [debug] [Thread-2 (]: Finished running node snapshot.azure_dbt_spark.product_snapshot
[0m21:39:18.534108 [debug] [Thread-2 (]: Began running node snapshot.azure_dbt_spark.productmodel_snapshot
[0m21:39:18.536101 [info ] [Thread-2 (]: 5 of 7 START snapshot snapshots.productmodel_snapshot .......................... [RUN]
[0m21:39:18.537117 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly snapshot.azure_dbt_spark.product_snapshot, now snapshot.azure_dbt_spark.productmodel_snapshot)
[0m21:39:18.538100 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=8d2be102-71b3-41bd-af46-bee961ec614c, name=snapshot.azure_dbt_spark.productmodel_snapshot, idle-time=0.008088111877441406s, language=sql, compute-name=) - Reusing connection previously named snapshot.azure_dbt_spark.product_snapshot
[0m21:39:18.539717 [debug] [Thread-2 (]: Began compiling node snapshot.azure_dbt_spark.productmodel_snapshot
[0m21:39:18.544815 [debug] [Thread-2 (]: Began executing node snapshot.azure_dbt_spark.productmodel_snapshot
[0m21:39:18.549795 [debug] [Thread-2 (]: Using databricks connection "snapshot.azure_dbt_spark.productmodel_snapshot"
[0m21:39:18.550838 [debug] [Thread-2 (]: On snapshot.azure_dbt_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.productmodel_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(ProductModelID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        with product_snapshot as (
    SELECT
        ProductModelID,
        Name,
        CatalogDescription
    FROM `hive_metastore`.`saleslt`.`productmodel`
)

select * from product_snapshot
    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

[0m21:39:18.888189 [debug] [Thread-2 (]: SQL status: OK in 0.340 seconds
[0m21:39:18.891275 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=8d2be102-71b3-41bd-af46-bee961ec614c, command-id=d6fa4677-3314-4d35-8c4c-4d4b21b55fa7) - Closing
[0m21:39:18.893362 [debug] [Thread-2 (]: Using databricks connection "snapshot.azure_dbt_spark.productmodel_snapshot"
[0m21:39:18.894269 [debug] [Thread-2 (]: On snapshot.azure_dbt_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.productmodel_snapshot"} */
select * from (
        select 
    current_timestamp()
 as dbt_snapshot_time
    ) as __dbt_sbq
    where false
    limit 0

[0m21:39:19.148694 [debug] [Thread-2 (]: SQL status: OK in 0.250 seconds
[0m21:39:19.150266 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=8d2be102-71b3-41bd-af46-bee961ec614c, command-id=2d01c4b6-4d1d-4fc6-a0f8-af0e1bbd99e9) - Closing
[0m21:39:19.152366 [debug] [Thread-2 (]: Writing runtime sql for node "snapshot.azure_dbt_spark.productmodel_snapshot"
[0m21:39:19.155271 [debug] [Thread-2 (]: Using databricks connection "snapshot.azure_dbt_spark.productmodel_snapshot"
[0m21:39:19.157356 [debug] [Thread-2 (]: On snapshot.azure_dbt_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.productmodel_snapshot"} */

      
  
    
        create or replace table `hive_metastore`.`snapshots`.`productmodel_snapshot`
      
      using delta
      
      
      
      
      
    location '/mnt/silver/productmodel/productmodel_snapshot'
      
      
      as
      
    

    select *,
        md5(coalesce(cast(ProductModelID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        with product_snapshot as (
    SELECT
        ProductModelID,
        Name,
        CatalogDescription
    FROM `hive_metastore`.`saleslt`.`productmodel`
)

select * from product_snapshot
    ) sbq



  
  
[0m21:39:20.462681 [debug] [Thread-1 (]: SQL status: OK in 6.660 seconds
[0m21:39:20.465702 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b5b74f1b-44aa-4836-a4f1-046099b9dacf, command-id=563e272f-3647-41f0-9f3d-e5ba714aa29e) - Closing
[0m21:39:20.667798 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f7fbda05-c48e-4eea-b16a-1b6ae19784e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0F812EAB0>]}
[0m21:39:20.670365 [info ] [Thread-1 (]: 3 of 7 OK snapshotted snapshots.customeraddress_snapshot ....................... [[32mOK[0m in 7.49s]
[0m21:39:20.672385 [debug] [Thread-1 (]: Finished running node snapshot.azure_dbt_spark.customeraddress_snapshot
[0m21:39:20.673372 [debug] [Thread-1 (]: Began running node snapshot.azure_dbt_spark.salesorderdetail_snapshot
[0m21:39:20.674894 [info ] [Thread-1 (]: 6 of 7 START snapshot snapshots.salesorderdetail_snapshot ...................... [RUN]
[0m21:39:20.675908 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.azure_dbt_spark.customeraddress_snapshot, now snapshot.azure_dbt_spark.salesorderdetail_snapshot)
[0m21:39:20.675908 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=b5b74f1b-44aa-4836-a4f1-046099b9dacf, name=snapshot.azure_dbt_spark.salesorderdetail_snapshot, idle-time=0.008109807968139648s, language=sql, compute-name=) - Reusing connection previously named snapshot.azure_dbt_spark.customeraddress_snapshot
[0m21:39:20.676910 [debug] [Thread-1 (]: Began compiling node snapshot.azure_dbt_spark.salesorderdetail_snapshot
[0m21:39:20.681580 [debug] [Thread-1 (]: Began executing node snapshot.azure_dbt_spark.salesorderdetail_snapshot
[0m21:39:20.687555 [debug] [Thread-1 (]: Using databricks connection "snapshot.azure_dbt_spark.salesorderdetail_snapshot"
[0m21:39:20.688566 [debug] [Thread-1 (]: On snapshot.azure_dbt_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.salesorderdetail_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(SalesOrderDetailID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`saleslt`.`salesorderdetail`
)

select * from salesorderdetail_snapshot
    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

[0m21:39:21.037257 [debug] [Thread-1 (]: SQL status: OK in 0.350 seconds
[0m21:39:21.038254 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b5b74f1b-44aa-4836-a4f1-046099b9dacf, command-id=06430311-d85e-4635-b1b5-045b455ef346) - Closing
[0m21:39:21.039856 [debug] [Thread-1 (]: Using databricks connection "snapshot.azure_dbt_spark.salesorderdetail_snapshot"
[0m21:39:21.040879 [debug] [Thread-1 (]: On snapshot.azure_dbt_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.salesorderdetail_snapshot"} */
select * from (
        select 
    current_timestamp()
 as dbt_snapshot_time
    ) as __dbt_sbq
    where false
    limit 0

[0m21:39:21.315942 [debug] [Thread-1 (]: SQL status: OK in 0.270 seconds
[0m21:39:21.315942 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b5b74f1b-44aa-4836-a4f1-046099b9dacf, command-id=702196ee-3b0d-4f71-a6b8-0ddabf90c5f7) - Closing
[0m21:39:21.318029 [debug] [Thread-1 (]: Writing runtime sql for node "snapshot.azure_dbt_spark.salesorderdetail_snapshot"
[0m21:39:21.319649 [debug] [Thread-1 (]: Using databricks connection "snapshot.azure_dbt_spark.salesorderdetail_snapshot"
[0m21:39:21.320659 [debug] [Thread-1 (]: On snapshot.azure_dbt_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.salesorderdetail_snapshot"} */

      
  
    
        create or replace table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
      
      using delta
      
      
      
      
      
    location '/mnt/silver/salesorderdetail/salesorderdetail_snapshot'
      
      
      as
      
    

    select *,
        md5(coalesce(cast(SalesOrderDetailID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`saleslt`.`salesorderdetail`
)

select * from salesorderdetail_snapshot
    ) sbq



  
  
[0m21:39:23.266161 [debug] [Thread-2 (]: SQL status: OK in 4.110 seconds
[0m21:39:23.269285 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=8d2be102-71b3-41bd-af46-bee961ec614c, command-id=bfb0b6af-5b2e-445c-b330-2aefc353d48b) - Closing
[0m21:39:23.273002 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f7fbda05-c48e-4eea-b16a-1b6ae19784e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0F9531310>]}
[0m21:39:23.274894 [info ] [Thread-2 (]: 5 of 7 OK snapshotted snapshots.productmodel_snapshot .......................... [[32mOK[0m in 4.74s]
[0m21:39:23.275895 [debug] [Thread-2 (]: Finished running node snapshot.azure_dbt_spark.productmodel_snapshot
[0m21:39:23.275895 [debug] [Thread-2 (]: Began running node snapshot.azure_dbt_spark.salesorderheader_snapshot
[0m21:39:23.276895 [info ] [Thread-2 (]: 7 of 7 START snapshot snapshots.salesorderheader_snapshot ...................... [RUN]
[0m21:39:23.276895 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly snapshot.azure_dbt_spark.productmodel_snapshot, now snapshot.azure_dbt_spark.salesorderheader_snapshot)
[0m21:39:23.277978 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=8d2be102-71b3-41bd-af46-bee961ec614c, name=snapshot.azure_dbt_spark.salesorderheader_snapshot, idle-time=0.004975795745849609s, language=sql, compute-name=) - Reusing connection previously named snapshot.azure_dbt_spark.productmodel_snapshot
[0m21:39:23.277978 [debug] [Thread-2 (]: Began compiling node snapshot.azure_dbt_spark.salesorderheader_snapshot
[0m21:39:23.280670 [debug] [Thread-2 (]: Began executing node snapshot.azure_dbt_spark.salesorderheader_snapshot
[0m21:39:23.285749 [debug] [Thread-2 (]: Using databricks connection "snapshot.azure_dbt_spark.salesorderheader_snapshot"
[0m21:39:23.286670 [debug] [Thread-2 (]: On snapshot.azure_dbt_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.salesorderheader_snapshot"} */
select * from (
        
    

    select *,
        md5(coalesce(cast(SalesOrderID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        with salesorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
)

select * from salesorderheader_snapshot
    ) sbq



    ) as __dbt_sbq
    where false
    limit 0

[0m21:39:23.652605 [debug] [Thread-2 (]: SQL status: OK in 0.370 seconds
[0m21:39:23.653599 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=8d2be102-71b3-41bd-af46-bee961ec614c, command-id=1707ca73-e7e2-49d3-a4a6-b231b28974c5) - Closing
[0m21:39:23.654600 [debug] [Thread-2 (]: Using databricks connection "snapshot.azure_dbt_spark.salesorderheader_snapshot"
[0m21:39:23.654600 [debug] [Thread-2 (]: On snapshot.azure_dbt_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.salesorderheader_snapshot"} */
select * from (
        select 
    current_timestamp()
 as dbt_snapshot_time
    ) as __dbt_sbq
    where false
    limit 0

[0m21:39:24.012692 [debug] [Thread-2 (]: SQL status: OK in 0.360 seconds
[0m21:39:24.013695 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=8d2be102-71b3-41bd-af46-bee961ec614c, command-id=8e5214c6-6700-48c1-9850-6a9e91643e4a) - Closing
[0m21:39:24.014700 [debug] [Thread-2 (]: Writing runtime sql for node "snapshot.azure_dbt_spark.salesorderheader_snapshot"
[0m21:39:24.016699 [debug] [Thread-2 (]: Using databricks connection "snapshot.azure_dbt_spark.salesorderheader_snapshot"
[0m21:39:24.016699 [debug] [Thread-2 (]: On snapshot.azure_dbt_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "snapshot.azure_dbt_spark.salesorderheader_snapshot"} */

      
  
    
        create or replace table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
      
      using delta
      
      
      
      
      
    location '/mnt/silver/salesorderheader/salesorderheader_snapshot'
      
      
      as
      
    

    select *,
        md5(coalesce(cast(SalesOrderID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        
  
  coalesce(nullif(
    current_timestamp()
, 
    current_timestamp()
), null)
  as dbt_valid_to
from (
        with salesorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
)

select * from salesorderheader_snapshot
    ) sbq



  
  
[0m21:39:25.555288 [debug] [Thread-1 (]: SQL status: OK in 4.230 seconds
[0m21:39:25.558289 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b5b74f1b-44aa-4836-a4f1-046099b9dacf, command-id=e68e2488-fc68-49c2-bab2-94c22789cc6e) - Closing
[0m21:39:25.562830 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f7fbda05-c48e-4eea-b16a-1b6ae19784e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0F80C6E40>]}
[0m21:39:25.564833 [info ] [Thread-1 (]: 6 of 7 OK snapshotted snapshots.salesorderdetail_snapshot ...................... [[32mOK[0m in 4.89s]
[0m21:39:25.565832 [debug] [Thread-1 (]: Finished running node snapshot.azure_dbt_spark.salesorderdetail_snapshot
[0m21:39:28.048025 [debug] [Thread-2 (]: SQL status: OK in 4.030 seconds
[0m21:39:28.050784 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=8d2be102-71b3-41bd-af46-bee961ec614c, command-id=363c4606-8ca8-499d-aebc-1c46b19e76b1) - Closing
[0m21:39:28.055737 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f7fbda05-c48e-4eea-b16a-1b6ae19784e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0F951DEE0>]}
[0m21:39:28.057748 [info ] [Thread-2 (]: 7 of 7 OK snapshotted snapshots.salesorderheader_snapshot ...................... [[32mOK[0m in 4.78s]
[0m21:39:28.059020 [debug] [Thread-2 (]: Finished running node snapshot.azure_dbt_spark.salesorderheader_snapshot
[0m21:39:28.062612 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=23.240110397338867s, language=None, compute-name=) - Reusing connection previously named master
[0m21:39:28.064139 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:39:28.064139 [debug] [MainThread]: Connection 'create_hive_metastore_snapshots' was properly closed.
[0m21:39:28.065137 [debug] [MainThread]: On create_hive_metastore_snapshots: Close
[0m21:39:28.066138 [debug] [MainThread]: Databricks adapter: Connection(session-id=a9a11716-b0d5-4867-8b95-6b593cac72df) - Closing
[0m21:39:28.270779 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m21:39:28.271833 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m21:39:28.272820 [debug] [MainThread]: Databricks adapter: Connection(session-id=664413d3-d8b9-45e2-9ab4-1ecc21e2c7ef) - Closing
[0m21:39:28.472240 [debug] [MainThread]: Connection 'snapshot.azure_dbt_spark.salesorderdetail_snapshot' was properly closed.
[0m21:39:28.473242 [debug] [MainThread]: On snapshot.azure_dbt_spark.salesorderdetail_snapshot: Close
[0m21:39:28.474725 [debug] [MainThread]: Databricks adapter: Connection(session-id=b5b74f1b-44aa-4836-a4f1-046099b9dacf) - Closing
[0m21:39:28.678088 [debug] [MainThread]: Connection 'snapshot.azure_dbt_spark.salesorderheader_snapshot' was properly closed.
[0m21:39:28.679605 [debug] [MainThread]: On snapshot.azure_dbt_spark.salesorderheader_snapshot: Close
[0m21:39:28.680623 [debug] [MainThread]: Databricks adapter: Connection(session-id=8d2be102-71b3-41bd-af46-bee961ec614c) - Closing
[0m21:39:28.884605 [info ] [MainThread]: 
[0m21:39:28.884605 [info ] [MainThread]: Finished running 7 snapshots in 0 hours 0 minutes and 28.40 seconds (28.40s).
[0m21:39:28.891155 [debug] [MainThread]: Command end result
[0m21:39:28.929986 [debug] [MainThread]: Wrote artifact WritableManifest to H:\Acadmy\My\Projects\Azure_Spark_DBT\azure_dbt_spark\target\manifest.json
[0m21:39:28.932988 [debug] [MainThread]: Wrote artifact SemanticManifest to H:\Acadmy\My\Projects\Azure_Spark_DBT\azure_dbt_spark\target\semantic_manifest.json
[0m21:39:28.938077 [debug] [MainThread]: Wrote artifact RunExecutionResult to H:\Acadmy\My\Projects\Azure_Spark_DBT\azure_dbt_spark\target\run_results.json
[0m21:39:28.938077 [info ] [MainThread]: 
[0m21:39:28.939582 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:39:28.940038 [info ] [MainThread]: 
[0m21:39:28.940038 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m21:39:28.942049 [debug] [MainThread]: Command `dbt snapshot` succeeded at 21:39:28.942049 after 30.96 seconds
[0m21:39:28.943058 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0E43ABB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0E47D48F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0E1DA5C40>]}
[0m21:39:28.944049 [debug] [MainThread]: Flushing usage events
[0m21:39:29.832649 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:48:10.300328 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002208F837B30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002208F837140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002208F8356D0>]}


============================== 21:48:10.309141 | f9418746-dcf7-4c9f-8efb-e6ac2ffe2984 ==============================
[0m21:48:10.309141 [info ] [MainThread]: Running with dbt=1.9.4
[0m21:48:10.309141 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'H:\\Acadmy\\My\\Projects\\Azure_Spark_DBT\\azure_dbt_spark\\logs', 'profiles_dir': 'C:\\Users\\morsi\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m21:48:11.058371 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m21:48:11.058371 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m21:48:11.059371 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m21:48:11.852649 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f9418746-dcf7-4c9f-8efb-e6ac2ffe2984', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000220A2E3ACF0>]}
[0m21:48:11.900311 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f9418746-dcf7-4c9f-8efb-e6ac2ffe2984', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022091466D80>]}
[0m21:48:11.900311 [info ] [MainThread]: Registered adapter: databricks=1.10.1
[0m21:48:12.217579 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m21:48:12.360060 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 6 files added, 0 files changed.
[0m21:48:12.360060 [debug] [MainThread]: Partial parsing: added file: azure_dbt_spark://models\customer\dim_customer.yml
[0m21:48:12.360060 [debug] [MainThread]: Partial parsing: added file: azure_dbt_spark://models\customer\dim_customer.sql
[0m21:48:12.360060 [debug] [MainThread]: Partial parsing: added file: azure_dbt_spark://models\sales\sales.yml
[0m21:48:12.360060 [debug] [MainThread]: Partial parsing: added file: azure_dbt_spark://models\product\dim_product.sql
[0m21:48:12.360060 [debug] [MainThread]: Partial parsing: added file: azure_dbt_spark://models\sales\sales.sql
[0m21:48:12.360060 [debug] [MainThread]: Partial parsing: added file: azure_dbt_spark://models\product\dim_product.yml
[0m21:48:12.614505 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'dim_customers' in the 'models' section of file 'models\customer\dim_customer.yml'
[0m21:48:12.725129 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'dim_sales' in the 'models' section of file 'models\sales\sales.yml'
[0m21:48:12.779739 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'dim_products' in the 'models' section of file 'models\product\dim_product.yml'
[0m21:48:12.787722 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.azure_dbt_spark.unique_dim_customers_customer_sk.22a014df62' (models\customer\dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m21:48:12.787722 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.azure_dbt_spark.not_null_dim_customers_customer_sk.8ae5836863' (models\customer\dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m21:48:12.789721 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.azure_dbt_spark.not_null_dim_customers_customerid.209fbdda85' (models\customer\dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m21:48:12.790380 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.azure_dbt_spark.not_null_dim_customers_AddressId.86b771f63e' (models\customer\dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m21:48:12.791442 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.azure_dbt_spark.unique_dim_sales_saleOrderID.810c5f247c' (models\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m21:48:12.791442 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.azure_dbt_spark.not_null_dim_sales_saleOrderID.48ce11e7f3' (models\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m21:48:12.792851 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.azure_dbt_spark.unique_dim_sales_saleOrderDetailID.343b942405' (models\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m21:48:12.793859 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.azure_dbt_spark.not_null_dim_sales_saleOrderDetailID.a60664de3a' (models\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m21:48:12.793859 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.azure_dbt_spark.not_null_dim_sales_orderQty.66af966596' (models\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m21:48:12.794860 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.azure_dbt_spark.not_null_dim_sales_productID.cbf6d34890' (models\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m21:48:12.795930 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.azure_dbt_spark.not_null_dim_sales_unitPrice.3545b5473a' (models\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m21:48:12.795930 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.azure_dbt_spark.not_null_dim_sales_lineTotal.d55bca27f8' (models\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m21:48:12.797202 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.azure_dbt_spark.not_null_dim_sales_name.4c7b961f77' (models\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m21:48:12.798213 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.azure_dbt_spark.not_null_dim_sales_productNumber.3a23a94ddd' (models\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m21:48:12.798213 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.azure_dbt_spark.not_null_dim_sales_standardCost.d3f58be9a3' (models\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m21:48:12.799212 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.azure_dbt_spark.not_null_dim_sales_listPrice.4ee58b9e3f' (models\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m21:48:12.800211 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.azure_dbt_spark.not_null_dim_sales_sellStartDate.b44c8ea118' (models\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m21:48:12.800211 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.azure_dbt_spark.not_null_dim_sales_orderDate.6f6f720ec3' (models\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m21:48:12.801214 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.azure_dbt_spark.not_null_dim_sales_customerID.60b0993af5' (models\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m21:48:12.802212 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.azure_dbt_spark.not_null_dim_sales_subTotal.bfeb62a487' (models\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m21:48:12.802212 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.azure_dbt_spark.not_null_dim_sales_taxAmt.94cff67d6a' (models\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m21:48:12.803573 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.azure_dbt_spark.not_null_dim_sales_freight.ca13e04131' (models\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m21:48:12.804582 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.azure_dbt_spark.not_null_dim_sales_totalDue.920571e023' (models\sales\sales.yml) depends on a node named 'dim_sales' in package '' which was not found
[0m21:48:12.804582 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.azure_dbt_spark.unique_dim_products_product_sk.8f20ac7c5b' (models\product\dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m21:48:12.806068 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.azure_dbt_spark.not_null_dim_products_product_sk.2a2df3e1b9' (models\product\dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m21:48:12.807124 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.azure_dbt_spark.not_null_dim_products_product_name.991aec73f3' (models\product\dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m21:48:12.807622 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.azure_dbt_spark.not_null_dim_products_sellstartdate.f97a265a0f' (models\product\dim_product.yml) depends on a node named 'dim_products' in package '' which was not found
[0m21:48:12.920967 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.azure_dbt_spark.example
[0m21:48:12.940353 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f9418746-dcf7-4c9f-8efb-e6ac2ffe2984', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000220A359C890>]}
[0m21:48:13.093966 [debug] [MainThread]: Wrote artifact WritableManifest to H:\Acadmy\My\Projects\Azure_Spark_DBT\azure_dbt_spark\target\manifest.json
[0m21:48:13.097992 [debug] [MainThread]: Wrote artifact SemanticManifest to H:\Acadmy\My\Projects\Azure_Spark_DBT\azure_dbt_spark\target\semantic_manifest.json
[0m21:48:13.134811 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f9418746-dcf7-4c9f-8efb-e6ac2ffe2984', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000220A46697F0>]}
[0m21:48:13.136823 [info ] [MainThread]: Found 7 snapshots, 3 models, 9 sources, 660 macros
[0m21:48:13.136823 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f9418746-dcf7-4c9f-8efb-e6ac2ffe2984', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000220A46AF740>]}
[0m21:48:13.139341 [info ] [MainThread]: 
[0m21:48:13.141349 [info ] [MainThread]: Concurrency: 2 threads (target='dev')
[0m21:48:13.141349 [info ] [MainThread]: 
[0m21:48:13.142977 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m21:48:13.143987 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m21:48:13.152933 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_hive_metastore, idle-time=0s, language=None, compute-name=) - Creating connection
[0m21:48:13.154948 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m21:48:13.155926 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m21:48:13.155926 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m21:48:13.157254 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:48:14.288294 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=a24e58da-80ea-42b8-92a3-dd7f9971fd02) - Created
[0m21:48:14.992222 [debug] [ThreadPool]: SQL status: OK in 1.830 seconds
[0m21:48:14.996253 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=a24e58da-80ea-42b8-92a3-dd7f9971fd02, command-id=0ac4fb9a-41fa-4b46-9ae7-92727ece5184) - Closing
[0m21:48:15.002300 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_hive_metastore_snapshots, idle-time=0s, language=None, compute-name=) - Creating connection
[0m21:48:15.004063 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_snapshots'
[0m21:48:15.004063 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_hive_metastore_saleslt, idle-time=0s, language=None, compute-name=) - Creating connection
[0m21:48:15.006078 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m21:48:15.006078 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m21:48:15.006078 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots)
[0m21:48:15.008090 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m21:48:15.008090 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:48:15.008090 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt)
[0m21:48:15.008090 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:48:15.917322 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=86b14148-16cf-4579-b79c-d6fcf68fe4bd) - Created
[0m21:48:15.923983 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=1d41e83b-3730-49dc-976b-c1298c96b5f4) - Created
[0m21:48:16.161947 [debug] [ThreadPool]: SQL status: OK in 1.150 seconds
[0m21:48:16.163934 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=1d41e83b-3730-49dc-976b-c1298c96b5f4, command-id=0cceb97d-305f-4d43-8c68-57d478671ac4) - Closing
[0m21:48:16.181320 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m21:48:16.181320 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

    
SELECT current_catalog()

  
[0m21:48:16.191743 [debug] [ThreadPool]: SQL status: OK in 1.180 seconds
[0m21:48:16.191743 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=86b14148-16cf-4579-b79c-d6fcf68fe4bd, command-id=49e7cd1e-5fc5-4303-9898-fd6ceec755cb) - Closing
[0m21:48:16.193754 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m21:48:16.195763 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

    
SELECT current_catalog()

  
[0m21:48:16.520115 [debug] [ThreadPool]: SQL status: OK in 0.340 seconds
[0m21:48:16.525910 [debug] [ThreadPool]: SQL status: OK in 0.330 seconds
[0m21:48:16.525910 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=1d41e83b-3730-49dc-976b-c1298c96b5f4, command-id=e10bc231-e41e-4286-8888-f59f5c5cfea5) - Closing
[0m21:48:16.537718 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=86b14148-16cf-4579-b79c-d6fcf68fe4bd, command-id=3cfcd292-2ccd-47dd-b31e-c83b85099320) - Closing
[0m21:48:16.539996 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m21:48:16.539996 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m21:48:16.539996 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

    
SHOW VIEWS IN `hive_metastore`.`saleslt`

  
[0m21:48:16.539996 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

    
SHOW VIEWS IN `hive_metastore`.`snapshots`

  
[0m21:48:16.890690 [debug] [ThreadPool]: SQL status: OK in 0.350 seconds
[0m21:48:16.891272 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=1d41e83b-3730-49dc-976b-c1298c96b5f4, command-id=cc04d680-8ca2-4742-a7f6-475d0663bb33) - Closing
[0m21:48:16.923722 [debug] [ThreadPool]: SQL status: OK in 0.380 seconds
[0m21:48:16.923722 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=86b14148-16cf-4579-b79c-d6fcf68fe4bd, command-id=0fa85958-b20d-47ba-aced-993804382fff) - Closing
[0m21:48:16.923722 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f9418746-dcf7-4c9f-8efb-e6ac2ffe2984', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000220A359F230>]}
[0m21:48:16.923722 [debug] [Thread-1 (]: Began running node model.azure_dbt_spark.dim_customer
[0m21:48:16.938708 [debug] [Thread-2 (]: Began running node model.azure_dbt_spark.dim_product
[0m21:48:16.938708 [info ] [Thread-2 (]: 2 of 3 START sql table model saleslt.dim_product ............................... [RUN]
[0m21:48:16.938708 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.azure_dbt_spark.dim_product, idle-time=0s, language=None, compute-name=) - Creating connection
[0m21:48:16.938708 [debug] [Thread-2 (]: Acquiring new databricks connection 'model.azure_dbt_spark.dim_product'
[0m21:48:16.938708 [debug] [Thread-2 (]: Began compiling node model.azure_dbt_spark.dim_product
[0m21:48:16.938708 [debug] [Thread-2 (]: Writing injected SQL for node "model.azure_dbt_spark.dim_product"
[0m21:48:16.923722 [info ] [Thread-1 (]: 1 of 3 START sql table model saleslt.dim_customer .............................. [RUN]
[0m21:48:16.938708 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.azure_dbt_spark.dim_customer, idle-time=0s, language=None, compute-name=) - Creating connection
[0m21:48:16.938708 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.azure_dbt_spark.dim_customer'
[0m21:48:16.938708 [debug] [Thread-1 (]: Began compiling node model.azure_dbt_spark.dim_customer
[0m21:48:17.039056 [debug] [Thread-1 (]: Writing injected SQL for node "model.azure_dbt_spark.dim_customer"
[0m21:48:17.040057 [debug] [Thread-2 (]: Began executing node model.azure_dbt_spark.dim_product
[0m21:48:17.056436 [debug] [Thread-2 (]: MATERIALIZING TABLE
[0m21:48:17.056943 [debug] [Thread-1 (]: Began executing node model.azure_dbt_spark.dim_customer
[0m21:48:17.058956 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m21:48:17.056943 [warn ] [Thread-2 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m21:48:17.060966 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'f9418746-dcf7-4c9f-8efb-e6ac2ffe2984', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000220A34F1D00>]}
[0m21:48:17.058956 [warn ] [Thread-1 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m21:48:17.092732 [debug] [Thread-2 (]: Writing runtime sql for node "model.azure_dbt_spark.dim_product"
[0m21:48:17.092732 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'f9418746-dcf7-4c9f-8efb-e6ac2ffe2984', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000220A34C6690>]}
[0m21:48:17.109551 [debug] [Thread-1 (]: Writing runtime sql for node "model.azure_dbt_spark.dim_customer"
[0m21:48:17.110594 [debug] [Thread-2 (]: Using databricks connection "model.azure_dbt_spark.dim_product"
[0m21:48:17.111548 [debug] [Thread-2 (]: On model.azure_dbt_spark.dim_product: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "model.azure_dbt_spark.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/products/dim_product'
      
      
      as
      with product_snapshot as (
    select
        productId,
        name,
        standardCost,
        listPrice,
        size,
        weight,
        productcategoryid,
        productmodelid,
        sellstartdate,
        sellenddate,
        discontinueddate
    from `hive_metastore`.`snapshots`.`product_snapshot`
    where dbt_valid_to is null
),

product_model_snapshot as (
    select
        productmodelid,
        name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot`
    where dbt_valid_to is null
),


transformed as (
    select
        row_number() over (order by p.productId) as product_sk,
        p.name as product_name,
        p.standardCost,
        p.listPrice,
        p.size,
        p.weight,
        pm.name as model,
        pm.CatalogDescription as description,
        p.sellstartdate,
        p.sellenddate,
        p.discontinueddate
    from product_snapshot p
    left join product_model_snapshot pm on p.productmodelid = pm.productmodelid
)

select * from transformed
  
[0m21:48:17.111548 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m21:48:17.112505 [debug] [Thread-1 (]: Using databricks connection "model.azure_dbt_spark.dim_customer"
[0m21:48:17.112505 [debug] [Thread-1 (]: On model.azure_dbt_spark.dim_customer: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "model.azure_dbt_spark.dim_customer"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_customer`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/customers/dim_customer'
      
      
      as
      with address_snapshot as (
    select
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`snapshots`.`address_snapshot` where dbt_valid_to is null
)

, customeraddress_snapshot as (
    select
        CustomerId,
        AddressId,
        AddressType
    from `hive_metastore`.`snapshots`.`customeraddress_snapshot` where dbt_valid_to is null
)

, customer_snapshot as (
    select
        CustomerId,
        concat(ifnull(FirstName,' '),' ',ifnull(MiddleName,' '),' ',ifnull(LastName,' ')) as FullName
    from `hive_metastore`.`snapshots`.`customer_snapshot` where dbt_valid_to is null
)

, transformed as (
    select
    row_number() over (order by customer_snapshot.customerid) as customer_sk, -- auto-incremental surrogate key
    customer_snapshot.CustomerId,
    customer_snapshot.fullname,
    customeraddress_snapshot.AddressID,
    customeraddress_snapshot.AddressType,
    address_snapshot.AddressLine1,
    address_snapshot.City,
    address_snapshot.StateProvince,
    address_snapshot.CountryRegion,
    address_snapshot.PostalCode
    from customer_snapshot
    inner join customeraddress_snapshot on customer_snapshot.CustomerId = customeraddress_snapshot.CustomerId
    inner join address_snapshot on customeraddress_snapshot.AddressID = address_snapshot.AddressID
)
select *
from transformed
  
[0m21:48:17.112505 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m21:48:17.859239 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=5d6a1653-1db3-4d64-b3ef-573dd3003288) - Created
[0m21:48:17.881265 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=7175ea01-5393-4517-9903-af36a76f39a1) - Created
[0m21:48:25.581207 [debug] [Thread-1 (]: SQL status: OK in 8.470 seconds
[0m21:48:25.597159 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=5d6a1653-1db3-4d64-b3ef-573dd3003288, command-id=c4e06f1a-48af-4ee4-a144-0c93dcbe9a73) - Closing
[0m21:48:25.791450 [debug] [Thread-1 (]: Applying tags to relation None []
[0m21:48:25.805231 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f9418746-dcf7-4c9f-8efb-e6ac2ffe2984', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002208CDE71A0>]}
[0m21:48:25.807241 [info ] [Thread-1 (]: 1 of 3 OK created sql table model saleslt.dim_customer ......................... [[32mOK[0m in 8.87s]
[0m21:48:25.807241 [debug] [Thread-1 (]: Finished running node model.azure_dbt_spark.dim_customer
[0m21:48:25.807241 [debug] [Thread-1 (]: Began running node model.azure_dbt_spark.sales
[0m21:48:25.807241 [info ] [Thread-1 (]: 3 of 3 START sql table model saleslt.sales ..................................... [RUN]
[0m21:48:25.807241 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.azure_dbt_spark.dim_customer, now model.azure_dbt_spark.sales)
[0m21:48:25.807241 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=5d6a1653-1db3-4d64-b3ef-573dd3003288, name=model.azure_dbt_spark.sales, idle-time=0.002009868621826172s, language=sql, compute-name=) - Reusing connection previously named model.azure_dbt_spark.dim_customer
[0m21:48:25.807241 [debug] [Thread-1 (]: Began compiling node model.azure_dbt_spark.sales
[0m21:48:25.807241 [debug] [Thread-1 (]: Writing injected SQL for node "model.azure_dbt_spark.sales"
[0m21:48:25.807241 [debug] [Thread-1 (]: Began executing node model.azure_dbt_spark.sales
[0m21:48:25.807241 [debug] [Thread-1 (]: MATERIALIZING TABLE
[0m21:48:25.807241 [debug] [Thread-1 (]: Writing runtime sql for node "model.azure_dbt_spark.sales"
[0m21:48:25.824165 [debug] [Thread-1 (]: Using databricks connection "model.azure_dbt_spark.sales"
[0m21:48:25.824165 [debug] [Thread-1 (]: On model.azure_dbt_spark.sales: /* {"app": "dbt", "dbt_version": "1.9.4", "dbt_databricks_version": "1.10.1", "databricks_sql_connector_version": "4.0.3", "profile_name": "azure_dbt_spark", "target_name": "dev", "node_id": "model.azure_dbt_spark.sales"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`sales`
      
      using delta
      
      
      
      
      
    location '/mnt/gold/sales/sales'
      
      
      as
      with salesorderdetail_snapshot as (
    SELECT
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    FROM `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
),

product_snapshot as (
    SELECT
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    FROM `hive_metastore`.`saleslt`.`product`
),

saleorderheader_snapshot as (
    SELECT
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment,
        row_number() over (partition by SalesOrderID order by SalesOrderID) as row_num
    FROM `hive_metastore`.`saleslt`.`salesorderheader`
),

transformed as (
    select
        sod.SalesOrderID,
        sod.SalesOrderDetailID,
        sod.OrderQty,
        sod.ProductID,
        sod.UnitPrice,
        sod.UnitPriceDiscount,
        sod.LineTotal,
        p.Name,
        p.ProductNumber,
        p.Color,
        p.StandardCost,
        p.ListPrice,
        p.Size,
        p.Weight,
        p.SellStartDate,
        p.SellEndDate,
        p.DiscontinuedDate,
        p.ThumbNailPhoto,
        p.ThumbnailPhotoFileName,
        soh.RevisionNumber,
        soh.OrderDate,
        soh.DueDate,
        soh.ShipDate,
        soh.Status,
        soh.OnlineOrderFlag,
        soh.SalesOrderNumber,
        soh.PurchaseOrderNumber,
        soh.AccountNumber,
        soh.CustomerID,
        soh.ShipToAddressID,
        soh.BillToAddressID,
        soh.ShipMethod,
        soh.CreditCardApprovalCode,
        soh.SubTotal,
        soh.TaxAmt,
        soh.Freight,
        soh.TotalDue,
        soh.Comment
    from salesorderdetail_snapshot sod
    left join product_snapshot p on sod.ProductID = p.ProductID
    left join saleorderheader_snapshot soh on sod.SalesOrderID = soh.SalesOrderID
)

select * from transformed
  
[0m21:48:27.036464 [debug] [Thread-2 (]: SQL status: OK in 9.920 seconds
[0m21:48:27.036464 [debug] [Thread-2 (]: Databricks adapter: Cursor(session-id=7175ea01-5393-4517-9903-af36a76f39a1, command-id=291e5e37-658b-4ec1-90b5-090d6791fd4f) - Closing
[0m21:48:27.245404 [debug] [Thread-2 (]: Applying tags to relation None []
[0m21:48:27.247400 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f9418746-dcf7-4c9f-8efb-e6ac2ffe2984', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000220A34EF620>]}
[0m21:48:27.248401 [info ] [Thread-2 (]: 2 of 3 OK created sql table model saleslt.dim_product .......................... [[32mOK[0m in 10.31s]
[0m21:48:27.249105 [debug] [Thread-2 (]: Finished running node model.azure_dbt_spark.dim_product
[0m21:48:30.704942 [debug] [Thread-1 (]: SQL status: OK in 4.880 seconds
[0m21:48:30.704942 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=5d6a1653-1db3-4d64-b3ef-573dd3003288, command-id=2b875ffb-aec7-4c0e-b5d7-a9e9eac44164) - Closing
[0m21:48:30.704942 [debug] [Thread-1 (]: Applying tags to relation None []
[0m21:48:30.704942 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f9418746-dcf7-4c9f-8efb-e6ac2ffe2984', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000220A3478500>]}
[0m21:48:30.704942 [info ] [Thread-1 (]: 3 of 3 OK created sql table model saleslt.sales ................................ [[32mOK[0m in 4.90s]
[0m21:48:30.720801 [debug] [Thread-1 (]: Finished running node model.azure_dbt_spark.sales
[0m21:48:30.720801 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=13.79707956314087s, language=None, compute-name=) - Reusing connection previously named master
[0m21:48:30.722811 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:48:30.722811 [debug] [MainThread]: Connection 'list_hive_metastore' was properly closed.
[0m21:48:30.722811 [debug] [MainThread]: On list_hive_metastore: Close
[0m21:48:30.722811 [debug] [MainThread]: Databricks adapter: Connection(session-id=a24e58da-80ea-42b8-92a3-dd7f9971fd02) - Closing
[0m21:48:30.919595 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m21:48:30.919595 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m21:48:30.919595 [debug] [MainThread]: Databricks adapter: Connection(session-id=86b14148-16cf-4579-b79c-d6fcf68fe4bd) - Closing
[0m21:48:31.113549 [debug] [MainThread]: Connection 'list_hive_metastore_saleslt' was properly closed.
[0m21:48:31.115318 [debug] [MainThread]: On list_hive_metastore_saleslt: Close
[0m21:48:31.117340 [debug] [MainThread]: Databricks adapter: Connection(session-id=1d41e83b-3730-49dc-976b-c1298c96b5f4) - Closing
[0m21:48:31.303439 [debug] [MainThread]: Connection 'model.azure_dbt_spark.dim_product' was properly closed.
[0m21:48:31.319525 [debug] [MainThread]: On model.azure_dbt_spark.dim_product: Close
[0m21:48:31.319525 [debug] [MainThread]: Databricks adapter: Connection(session-id=7175ea01-5393-4517-9903-af36a76f39a1) - Closing
[0m21:48:31.527681 [debug] [MainThread]: Connection 'model.azure_dbt_spark.sales' was properly closed.
[0m21:48:31.527681 [debug] [MainThread]: On model.azure_dbt_spark.sales: Close
[0m21:48:31.527681 [debug] [MainThread]: Databricks adapter: Connection(session-id=5d6a1653-1db3-4d64-b3ef-573dd3003288) - Closing
[0m21:48:31.717355 [info ] [MainThread]: 
[0m21:48:31.717355 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 18.57 seconds (18.57s).
[0m21:48:31.717355 [debug] [MainThread]: Command end result
[0m21:48:31.763890 [debug] [MainThread]: Wrote artifact WritableManifest to H:\Acadmy\My\Projects\Azure_Spark_DBT\azure_dbt_spark\target\manifest.json
[0m21:48:31.763890 [debug] [MainThread]: Wrote artifact SemanticManifest to H:\Acadmy\My\Projects\Azure_Spark_DBT\azure_dbt_spark\target\semantic_manifest.json
[0m21:48:31.775639 [debug] [MainThread]: Wrote artifact RunExecutionResult to H:\Acadmy\My\Projects\Azure_Spark_DBT\azure_dbt_spark\target\run_results.json
[0m21:48:31.775639 [info ] [MainThread]: 
[0m21:48:31.775639 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:48:31.775639 [info ] [MainThread]: 
[0m21:48:31.775639 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
[0m21:48:31.775639 [debug] [MainThread]: Command `dbt run` succeeded at 21:48:31.775639 after 21.78 seconds
[0m21:48:31.775639 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002208F9130E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002208F895940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002208E6CE3C0>]}
[0m21:48:31.775639 [debug] [MainThread]: Flushing usage events
[0m21:48:32.642336 [debug] [MainThread]: An error was encountered while trying to flush usage events
